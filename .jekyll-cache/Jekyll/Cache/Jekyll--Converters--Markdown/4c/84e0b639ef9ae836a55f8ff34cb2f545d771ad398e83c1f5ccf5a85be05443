I"èZ<h2 id="1-vorlesung">1. Vorlesung</h2>

<p>Link zu den <a href="https://ilias.studium.kit.edu/goto.php?target=file_884805_download&amp;client_id=produktiv">Folien</a> der ersten Vorlesung.</p>

<h3 id="organisation">Organisation</h3>

<p>Webseite: <a href="http://isl.ira.uka.de">http://isl.ira.uka.de</a></p>

<p>Videos von WS17/18 auf DIVA: <a href="https://mediaservice.bibliothek.kit.edu/#/details/DIVA-2017-518">DIVA Videos</a></p>

<p>WS 2013/2014 auf Youtube: <a href="https://www.youtube.com/playlist?list=PLfk0Dfh13pBMPGYh-vP9c6rSwkYcTUFo3">Playlist</a></p>

<p><a href="https://ilias.studium.kit.edu/goto.php?target=crs_884802&amp;client_id=produktiv">Ilias-Kurs</a></p>

<p>M√ºndliche Klausur: Anmeldung √ºber das Sekretariat (per E-Mail).</p>

<h3 id="literatur">Literatur</h3>

<ul>
  <li>Xuedong Huang, Alex Acero, Hsiao-Wuen Hon:  Spoken Language Processing
<em>sehr umfangreich. Mehr drin als in der Vorlesung. Sch√∂n.</em></li>
  <li>A. Waibel, K. F. Lee: Readings in Speech Recognition
<em>√§lter, aber zwei gute Paper.</em></li>
  <li>F. Jelinek: Statistical Methods of Speech Recognition
<em>klein, oberfl√§chig, gutes Material zu Sprachmodellierung</em></li>
  <li>Schukat-Talamazzini: Automatische Spracherkennung
<em>PDF verf√ºgbar</em></li>
  <li>Ivica Rogina: Sprachliche Mensch-Maschine-Kommunikation
<em>unvollst√§ndig. Download Ilias</em></li>
</ul>

<p><strong>Eindr√ºcklicher Hinweis darauf, dass die Literatur zu lesen ist!</strong> Die Folien alleine reichen definitiv nicht aus.</p>

<p>‚ÄúUntersch√§tzen Sie das nicht!‚Äù</p>

<h3 id="definition">Definition</h3>

<p>Automatic Speech Recognition (ASR):</p>

<p>Definition:
    Automatische Umwandlung menschlicher, gesprochener Sprache in die dazugeh√∂rige Wortsequenz in maschinenverarbeitbarer Form.</p>

<p>H√§ufig f√§lschlicherweise als Voice-Recognition bezeichnet.</p>

<p>Menschliche, gesprochene Sprache ist auf Englisch: <em>speech</em></p>

<p><em>Language</em> hingegen umfasst auch Text, Gestik, Tonh√∂he‚Ä¶</p>

<p>Bei ASR geht es weder um die Tonh√∂he noch ob Sprecher m√§nnlich oder weiblich etc. Auch die Semantik ist in der Definition nicht enthalten. Natural/Spoken Language Understanding versucht Semantik zu erfassen.</p>

<p>Frage: Wie kann man W√∂rter korrekt niederschreiben ohne den Sinn zu kennen?</p>

<p>Antwort: Das geht nicht wirklich. Die immer gleichen Fehler passieren.</p>

<p><strong>L√ºcke:</strong> Es gibt eine L√ºcke zwischen gesprochener Sprache und der Veschriftung. Zum Beispiel Satzzeichen. Diese sind nicht im ‚ÄúSignal‚Äù enthalten, der Mensch imaginiert sie sich hinzu. Sowetwas braucht die Maschine quasi auch.</p>

<p>Anmerkung zu den Satzzeichen: Pausen markieren nicht pauschal Satzgrenzen. Es gibt sogar Studien, die zeigen, dass mehr als die H√§lfte von Pause innerhalb von S√§tzen gemacht werden. (Das ber√ºhmte ‚Äú√Ñ√§√§h‚Äù von Boris Becker.)</p>

<h3 id="anwendungsgebiete">Anwendungsgebiete</h3>

<ul>
  <li>Diktat (√ºberall wo man nicht schnell genug tippen kann, H√§nde besch√§ftigt)</li>
  <li>Untertitel f√ºr Filme (-&gt; Durchsuchbarkeit von Videos)</li>
  <li>Sprachsuche, Spionage, √úberwachung</li>
  <li>Assistenzsysteme (Blinde, Taube, k√∂rperlich Eingeschr√§nkte)</li>
  <li>Pick-To-Voice-Systeme: Amazonlogistiker im Lager geben und Empfangen Befehle per Headset beim Packen von Paketen</li>
</ul>

<p>Anmerkung: Gesprochene Sprache ist zeitlinear -&gt; die wenigsten Menschen k√∂nnen diktieren.</p>

<p><strong>Was sind gute Anwendungsgebiete:</strong></p>

<ul>
  <li>Geschwindigkeit von Eingabe erh√∂hen (Sprache &gt; Stenographie &gt; Tippen)</li>
  <li>Bei Verhinderung der H√§nde etc.</li>
  <li>Schon gelerntes Werkzeug - ‚Äúnat√ºrliche Art der Eingabe‚Äù (vgl. Programmieren eines alten Videorecorders)</li>
  <li>Mikrofone sind billig, leicht und klein</li>
  <li>Zus√§tzlicher Kanal (Sprache und Kn√∂pfe, Ausfallsicherheit)</li>
</ul>

<h2 id="2-vorlesung">2. Vorlesung</h2>

<h3 id="nachteile-der-spracherkennung">Nachteile der Spracherkennung:</h3>

<ul>
  <li>Umgebungen, in denen nicht gesprochen werden kann (Theater, laute Umgebung):
Es gibt auch lautlose Spracherkennung: Ultraschalle am Kinn, Muskel EEG, Hirnstr√∂mungen</li>
  <li>Entferntes Mikrofon: Ergebnis sind dann sehr schlecht und sie werden mehr als
linear schlechter mit der Distanz</li>
</ul>

<blockquote>
  <p>Beispiel zur Distanz: Google Homepod. Ver√∂ffentlichung von Google: Eigene Mitarbeiter
produzieren eine h√∂here Wortfehlerrate als normale Nutzer, weil sie kompliziertere
Befehle ausf√ºhren wollen (20% vs 8%). Das liegt daran, dass einfachere Aufgaben
‚ÄúWie wird das Wetter?‚Äù leichter zu verstehen sind und nicht alle Nuancen des
Satzes ben√∂tigen im Gegensatz zu ‚ÄúBestelle mir das rote Pucky-Fahrrad von Amazon
f√ºr meine Tochter‚Äù.</p>
</blockquote>

<p>In einem realen Projekt muss immer die erste Frage sein: ‚ÄúWo kann ich das Mikrofon anbringen?‚Äù
Wenn die Antwort nicht nah genug am Mund ist, dann ist es schon ein schlechtes Zeichen.
Beispiel im Auto: 30 cm in der Deckenverkleidung ist schon problematisch.
Beim Live-Transkribieren im europ√§ischen Parlament (wo der Dozent wohl schon einmal
  mit dem L√∂tkolben die Mikrofonkabel angezapft hat), war es wichtig, keine
  Hintergrundger√§usche im Audiosignal dazugemischt zu bekommen.</p>

<p>Das Audiosignal von alten Kampfjets kam √ºber den Kehlkopf fr√ºher, weil es einfach
zu laut war. Heute hat man die Mikrofone vorne in der Maske.</p>

<p>Die Hintergrundger√§usche sind dabei immer ein Problem. Bei Autos hat man das so
gel√∂st, dass man einen neuen Datensatz von Stimmaufnahmen mit Autoger√§uschen im
Hintergrund erstellt hat.</p>

<h3 id="taxonomie-sprache">Taxonomie Sprache</h3>

<p>Hier geht es um language.</p>

<p>K√ºnstliche Sprache, zum Beispiel eine kontextfreie Grammatik mit festem Vokabular.
Der Nutzer muss geschult werden. Das kann man nur mit Arbeitnehmern machen.</p>

<p>Normalen Anwendern kann man keine Schulung zumuten, das hei√üt, man m√∂chte
<strong>Spracherkennung nat√ºrlicher Sprache</strong>.</p>

<p>Man unterscheidet bei nat√ºrlicher Sprache drei Schwierigkeitsstufen:</p>

<ol>
  <li>Gelesene Sprache (Vorlesen eines bereits existierenden Textes, um System zu testen)</li>
  <li>Geplante Sprache (Vortragen einer Rede im Bundestag sollte zwar geplant sein,
  aber frei gesprochen sein)</li>
  <li>Ungeplante Sprache (Konversationen. Und die werden immer schwieriger je
  kodifizierter die Kommunikation ist, was bei engen Beziehungen der Sprecher
  der Fall ist, zum Beispiel in einer Familie)</li>
</ol>

<h3 id="taxonomie-f√ºr-sprache-und--erkennung">Taxonomie f√ºr Sprache und -erkennung</h3>

<ul>
  <li>Sprecherabh√§ngig: Dialekte, Akzente werden dar√ºber bedacht, dass das System
auf einen Sprecher eintrainiert wird. Alte Diktierger√§te musste man 15-Minuten
antrainieren, indem man ihnen einen bestimmten Text vorla√ü. Andere Personen
konnten das Ger√§t dann nicht benutzen.
Wenn sich die Umgebung (Teppich statt Parkett) oder das Mikrofon ver√§ndert hat,
dann funktionierten diese Systeme schnell nicht mehr.</li>
  <li>Geschlossene Sprechermenge, zum Beispiel in einem Parlament. Da hat man viele
Trainingsdaten und kann sich auf jede Person einstellen.</li>
  <li>Sprecherunabh√§ngig: Das gew√ºnschte System</li>
  <li>Gr√∂√üe des Diskurs: Dialog versus Gruppengespr√§ch -&gt; unerwartete W√∂rter
    <ul>
      <li>dom√§nenlimitiert: Sehr gro√üe Dom√§ne (Parlament)</li>
      <li>dom√§nenunabh√§ngig: Gibt es noch nicht wirklich (ist streitbar wie gro√ü eine Dom√§ne sein kann)</li>
    </ul>
  </li>
  <li>Sprecherverhalten:
    <ul>
      <li>kooperativ: Sprecher kennt das System nicht, will aber dass es funktioniert</li>
      <li>unkooperativ: Beispiel Telekom-Kundenhotline, man will gar nicht erkannt werden,
wenn man sich beschwert</li>
      <li>Vertraut: Menschen adaptieren sich im gegenseitigen Gespr√§ch (Spiegelneuronen),
das gleiche Ph√§nomen passiert auch bei Mensch und Maschine. Es werden sogar
neue W√∂rter erfunden (Studie √ºber das Bestellen eines Mobilfunkvertrages in
fremder Sprache)</li>
    </ul>
  </li>
  <li>√Ñu√üerungsart:
    <ul>
      <li>Isolierte W√∂rter</li>
      <li>Phrasen</li>
      <li>Kontinuierliche Sprache</li>
    </ul>
  </li>
</ul>

<p>Frage: Wie funktioniert die Trennung von Kontinuum in isolierte W√∂rter?</p>

<p><strong>Achtung!</strong> Der nat√ºrlichste Gedanke w√§re, dass man sich einen Satz von unten
nach oben (Phonem f√ºr Phonem) zusammenbaut, <strong>aber das ist genau, wie die
guten Systeme nicht funktionieren.</strong></p>

<p>Es wird mit dem ganzen Satz begonnen und der wird dann zerlegt. Von oben nach
unten. (Monolithischer Block)
Es werden quasi alle S√§tze einmal probiert und f√ºr jeden Satz gibt es eine
Wahrscheinlichkeit, circa: ‚ÄúWie h√§tte dieser Satz geklungen, wenn er
die vorliegende Audioaufnahme gewesen w√§re.‚Äù</p>

<p>Dabei fallen dann wiederum quasi als Nebenprodukt Wort- und Phonemgrenzen ab.</p>

<p>Frage: Hat man bei Satzgrenzen wieder das gleiche Problem?</p>

<p>Antwort: Ja. Da kommt man etwas in tricksen mit Mathe. Ans√§tze basiernd auf
neuronalen Netzen k√∂nnen das umgehen, sind aber noch nicht so gut‚Ä¶</p>

<p>Spracherkennung ist schwer, weil die menschliche Sprache sehr variant ist:</p>

<ul>
  <li>Signalebene: Kein Mensch kann ein Wort zwei mal artikulieren und es produziert
das gleiche Signal. Akustik, Umgebung, Mikrofone haben auch Einfluss auf das Signal.</li>
  <li>Phonetische Ebene: Kein Mensch spricht ein Phonem gleich aus. Alle Menschen
haben eine unterschiedliche Sprache, Dialekte, Sprachfehler‚Ä¶</li>
  <li>Linguistische Ebene: Mensch ist sehr kreativ. Variation der S√§tze ist sehr
hoch. Analyse der Texte aus dem Wall Street Journal. Mit jeder Ausgabe gab
es neue W√∂rter, das menschliche Gehirn lernt diese einfach dazu und erschlie√üt
sie sich. Der Computer ist noch nicht so adaptiv.</li>
</ul>

<p>Frage: Ist Betonung ein Problem?</p>

<p>Antwort: Verlauf der Grundfrequenz ist ein besserer Ausdruck. Bei Deutsch und
Englisch relativ irrelevant, wird teilweise ignoriert.</p>

<p><strong>Experiment zum McGurk-Effekt:</strong> Immer gleiches Audioschnipsel wird √ºber
Video von unterschiedlichen Lippenbewegungen gelegt und die Menschen h√∂ren
unterschiedliches, weil der Mensch indirekt Lippen lesen kann.</p>

<p>Ein Komilitone im Raum hatte den Effekt nicht. M√∂gliche Begr√ºndung, weil er
als Kind die F√§higkeit aufgrund seiner Sehschw√§che nicht aufbauen konnte.</p>

<p>Auf bei Japanern funktioniert das nicht. Unwahrscheinliche Erkl√§rung ist aufgrund
fehlenden Blickkontaktes. Wahrscheinlicher ist, dass die sichtbaren Feature
(Lippen, Kinn, Spitze der Zunge) nicht hilfreich sind f√ºr das Japanische.</p>

<h2 id="3-vorlesung">3. Vorlesung</h2>

<p>H√∂rsaal Akustik -&gt; abgerundete Kanzel (Schallreflektion zum Publikum wie in
  evangelischen Kirchen)
Glatter Linoleumboden, glatte Tafel =&gt; reflektieren Schall -&gt; Schall kommt dann
unterschiedlich schnell beim Ohr des H√∂rers an und ergibt einen ‚ÄúVerschmierungs‚Äù-Effekt.</p>

<p>An den W√§nden und der Decke sind Lochblenden zwischen denen sich der Schall
verfangen kann. Menschen absorbieren Schall sehr gut.</p>

<p>An der Decke des H√∂rsaals, der auch f√ºr Videoaufnahmen genutzt wird, befinden
sich Grenzfl√§chenmikrofone, um Fragen des Publikums aufnehmen zu k√∂nnen.
Es handelt sich um ein spezielles Mikrofon auf einer Plexiglasscheibe.</p>

<p>=&gt; Warnung: Aufnahmetechnik ist sehr wichtig f√ºr Erfolg bei Spracherkennung</p>

<p><strong>Lombard-Effekt</strong>
Bei Blaupunkt wurde eine Freisprecheinrichtung f√ºr Autos entwickelt, die das
Signal des Autoradios aus der Sprachaufnahme herausrechnen kann, wodurch das
Radio bei Telefonaten im Auto an bleiben kann, jedoch nicht am anderen Ende
der Leitung geh√∂rt wird.
Es hat sich herausgestellt, dass die Tester des Systems vollkommen anders
gesprochen haben als normal, weil sie sich der lauten Umgebung angepasst haben.</p>

<p><strong>Homonyme:</strong> Gleichkl√§nge von W√∂rtern mit unterschiedlicher Bedeutung</p>
<ul>
  <li>Homographene: Man schreibt sie sogar gleich, z. B. Schloss, Schloss</li>
  <li>Homophone: Sie klingen gleich, aber sie werden unterschiedlich geschrieben,
z. B. bis und Biss, Verse &lt;-&gt; Ferse</li>
</ul>

<p>Mehrdeutigkeiten von S√§tzen: ‚ÄúTime flies like an arrow‚Äù (Time flies like an arrow. Fruit flies like a banana.)</p>

<p>Spracherkennung ist Mustererkennung. Klasse ist der komplette Satz.</p>

<p>Folie 13: Daten√ºbertragung ist bei 30.000 bps, aber durch Redundanz und unn√ºtzes Signal werden
die 50.000 bps aufgef√ºllt.</p>

<p>Schichtenarchitektur aus Modell funktionieren nicht so top-down wie dargestellt,
es existieren Beeinflussungen und Feedbackschleifen.</p>

<p>‚ÄúFlugzeuge schlagen nicht mit den Fl√ºgeln und Autos haben keine Beine‚Äù</p>

<p>Gaumensegel macht den Nasenraum zu. Harter Gaumen und Z√§hne sind passive
Artikulatoren. Impulse/Radst√∂√üe (aus der Vorlesung Kognitive Systeme) sind gut
geeignet um Zerhackung des Luftstroms aus der Lunge durch die Stimmb√§nder zu
modellieren.
-&gt; Impulszug wird moduliert</p>

<p><strong>Bernoulli-Effekt</strong>
Medium (Gas,Fl√ºssigkeit) str√∂mt durch begrenzten Kanal.</p>

<ol>
  <li>Die Str√∂mungsgeschwindigkeit eines Molek√ºls passt sich seiner Umgebung an
-&gt; der Rand des Kanals bewegt sich garnicht, daher ist au√üen die Flussgeschwindigkeit
   geringer</li>
  <li>Durch die unterschiedlichen Flussgeschwindigkeiten entsteht ein Unterdruck in
der Mitte. -&gt; Dieser Unterdruck zieht die Stimmlippen zusammen.</li>
</ol>

<p>Das Zusammenziehen und √ñffnen durch den Druck aus der Lunge wiederholt sich sehr
oft pro Sekunde. Vergleichbar mit einem Luftballon, den man zum Quitschen bringt.</p>

<p>Pulmonische Sprache (pulmo = Lunge) macht fast alle menschlichen Sprachen aus,
es gibt aber Ausnahmen (f√ºr uns verst√§ndliches Beispiel: Amis/Engl√§nder machen Gluk-Gluk
  um S√§ufer zu imitieren mit dem Gaumen)</p>

<p>Die Frequez des √ñffnen und Schlie√üens der Stimmb√§nder ergibt die Stimmh√∂he.
Eine hohe Frequenz macht einen h√∂heren Ton. (f√§lschlicherweise denken manche,
  dass es mit der Gr√∂√üe des Vokaltraktes zusammenh√§ngt, dieser Zusammenhang
  besteht aber nur √ºber die Gr√∂√üe der Stimmb√§nder; Mann Testosteron gr√∂√üere
  Stimmb√§nder -&gt; Sichtbarkeit des Kehlkopfes -&gt; niedrigere Frequenz Stimme)</p>

<p>Langoroskop (Kamera in Rachen)  + Stroposkoplicht (Subsamplingder Bildrate)
-&gt; Aufnahmen von Stimmb√§ndern</p>

<p>Mitlaute v.s. Selbstlaute</p>

<p>Vokale sind immer stimmhaft = Stimmb√§nder schwingen</p>

<p>Definition: <strong>Phon</strong>
Kleinster Bestandteil der Spreche, der wahrgenommen wird. Der Mensch glaubt
zumindest, dass da etwas ist, auch wenn manchmal im Signal nichts spezielles da ist
(vergleiche mit Pausen)</p>

<p>Phonetiker und Linguisten nutzen die menschliche Wahrnehmung als Forschungsbasis</p>

<p>-&gt; Optische T√§uschungen zeigen, dass die Wahrnehmung get√§uscht und interpretiert
   ist</p>

<p>Durch das exakte Scannen des Hirns k√∂nnen sog. Nuklei aufgenommen werden, die
soetwas wie eine Taktrate haben. Diese Takte sind im Vielfachen (zum Beispiel
  Nucleus A ist doppelt so schnell wie B) miteinander getaktet
  -&gt; Wiederum eingeschlossen in gr√∂√üere Nuklei, die mitunter einen Taktschlag
     pro 15-20 Millisekunden haben -&gt; Das w√ºrde bedeuten, dass auf dieser
     Millisekundenebene gar nichts zuverl√§ssig verarbeitet wird -&gt; Theorie: Phone existieren nicht.</p>

<p>Lippenform: Flach oder rund
Zungenspitze: Dorsum Linguae</p>

<p>Vokalviereck: Bestimmung eines Vokals durch Position des dorsum linguae und der
√ñffnung der Lippen. Zwei Buchstaben pro Position. Der rechte ist mit runden Lippen.</p>

<p>Diphtonge wandern durch das Viereck im Laufe der Artikulation. Monophtonge nicht.
Vokale k√∂nnen auch nasal ausgesprochen werden: Franzosen</p>

<p>Einordnung der Konsonanten:</p>

<ul>
  <li>Art der Artikulation</li>
  <li>Ort der Artikulation</li>
  <li>Stimmb√§ndereinsatz (Hand an Stimmb√§nder halten um zu sp√ºren, z. B. ‚Äúb‚Äù vs ‚Äúp‚Äù)</li>
</ul>

<p>‚ÄúPf‚Äù ist zum Beispiel ein Phon. Es ist nicht ‚ÄúP‚Äù und ‚Äúf‚Äù sondern zusammen wahrgenommen.</p>

<p>Mit wenigen Monaten k√∂nnen Kinder alles h√∂ren und √ºber die Zeit lernt man dann,
dass manches kein sinnvolles Signal ist oder nicht unterschieden werden muss,
dann f√§ngt man an, diese Unterschiede nicht mehr zu h√∂ren.
Das sorgt f√ºr ein schwierigeres Erlernen von Fremdsprachen, wenn man √§lter ist,
zum Beispiel russische Muttersprachlerin ‚ÄúJochen‚Äù und ‚ÄúJohann‚Äù klingen, wegen des
fehlenden Verst√§ndnis f√ºr ein aspiriertes ‚ÄúH‚Äù, gleich.</p>

<h2 id="4-vorlesung">4. Vorlesung</h2>

<p>Offene Konfiguration des Vokaltraktes: Vokal (vowel)
Geschlossene: Konsonant</p>

<p>IPA = International Phonetic Alphabet
herausgegeben von der International Phonetic Association</p>

<p>Alle bekannten Sprachlaute der Welt sollen durch ein eindeutiges Alphabet
beschrieben werden k√∂nnen, damit man alle Sprachen so schreiben kann, wie sie
klingen, wie Noten in der Musik.</p>

<p>Bei den Vokalen gibt es zwei Symbole, die stehen alleine und zwischen zwei Knoten.
Die gibt es nur gerundet. Die grauen k√∂nnen theoretisch artikuliert werden,
existieren aber in keiner Sprache.</p>

<p>Aus dem Audiosignal kann man offensichtlich nicht die Konfiguration des Vokal-
traktes oder √§hnliches ableiten. Daher misst man mit Metallpl√§ttchen im Mund,
die man elektromagnetisch pr√ºfen kann.
Es gibt auch noch den Laryngograph (<a href="https://en.wikipedia.org/wiki/Electroglottograph">electroglottograph</a>),
dieser misst von au√üen am Kehlkopf.
Man kann auch den Luftdruck in und au√üerhalb des Mundes messen, um Unterschiede
bei plosiven o. √Ñ. zu quantifizieren.</p>

<p>Die gepulsten Luftstr√∂me der pulmonischen Sprache werden dann durch die Formung
des Mundes moduliert. Die unterschiedlich gro√üen R√§ume durch die der
Schall hindurch muss, k√∂nnen als <strong>Helmholtz-Resonator</strong> modelliert werden.
Der Helmholtz-Resonator ist eine R√∂hre, in der sich Wellen ausbreiten.</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/PoEyIJx3uM0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p>IPA Konsonanten: links stehen jeweils die stimmlosen und rechts die stimmhaften Konsonanten.
Zum Beispiel p, b. Die lila hinterlegten Zellen sind unm√∂glich zu artikulieren.
Leere, wei√üe Zellen w√§ren m√∂glich, aber sind noch nicht in menschlicher Sprache
gefunden worden. Der Gro√üteil der gesprochenen Sprachen ist jedoch nicht
durch Linguisten oder Phonetiker erforscht worden. (Es gibt sehr viele kleine
  Sprachen)</p>

<p><strong>Diakritika:</strong> Kleine Zeichen zur Modifikation des IPA, zum Beispiel Hochkomma
f√ºr Aspiration oder Doppelpunkt nach Vokal f√ºr Streckung (W<strong>e</strong>k, w<strong>e</strong>g) [v…õÀê…ï], [v…õk]</p>

<p>Die ersten paar Kapitel des IPA Handbuchs f√ºhren nochmal besser in dieses Thema
ein.</p>

<h3 id="phoneme">Phoneme</h3>

<p>Ein Phonem ist die kleinste bedeutungsunterscheidende Einheit einer menschlichen
Sprache.</p>

<p>Das <strong>Phon</strong> ist unabh√§ngig von einer spezielle Sprache; ein Phonem hingegen nicht.</p>

<p>Wie findet man Phoneme? F√ºr Laien: Unterscheidung zweier W√∂rter durch Tausch
eines Phons, zum Beispiel ‚ÄúHaus‚Äù und ‚ÄúMaus‚Äù.</p>

<p>Austauschbare Laute, die die Bedeutung erhalten, hei√üen <strong>Allophone</strong>.
Beispiel: ‚ÄúChemie‚Äù, ‚ÄúSchemie‚Äù. Das schlie√üt aber nicht auf alle Phone, zum Beispiel
gibt es trotzdem Unterscheidungen, wie  ‚ÄúRauch‚Äù, ‚ÄúRausch‚Äù.
Weiteres Beispiel ist das gerollte und ungerollte ‚ÄúR‚Äù im Deutschen.</p>

<p>Jedes Phon ist entweder Phonem oder Allophone und letzere k√∂nnen auch wieder zu
Phonemen zusammengesetzt werden. ‚ÄúMonophon‚Äù &lt;-&gt; Allophon</p>

<p>Weiteres Beispiel: Im Japanischen klingen ‚ÄúR‚Äù und ‚ÄúL‚Äù gleich, daher wissen
Japaner oft nicht, was genutzt werden soll, wenn sie eine Sprache sprechen in
der ‚Äúr‚Äù und ‚Äúl‚Äù keine Allophone sind. Prinzipiell k√∂nnen sie aber schon ‚ÄúR‚Äù und ‚ÄúL‚Äù
differenziert sprechen, es ist nur nicht n√∂tig im Japanischen.</p>

<p>Die Phoneme sind nicht eindeutig definierbar. Gegenstand von Diskussionen
zwischen Linguisten und Phonologen. F√ºr Informatiker ist das nicht so wichtig.
Eher die Frage, ob das Phonem geeignet ist f√ºr die Spracherkennung.
Zum Beispiel: Wenn ein sehr diskutiertes Phonem (glotaler Stopp bei ‚Ä¶) nur
drei mal in 10 Stunden sprache annotiert wurde, dann ist es aus einer probabilistischen
Sicht nicht so wichtig. Praktisch hei√üt das, dass man sie ignoriert und hofft,
dass sich minimale Paare aus dem Kontext unterscheiden lassen.</p>

<p>Bei jeder Sprache gibt es neue Ph√§nomene. Zum Beispiel Klicks oder nasale Vokale
im Franz√∂sischen. Oder tonale Sprachen.</p>

<p><strong>Tonale Sprachen:</strong> Beim Englischen, wo man fr√ºher sehr viel geforscht hatte, war
und ist die ‚ÄúTonh√∂he‚Äù (-&gt; besser Grundfrequenz nennen) uninteressant.
Im Mandarin sind sie aber bedeutungsunterscheidend. Dort gibt es f√ºnf T√∂ne:
steigend, fallend, bergf√∂rmig, flacher Ton und unbetonter Ton.
Beim Kantonesischen sind es sogar 8 T√∂ne + der flache Ton = 9.
In afrikanischen Zulu-Sprachen wird es noch mehr, denn dort √§ndert sich die
Tonh√∂he mit der Stelle des Wortes im Satz‚Ä¶</p>

<h3 id="physik-wellen">Physik Wellen</h3>

<p>Schall ist eine Longitudinalwelle. Kleine Auffrischung von Schulwissen:</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/eyO1UlrPqIQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p>Ruhe Luftdruck auf NN 1013 mbar (Millibar).</p>

<p>Schalldruck: Wechseldruck in Pascall ($ {N \over m^2}  [Pa] $), der zum Ruhedruck
hinzukommt. Der leisteste h√∂rbare Schall hat einen Schalldruck von 1e^-5 Pa.
Die Schmerzgrenze f√ºr den Menschen liegt bei 63 Pa. -&gt; 10 Milliarden mal gr√∂√üer als das Leiseste
=&gt; gro√üer dynamischer Umfang</p>

<p>Schall kann als eine √úberlagerung von sinoiden Wellen modelliert werden.</p>

<p>T = Dauer f√ºr eine Schwingung
Frequenz f = 1/T [Hz]</p>

<p>James Clerk Maxwell hat die Theorie aufgestellt, die elektromagnetische Wellen
vorstellt. Deren Existenz wurde durch Hertz nachgewiesen (1/s ist 1 Hz):</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/b2cVLHozb9k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p>Schall breitet sich mit begrenzter Geschwindigkeit (abh√§ngig vom Medium) aus.
Wellenl√§nge $ l = {c \over f} $</p>

<p>Dazu ein kleines Video, das den Zusammenhang zwischen elektromagnetischen
Wellen und deren Frequenz illustriert:</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/7WXW2bBWBEg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p><strong>Schalldruckpegel</strong>: $ L_{p} = 10 log_{10} ( { \tilde{p}^2 \over p_{0}^2 } ) [dB] $
  oder $  L_{p} = 20 log_{10} ( { \tilde{p} \over p_0} ) [dB]  $.</p>

<p>Wobei $\tilde{p}$ der vorzeichenbereinigte Schalldruck und $p_0$ der Referenzdruck $2 * 10^{-5} Pa$.
Urspr√ºnglich wurde letzteres f√ºr die H√∂rschwelle bei 1 kHz gehalten. Jetzt leicht
anders, aber man passt die Referenz nicht mehr an, daher kann es zu zu negativen
Werten in niedrigen Bereichen kommen, da: $log(x) &lt; 0$ f√ºr x kleiner 1.</p>

<p>Die Wahl des Logarithmus spiegelt auch das menschliche Empfinden beim Schall,
da er nicht linear als lauter empfunden wird.</p>

<p>dezi Bell gibt es auch bei Elektromagnetismus, daher nutzt man in der Akustik noch
ein angeh√§ngtes ‚ÄúA‚Äù f√ºr ‚ÄúAkustisch‚Äù: <strong>dB (A)</strong></p>

<table>
  <thead>
    <tr>
      <th>Referenz</th>
      <th>Ungef√§hrer Schalldruck [dB (A)]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$P_{}$</td>
      <td>0</td>
    </tr>
    <tr>
      <td>K√ºhlschrank</td>
      <td>40</td>
    </tr>
    <tr>
      <td>Schreien</td>
      <td>65</td>
    </tr>
    <tr>
      <td>Konzert (ab hier beginnt das Geh√∂r Schaden zu nehmen)</td>
      <td>120</td>
    </tr>
    <tr>
      <td>Gewehrschuss (ab hier beginnt Schmerzgrenze)</td>
      <td>140</td>
    </tr>
    <tr>
      <td>Raketenstart</td>
      <td>180</td>
    </tr>
  </tbody>
</table>

<p>Man beachte, dass die Schmerzgrenze leider erst nach der Schadensgrenze beginnt.</p>

<p><strong>Schallenergie:</strong> Zusammensetzung aus kinematischer und potentieller Energie.
Modellannahme: Luftteilchen, die an fiktivem Punkt mit einer fiktiven Feder
verbunden sind. ‚ÄúAufziehen‚Äù und ‚ÄúZur√ºckschwingen‚Äù (siehe Erkl√§rung zu Helmholtz
  Resonator)</p>

<p>Energie wird Schall zugewiesen. Schalldruck und Energie verhalten sich unterschiedlich
bez√ºglich der Distanz.</p>

<ul>
  <li>$E ~ { 1 \over r^2 }$: die Energie verh√§lt sich umgekehrt, quadratisch
proportional zum Radius</li>
  <li>$P ~ { 1 /over r}$: der Druck hingegen umgekehrt linear proportional</li>
</ul>

<p>Die Schallwelle breitet sich als Kugel aus -&gt; Oberfl√§che der Kugel -&gt; quadratische Abnahme der Energie
 ???</p>

<h3 id="das-ohr">Das Ohr</h3>

<p>Ohrmuschel dient als Empf√§nger.</p>

<p>Druck auf Trommelfell von au√üen unterschiedlich. Die Longitudinalwelle wird
am Trommelfell in eine mechanische Bewegung √ºbertragen.
√úber Steigb√ºgel, Hammes und Amboss findet dann eine Verst√§rkung des Signals
auf eine kleinere Fl√§che statt. <strong>Ovales Fenster</strong>
Dieses Ovale Fenster √ºbertr√§gt die mechanische Bewegung in eine
<a href="https://de.wikipedia.org/wiki/Wanderwelle">Wanderwelle</a> auf die
Fl√ºssigkeit innerhalb der geschlossenen <strong>Schnecke (latein Cochlea)</strong>.
Es handelt sich um eine stehende Welle.</p>

<p>Die Schnecke wird nach hinten immer schmaler. Dadurch findet sich zu jeder Frequenz
ein Abschnitt der Cochlea mit einer passenden Resonanzfrequenz. Das hei√üt, es
gibt dort besonders starke Wechseldrucke. Salopp: ‚ÄúEs schwappt dort besonder
viel Wasser hin und her‚Äù.</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/WeQluId1hnQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p>Ungef√§hr: Diese Resonanz wird √ºbertragen und an der basal membran auf die Haarzellen
√ºbertragen, welche dann das Signal in elektrische Reize √ºberf√ºhren.</p>

<p>Mehr Infos: https://de.wikipedia.org/wiki/H%C3%B6rschnecke</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/HgZURbqJPUo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p>Effekte:</p>

<ul>
  <li>Es ist nicht immer nur ein Resonanzbereich aktiv</li>
  <li>Schwingungen √ºberlagern sich</li>
</ul>

<p>=&gt; komplexe Muster</p>

<p><strong>Aktivierung eines Neurons:</strong> Es gibt ein Mindesniveau zum Aktivieren eines
Neurons. Davor feuert es nicht, sondern regeneriert sich nur. Aktivierungspotential.
Die notwendige Zeit zum Aktivieren eines Neurons ist l√§nger als die Dauer, einer
Schwingung eines Haars an der Cochlea.
Daraus folgt, dass der Mensch Schallwellen nur beschr√§nkt genau aufl√∂sen kann.
Es gibt also Frequenzunterschiede, die kann der Mensch nicht h√∂ren.
-&gt; pychoaktive Effekte
Diese ‚ÄúL√ºcke‚Äù wird zum Beispiel bei der Codierung von MP3 verwendet, um die
Datenrate reduzieren zu k√∂nnen.</p>

<p><img src="/assets/images/Aktionspotential.png" alt="" /></p>

<p>https://de.wikipedia.org/wiki/Aktionspotential</p>

<h2 id="5-vorlesung">5. Vorlesung</h2>

<p>Das Frequenzspektrum (oft auch nur Spektrum) eines Signals, gibt dessen Zusammensetzung
aus verschiedenen Frequenzen an. Ein Frequenzband bezeichnet einen Bereich in einem
Frequenzspektrum. Den Abstand zwischen der unteren und oberen Grenzfrequenz hei√üt
Bandbreite.</p>

<p>Die <strong>kritische</strong> Bandbreite ist der Mindestabstand (minimale Breite),
damit zwei T√∂ne vom Menschen als unterschiedlich wahrgenommen werden k√∂nnen.</p>

<p>Dadurch kann es zu spannenden Ph√§nomenen kommen.</p>

<p>Wenn man sich anschaut, was eine an das Ohr angelegte Frequenz bewirkt, dann
kann man sich die <strong>Frequenzantwort</strong> der Basilarmembran anschauen. Hier
entstehen √úberlagerungen und Kurven:</p>

<ul>
  <li>niedrige T√∂ne: eher breit und n√§her an einer Glockenform</li>
  <li>hohe T√∂ne: weniger glockenf√∂rmig, Kurve mit Plateus</li>
</ul>

<p>Dieses Wissen kann man nutzen, um Kompressoren zu designen.</p>

<h3 id="menschl-empfinden">Menschl. Empfinden</h3>

<p>Menschen haben ein subjektives Lautst√§rkeempfinden, das nicht direkt mit
Schalldruck und -energie zusammenh√§ngt.</p>

<p>Um zu messen, wie laut ein Mensch was empfindet, gibt es die Einheit <strong>Phon</strong>:</p>

<p>Der Wert in Phon gibt an, welchen Schalldruckpegel (dB) ein Sinuston mit
einer Frequenz von 1000 Hz besitzt, der gleich laut empfunden wird, wie
das eigentliche Schallereignis, das eine eigene Frequenz besitzt.
Daduch ist es m√∂glich, H√∂rempfinden mit einem Pegelwert zu beschreiben, der
unabh√§ngig vom Spektrum des Signals ist.</p>

<p>‚ÄúEin Lautst√§rkeunterschied von etwa 1 phon liegt als Unterschiedsschwelle gerade so an der Grenze der Erkennbarkeit. Deshalb ist es weder n√∂tig noch sinnvoll, Bruchteile eines Phon anzugeben.‚Äù
Null Phon ist die H√∂rschwelle.</p>

<p><img src="/assets/images/phon_skala.jpg" alt="Bild gemeinfrei" /></p>

<p>Die Linien bedeuten, dass der Sinuston den gleichen Schalldruckpegel hatte.
Man spricht von der Phon-Skala.</p>

<p><strong>Absolutes Geh√∂r:</strong> Die Frequenz eines Tones kann unabh√§ngig von der Lautst√§rke
erkannt werden. St√ºker meint, dass es sich um eine genetische Disposition handelt
in Verbindung mit Training.
Normale Geh√∂re lassen sich durch Lautst√§rke t√§uschen, wodurch zum Beispiel ein
Ton mit geringerem Schalldruck f√ºr tiefer gehalten werden kann.</p>

<p><strong>Rauschen:</strong> Es handeld sich um ein Schallsignal, das aus vielen unterschiedlichen
Frequenzen und Energieanteilen zusammengesetzt ist.</p>

<p>Wenn man Menschen einem Rauschen aussetzt, dabei aber ein gewisses Frequenzband
ausl√§sst, dann sind die Neuronen an der Basilarmembran f√ºr ‚Äúleergepumpt‚Äù bis auf
die, die das Frequenzband ablesen k√∂nnen. Dadurch entsteht nach Abschalten des
Rauschens ein Effekt, dass die Probanden √ºberempfindlich in diesem Frequenzband
sind.</p>

<p>Weiteres Experiment: Wenn man zwei nah beeinander liegende Frequenzen (Abstand
  zum Beispiel 2 Hz) in ein Signal mischt und es Menschen vorspielt, dann werden
  sie nicht als zwei T√∂ne wahrgenommen, sondern ein hin und her schwingen.
  Das nennt man auch <strong>Schwebung</strong>.</p>

<p>Wenn der Frequenzunterschied etwas gr√∂√üer ist, dann wird das Signal als ‚Äúrauer Ton‚Äù
wahrgenommen (40 Hz Unterschied).
Und wenn sie dann ausreichend weit entfernt sind, dann kann der Mensch sie als
zwei Frequenzen wahrnehmen.</p>

<h3 id="schriften">Schriften</h3>

<p>Um nun Niederschreiben zu k√∂nnen, was in einem Signal erkannt wurde, ben√∂tigt
man Schriften. Zu Beginn der automatischen Spracherkennung hat man nur im
Englischen gearbeitet und daher war das lateinische Alphabet ausreichend.</p>

<p>F√ºr ASR ist ein ‚ÄúAussprachew√∂rterbuch‚Äù sehr wichtig: Wie klingt ein Wort, wenn
es aus diesen Phonemen zusammengesetzt ist.</p>

<p>Es gibt auf der Welt einige Schriftsysteme. Prof. St√ºker findet, dass David
Christobal eher ein ‚ÄúMcDonalds-Linguist‚Äù ist, das hei√üt, es wird von vielen
Nicht-Linguisten gelesen und es ist streitbar, was er schreibt.</p>

<p><strong>Einteilung der Schriftsysteme in sechs Klassen</strong>:</p>

<ol>
  <li>
    <p>Logosyllabisch: Ein Zeichen entspricht einem Wort oder einer Silbe.
Zum Beispiel die chinesische Schrift. Laut Wikipedia:
‚ÄúDie chinesische Schrift (chinesisch ‰∏≠ÊñáÂ≠ó, Pinyin zh≈çngw√©nz√¨, Zhuyin „Ñì„Ñ®„Ñ• „Ñ®„Ñ£Àä „ÑóÀã) oder Han-Schrift (Êº¢Â≠ó / Ê±âÂ≠ó, h√†nz√¨, Zhuyin „Ñè„Ñ¢Àã „ÑóÀã) fixiert die chinesischen Sprachen, vor allem das Hochchinesische, mit chinesischen Schriftzeichen. Sie ist damit ein zentraler Tr√§ger der chinesischen Kultur und diente auch als Grundlage der japanischen Schriften (Kanji, Hiragana, Katakana), einer vietnamesischen Schrift (Ch·ªØ n√¥m) und einer der koreanischen Schriften (Hanja). Insgesamt gibt es √ºber 100.000 Schriftzeichen[1], von denen der √ºberwiegende Teil jedoch heute nur selten verwendet wird bzw. ungebr√§uchlich ist, in der Vergangenheit nur zeitweilig verwendet wurde oder Varianten darstellt. F√ºr den allt√§glichen Bedarf ist die Kenntnis von 3.000 bis 5.000 Zeichen ausreichend. ‚Äú</p>

    <p>Auch die Hyroglyphen sind ein Schriftsystem, da sie auch Zusammensetzungen k√∂nnen. Es handelt sich nicht
nur um Bilder.
<strong>Argumentation nach Daniel</strong> (Linguist): Es handelt sich nur um eine Schrift,
wenn abstrakte Ideen dargestellt werden k√∂nnen. Das ist bei Piktogrammen nicht
der Fall. Man ben√∂tigt ein Konzept von Silben oder Buchstaben.</p>
  </li>
  <li>Syllabisch: Zeichen entsprechen einer Silbe.  Zum Beispiel Cherokee.</li>
  <li>Abjad: Zeichen enstprechen Konsonatent. Abjad hei√üt ABC auf Arabisch, Namen der ersten drei Zeichen.
Arabisch und Hebr√§isch.
Problem: Gleiche Schriftliche Darstellung eines Signals, aber unterschiedliche Aussprache,
weil Vokale nicht enthalten.</li>
  <li>Alphabet: Zeichen entsprechen Vokalen und Konsonanten. Alph, bet sind die ersten
beiden Buchtaben des Herb√§ischen. Interessant also, dass Alphabet nach einem
Abjad benannt ist und nicht Griechisch.
Hier gibt es auch mehrere Laute f√ºr Buchstaben in gewissen Kombinationenn,
zum Beispiel Diphtonge oder |sch|. Das Phonem wird nicht so ausgesprochen,
wie seine Einzelteile. -&gt; Das kommt daher, dass sich die Sprache entwickelt hat,
aber die Schrift nicht angepasst wurde.
Witz von Alex Waibel: Ein Deutscher, ein Franzose und ein Engl√§nder gehen in
eine Bar. Alle nennen den Namen ihres Hundes und sagen, das schreibt man
wie man es spricht. Beim Engl√§nder ist es aber √ºberhaupt nicht klar.
Generell englische St√§dtenamen, kann man ihre Aussprache nicht gut von der Schrift
ableiten. Beim Englischen ist der Unterschied gro√ü, weil die Schrift sehr alt
und wenig ver√§ndert wurde.</li>
  <li>Abugida: Jedes Zeichen steht f√ºr einen Konsonanten und einen ihn beigleitenden
Vokal. Das Zeichen kann durch Diakritika modifiziert werden, um andere
Vokale darzustellen. Abugida leitet sich aus den ersten vier Konsontanten+Vokalen des √Ñthiopischen ab.</li>
  <li>Featural: ‚ÄúDie Form der Zeichen steht in Beziehung zu Merkmalen des Sprachsegments, f√ºr das sie stehen.‚Äù
Also alles andere.
Problem: Verschriftlichung des Signals klappt nicht gut.</li>
</ol>

<p>Die wenigsten Sprachen sind geschrieben. An vielen Orten dieser Welt gibt es
gesprochene Sprachen, aber die Schrift kam bei vielen erst im Nachhinein.
Junge Sprachen bekommen oft ein Alphabet. Kolonien haben es oft aufgedr√ºckt
bekommen, zum Beispiel Vietnam.</p>

<p>Vietnam hatte auch Hanse, dann kamen Missionare und haben dem Vietnamesischen
eine Alphabetschrift mit Diakritika aufgezogen.</p>

<p>In den Phillipinen: Tagallok hat erst sehr sp√§t √ºberhaupt eine Schrift bekommen.</p>

<h3 id="mikrofone">Mikrofone</h3>

<p>Alle Mikrofone haben eine Membran. Sie unterscheiden sich in Bauform und Wandlerprinzip.
Beim Wandlerprinzip ist zu beachten, ob man Schallenergie oder Schalldruck messen m√∂chte.
Ersteres ist empfindlicher gegen√ºber Distanz.</p>

<ul>
  <li><strong>Dynamisches Mikrofon:</strong> An die Membran ist ein Magnet befestigt, der sich bei
Schwingung durch eine Spule bewegt. Dadurch wird abh√§ngig von St√§rke des
Magnet und Fl√§che des durchdringenden Feldes elektrischer Strom induziert.
Die Geschwindigkeit der Membranschwingung kontrolliert die Stromst√§rke
in der Spule. Das Signal ist proportional zur Membrangeschwindigkeit.
Alle dynamischen Mikrofone sind Geschwindigkeitsempf√§nger.</li>
  <li><strong>Kondensatormikrofon:</strong> Eine starre und eine bewegliche Kondensatorplatte.
Die bewegliche ist mit der Membran verbunden, wodurch sich bei Ann√§herung
der beweglichen Platte die angelegte Spannung erh√∂ht.
Die absolute Auslenkung ist abh√§ngig vom Druck. Es wird also der Schalldruckpegel
gemessen und dieser ist nicht so empfindlich gg√º. Distanz. $P ~ {1 \over r}$
Alle Kondensatormikrofone sind Elongationsempf√§nger (es wird also die Auslenkung der membran gemessen)</li>
  <li><strong>Elektretmikrofon</strong>: Die Kondensatorplatte an der Membran ist bl√∂d und ben√∂tigt eine Gleichspannungsquelle.
Daher wird die Membran mit einer <strong>Elektretfolie</strong> √ºberzogen, welche dauerhaft positiv
geladen ist, indem ihr Elektronen entnommen wurden.
~90% aller Mikrofone heutzutage sind Elektretmikrofone, weil sie billig hergestellt werden k√∂nnen.
Aber eigentlich w√§re es w√ºnschenswert, dass zum Beispiel beim Telefonieren
keine Kondensatormikrofone verbaut w√§ren, damit man keine Hintergrundger√§usche h√∂rt.</li>
  <li>Kohlemikrofon: Stromfluss durch Kohlepulver ist besser, wenn Kohle unter Druck
gesetzt wird (durch Sprache). Sehr lange in Telefonh√∂rer, deswegen musste man
sie auch auf den Tisch hauen von Zeit zu Zeit, um den Kohlegries durchzusch√ºtteln.</li>
  <li>Piezomikrofon: Piezo-Kristall wird durch Membran ber√ºhrt und das induziert einen
Stromfluss. -&gt; Das hatte aber einen zu hohen <strong>Klirrfaktor</strong>. Anteil
hochfrequenter Schwingungen. Erinnert an klirrendes Glas.</li>
</ul>

<p>B√ºhnenmikrofone sind in der Regel dynamische Mikrofone, weil sie empfindlich
gg√º. Distanz sind. Die Membran wird mit Gitterk√§fig und Schaumstoff gesch√ºtzt.</p>

<p>Als <strong>Linearit√§t von Mikrofonen</strong> bezeichnet man die Qualit√§t der √úbertragung
des Schallsignals in ein elektrisches Signal. (1 kHz -&gt; 1 kHz ?)</p>

<p>Minitiatisierungseffekte werden bei Studiomikrofone f√ºr gew√∂hnlich mit Gr√∂√üe
kompensiert.</p>

<p><img src="/assets/images/mikro_wandlersystematik.png" alt="gemeinfrei aus Wikipedia" /></p>

<h4 id="richteigenschaft">Richteigenschaft</h4>

<p>Die zweite Eigenschaft von Mikrofonen ist die Richtcharakteristik.</p>

<ol>
  <li>Druckgradientenmikrofon: Gerichtet, achterf√∂rmig. Von vorne oder hinten
wird der Druckunterschied an der Membran gemessen.
An den Seiten ist das nicht m√∂glich.</li>
  <li>Druckmikrofon: Ungerichtet, kugelf√∂rmig. Eine Seite der Membran geht in eine
Box, in die der Druck von au√üen nicht so leicht hineinkommt. Dadurch kann das
Signal von allen Richtungen aus gleich stark gemessen werden.
In der Box ist der Druck konstant. Es gibt nur ein kleines Loch, welches
einen Druckausgleich wie beim menschlichen Ohr erlaubt.</li>
</ol>

<p>Der Druckausgleich beim menschlichen Ohr passiert durch eine Verbindung zwischen
Innenohr und Rachen.</p>

<p>Zus√§tzlich gibt es dann noch das Grenzfl√§chenmikrofon: Man stellt es auf eine
glatte Fl√§che, damit es den Schall von √ºberall her einsammeln kann, der auf der
glatten Fl√§che reflektiert.</p>

<p>Das Richtmikrofon (Richtrohrmikrofon) ist eine R√∂hre, die nur den Schall aus
einer speziellen Richtung beg√ºnstigt. Es ist kein Verst√§rker, man kann nur
das aufgenommene Signal elektrisch verst√§rken, da wenig Umgebungsger√§usche dabei
sind.</p>

<p>Es gibt auch noch das Parabolmikrofon, welches wie bei einer Satelitensch√ºssel
wirkt und tats√§chlich das Signal verst√§rkt.</p>

<p>Der Pop-Schutz: Schaumstoff soll niederfrequente Anteile (Atmen, Wind) herausfiltern.
Feine Pohren filtern. Das gro√üe Mikrofon, das an der Nordsee verwendet wird,
nennen die Tontechniker ‚Äútote Katze‚Äù.</p>

<h2 id="6-vorlesung">6. Vorlesung</h2>

<h3 id="geschichte-asr">Geschichte ASR</h3>

<p>Wie alt ist ASR? Nach der Kenntnis von Prof. St√ºker 1913 mit dem ‚ÄúVoice Operated Typewriter‚Äù.
Funktioniert analog mit elektromagnetischen Filtern. Kurzzusammenfassung des</p>

<p>Artikels: ‚ÄúVokale klappen, Konsonanten nicht, weil sie zu kurz sind‚Äù</p>

<p>Noch vor der ASR gab es Sprachsynthese.</p>

<p>1846: Euphonia - Sprachsynthesemaschine in England von Joseph Faber.
      Konstruiert wie ein Instrument, das den Vokaltrakt und eine Lunge nachstellt (√§hnlich Orgel).
      Konnt sogar ‚Äúfl√ºstern‚Äù und ‚Äúsingen‚Äù, dass k√∂nnen die aktuellen Systeme
      nicht so gut. Die Frau Faber hatte sogar Konzerte damit gegeben.
1922: Radio Rex - Ein Spielzeughund, der auf 500-Hz-Filterbank anspringt und dann
      aus seinem Haus gefedert wird. Das liegt so beim Buchstaben ‚Äúe‚Äù wodurch
      der Hund scheinbar auf das Wort ‚ÄúRex‚Äù herauskommt.
1939: Voice coder (Vocoder) - entwickelt von Dudley f√ºr Milit√§r. Aufteilung
      des Signals in Fenster (Abtastung), Gl√§ttung,
      √úbertragung (kann jetzt verschl√ºsselt sein) und zum Schluss wieder Synthese
      Dadurch konnte man Telefonie verschl√ºsseln f√ºr das Kampffeld.
1946: Visible Speech von Bell. Dieser kam sowieso aus dem Bereich der
      Arbeit f√ºr H√∂rgesch√§digte.
1965: Fast Fourier Transform (FFT). Sehr wichtiger Algorithmus, weil dadurch
      viele Aufgaben der Signalverarbeitung digital gel√∂st werden konnten.
      Oft sogar in Hardware gegossen.        <br />
1968: Dynamic Time Warping f√ºr Spracherkennung von Vintsyuk. Dieser hatte es zu
      diesem Jahr erfunden. Die Amis dachten bis zum Fall des Eisernen Vorhangs,
      das es bei ihnen zuerst erfunden wurde.
1971: DARPA SUR Projekt (bis 1976). Zwischen 69 und 71 hat ein sehr gro√üer
      Fortschritt stattgefunden. DARPA ist aus der Mondlandung entstanden und
      bekannt f√ºr viele Projekte: fr√ºhes Internet, Robotik, Autonome Autos
1975: Von Forschern aus DARPA SUR wurde HARPY gebaut. Diese nutzten
      Hidden Markow Models (HMM), was bis heute immernoch der Standard ist.
      Sie erzielten bereits auch schon eine gute Perplexit√§t, was ein
      Fehlerma√ü in Relation zu den bestehenden Auswahlm√∂glichkeiten ist.
1985: Kontextabh√§ngige HMMs. Danach wurden es immer mehr Daten und mehr
      Rechenleistung, die zur Verfeinerung der bestehenden Modelle genutzt wurden.
2000: Sprache-zu-Sprache (VERBMOBIL) https://www.youtube.com/watch?v=noZBab-Lmss
2006: GP-LVM und danach Emmissionswahrscheinlichkeit durch neuronale Netze ersetzen
2018: Trend geht in die Richtung, dass man HMM durch NN ersetzen will, aber
      State-of-the-Art ist weiterhin HMM.</p>

<h3 id="wer">WER</h3>

<p>Word Error Rate, Wortfehlerrate auf Deutsch. Es handelt sich um ein G√ºtema√ü
f√ºr die Funktionsf√§higkeit von ASR.
Dazu nimmt man eine transkripierte Audioaufnahme und vergleicht das Transkript
dann mit dem Ergebnis des ASR. Dazu nutzt man die Levenshtein-Distanz, welche
auch Minimale Editierdistanz (Minimum Edit Distance) genannt wird.
Diese bezeichnet die minimale Anzahl an Einf√ºgungen (INSERT), L√∂schungen (DELETION)
und Ersetzungen (SUBSTITUTION), welche man ben√∂tigt, um von der Hyptothese (Ergebnis des ASR)
zur Referenz aus dem Transkript zu kommen.</p>

<script type="math/tex; mode=display">WER = { #INS + #DEL + #SUBS \over N}</script>

<p>Wobei ‚Äú#‚Äù f√ºr die Anzahl an Operationen und N f√ºr die Anzahl an W√∂rtern in
der Referenz steht.</p>

<p>Die Wortakkuratheit ist $accuracy = 1-WER$.</p>

<p><strong>Wie findet man die minimale Editierdistanz?</strong> Dazu gibt es ein Verfahren mit
dynamischen Programmieren in quadratischer Zeit.</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/We3YDTzNXEk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>¬†</th>
      <th>H</th>
      <th>A</th>
      <th>L</th>
      <th>L</th>
      <th>O</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>¬†</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <td>H</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <td>A</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <td>U</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <td>S</td>
      <td>4</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<p>Die Zeile ist die Hypthese und die Spalte die Referenz. Wir wollen Hypthese
in Referenz umwandeln. Die Frage f√ºr Zelle X,Y ist quasi immer:
Wie viele Operationen ben√∂tige ich, um von meinem Wort aus der ersten Zeile bis
zum Buchstabe X eine Umwandlung in das Wort der ersten Spalte bis Y durchzuf√ºhren.</p>

<p>Man muss aber gar nicht mitdenken, wenn man die Regel kennt. Wenn die beiden
Buchstaben nicht gleich sind, dann nimmt man das Minimum von (links, oben oder links-oben)
und addiert eins dazu. Dadurch kann man die Tabelle stupide ausf√ºllen.</p>

<p>Der Wert der dann am Ende bei der vollen L√§nge in der Tabelle steht (unten rechts)
ist die Anzahl der n√∂tigen Operationen.</p>

<p><strong>Bemerkungen</strong></p>

<ol>
  <li>Die WER kann gr√∂√üer sein als 100%. Zum Beispiel kann das WER einfach sehr
viel mehr W√∂rter erkennen als im Satz enthalten sind.</li>
  <li>Die WER kann nicht negativ werden, da weder Nenner noch Z√§hler negativ werden k√∂nnen</li>
  <li>Es gibt mehrere Operationsfolgen, um die minimale Editierdistanz zu finden.</li>
</ol>

<p>Die WER als Fehlermetrik ist ganz nett, aber hat einige Haken. Bei maschineller
√úbersetzung zum Beispiel gibt es mehrere valide √úbersetzungen.
Oder wenn eine Segmentierung in W√∂rter nicht so leicht ist (Chinesisch Han-Schrift).
Oder wenn Fehler unterschiedlich zu bewerten sind. Zum Beispiel in einem medizinischen
Spracherkennungssystem ver√§ndert das Wort ‚Äúnicht‚Äù einfach alles, obwohl es
nur ein Wortfehler von eins ist.</p>

<p>NIST Benchmark Mai 2009 von ASR Systemen (St√ºker sagt, dass NIST gut messen kann.
  ‚ÄúSie betreiben die Atomuhr und pr√ºfen B√∂ller.‚Äù)</p>

<p>Fr√ºher hatte man Sprachverstehen und Spracherkennung noch miteinander behandelt.
Im Englischen: Speech Understanding and Recognition (SUR)</p>

<p>Am Anfang der 1990er war man dann bei einem 1000-Wort-Datensatz bereits auf
menschlicher WER (2 - 4%), also hat man neue, gr√∂√üere, schwierigere Datens√§tze
genutzt. Nicht nur Vorgelesenes, dann auch Meeting und freie Sprache.
Je gr√∂√üer das Vokabular desto gr√∂√üer auch der Fehler.</p>

<p>Die Testsets wurde nach Mikrofonaufbau unterschwieden, weil es einen erheblichen
Unterschied gemacht hat:</p>

<ul>
  <li>Kreis: Nahbesprechungsmikrofon</li>
  <li>Viereck: Single distant microphone</li>
  <li>Triangle: Multiple distant microphones (komplexe Zusammenf√ºhrung der Aufnahmen)</li>
</ul>

<p>Die Distanz macht gro√üe Probleme. Dieses Thema ist dann wieder mit Alex und HomePod
spannend geworden. Diese nutzen sog. Mikrofonarrays mit zum Beispiel acht
Mikrofonen. Die Mikrofon-Arrays (Triangle) aus dem Benchmark hatten auf eine
Distanz von circa 6-7 Metern 64 Mikrofone.</p>

<p>Ab 2000 hat man dann auch mal neue Sprachen hinzugef√ºgt und hatte dann bei
gleicher W√∂rterzahl h√∂here WER. Das lag daran, dass Mandarin zum Beispiel
tonalisch ist und das Schriftsystem anders, wodurch erst ein paar Jahre daran
gearbeitet werden musste.</p>

<p>WER k√∂nnen abh√§ngig von der Aufgabe √ºberall zwischen 4 - 90% sein.
Marketing ist hier oft irref√ºhrend.</p>

<h3 id="signalverarbeitung">Signalverarbeitung</h3>

<p>In Elektrotechnikb√ºchern nach ‚ÄúSystemtheorie‚Äù suchen, um Hintergrundwissen
zu bekommen.</p>

<p>Kurz und knapp: Warum Signalverarbeitung in der ASR? Unwichtiges Rausschmei√üen
und wichtiges Hervorheben.
Wenn das System n√§mlich mit Rauschen oder redundanten Signalen antrainiert wird,
dann lernt es diese im schlimmsten Fall und hat dann weniger Parameter √ºbrig,
um andere Merkmale zu lernen.</p>

<p>Beispiele f√ºr zu entfernende Signale:</p>

<ul>
  <li>Sprecheridentit√§t</li>
  <li>Akustischer Raum (Hall, absolute Lautst√§rke)</li>
  <li>Mikrofon</li>
</ul>

<p>H√§ufig geht das nicht. Manche Dinge bekommt man aber ganz gut raus, z. B.:</p>

<ul>
  <li>Lautst√§rkeschwankung</li>
  <li>Bestimmte Sprecheridentit√§ten</li>
</ul>

<p>Es geht jetzt in diesem Teil der Vorlesung darum, sich einen
<strong>Baukasten der Signalverarbeitung</strong> zuzulegen.</p>

<h4 id="system">System</h4>

<p>Ein System T wandelt ein eingehendes Signal in ein anderes um. Bei uns handelt
es sich um eine diskrete Folge von Werten (digital): <script type="math/tex">y[t] = T \{ X[n] \}</script></p>

<p>Einfaches Beispiel: Zeitliche Verz√∂gerung $y[n] = x[n - n_d]$ wobei $n_d$ den
Delay (Verz√∂gerung) definiert.</p>

<p>Weiteres Beispiel: Moving Average</p>

<script type="math/tex; mode=display">y[n] = {1 \over M_1 + M_2 + 1} \sum_{k = -M1}^{k = M2}{x[n-k]}</script>

<p>Ein lineares System kann ‚Äúinnen oder au√üen‚Äù skaliert werden:
<script type="math/tex">T \{ a_1 x_1[n] + a_2 x_2 [n] \} = a_1 T \{ x_1[n] \} + a_2 T \{ x_2[n] \}</script></p>

<p>Das waren Beispiele f√ºr lineare Systeme (LS). Wenn ein LS <strong>zeitinvariant</strong> ist,
dann nennt es sich <strong>Linear Time-Invariant</strong> system. Wenn die Eingabezeit
verz√∂gert wurde, dann soll das Ergebnis der Transformation trotzdem das gleiche sein.</p>

<p>Ein System hei√üt zeitinvariant, wenn gilt:
$ y_1[n] = y[n - n_d] $</p>

<p>Mit diesen kann sehr gut gerechnet werden.</p>

<h4 id="dirac-sto√ü">Dirac Sto√ü</h4>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/jzvkqV1Rf5E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Es handelt sich bei der Dirac-Distribution nicht um eine Funktion, sondern eine
Distribution.</p>

<script type="math/tex; mode=display">\delta ( f ) = \int_{-\infty}^{+\infty}{\delta (x) f(x) \, dx} = f(0)</script>

<p>d(x) ist der Diracsto√ü am Zeitpunkt x. Welcher bei x unendlich gro√ü ist und sonst null.
Zus√§tzlich gilt die Einschr√§nkung, dass die Fl√§che unter d(x) 1 sein soll,
was in dieser Kombination keine Funktion auf den reellen Zahlen bieten kann.</p>

<p>Ist eigentlich nicht integrierbar, weil d(x) nicht integrierbar.
Kann aber √ºber die Diracfolge ausgedr√ºckt
werden:</p>

<script type="math/tex; mode=display">\delta ( f ) = \lim_{\epsilon \to 0} \int_{-\infty}^{+\infty}{\delta_\epsilon (x) f(x) \, dx} = f(0)</script>

<p>Man kann den Dirac-Sto√ü aber als Verteilung darstellen und zwar zum Beispiel
durch eine Gau√üglocke, die bekannterma√üen die Fl√§che 1 hat. Wenn man die
Varianz nun gegen 1/unendlich gehen l√§sst, dann erh√§lt man folgende Ann√§herung:</p>

<p><img src="/assets/images/Dirac_function_approximation.gif" alt="" /></p>

<p>Entsprechend wird f√ºr das Rechnen angenommen:</p>

<script type="math/tex; mode=display">\int_{-\infty}^{+\infty} a \cdot \delta (x) = a</script>

<p>Das Integral eines Diracsto√ües multipliziert mit einer Funktion g(x) = g(0).
Wenn man Zeit nun verschiebt, dann kommt man auf die Siebeigenschaft:</p>

<script type="math/tex; mode=display">\int_{- \infty}^{+ \infty} \delta(x-\tau) \cdot g(x) dx = g(\tau)</script>

<p>Im diskreten Fall schreibt man:</p>

<p>$\delta[n] := 1 $ wenn $n = 0$, sonst $0$</p>

<p>Siehe <a href="https://de.wikipedia.org/wiki/Dirac-Kamm">Dirac-Kamm</a>.</p>

<h4 id="faltung">Faltung</h4>

<p>Mit der Faltung kann man die Wirkung eines Kanals auf ein Signal modellieren.
Sie ist salopp gesagt die Wirkung, die ein Signal auf ein anderes hat.</p>

<p>Sie ist:</p>

<ul>
  <li>Kommutativ: f * g = g * f</li>
  <li>Assoziativ: f * (g * h) = (f * g) * h</li>
  <li>Distributiv: f * (g + h) = f * g + f * h</li>
  <li>Assoziativ mit Skalar: a (f * g) = (a f) * g = f * (a g)</li>
</ul>

<p>Eine Faltung entspricht einer Multiplikation im Frequenzbereich.</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/_UFyMWDoISk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="impulsantwort">Impulsantwort</h4>

<p>Die Impulsantwort, auch Gewichtsfunktion oder Sto√üantwort genannt, ist das Ausgangssignal eines Systems, bei dem am Eingang ein Dirac-Impuls zugef√ºhrt wird. Definiert als:</p>

<script type="math/tex; mode=display">h_k [n] = T{ \delta [ n-k ] }</script>

<p>Wenn y[n] = T {x[n]}, dann gilt durch die Siebeigenschaft:</p>

<script type="math/tex; mode=display">y[n] = T {\sum_{k=-\infty}^{+ \infty}x[k]\delta[n-k]}</script>

<p>Wenn T linear, dann:</p>

<script type="math/tex; mode=display">y[n] = \sum_{k=-\infty}^{\infty} x[k]T\{  \delta[n-k] \}
= \sum_{k=- \infty}^{\infty} x[k] h_k [n]</script>

<p>Wenn T auch noch zeitinvariant (-&gt; LTI):</p>

<script type="math/tex; mode=display">y[n] = \sum_{k= - \infty}^{\infty} x[k] h[n-k] = x[n] * h[n]</script>

<p>Das hei√üt, dass die Ausgabe eines LTI Systems als eine Faltung des angelegten
Signals mit der Impulsantwort berechnet werden kann.</p>

<p>Beispiel:
Ein Raum mit Mikrofon sei ein LTI. Ich erzeuge eine Ann√§herung an den Dirac-Sto√ü,
wie zum Beispiel ein ‚ÄúChirp‚Äù, welches in einer Sekunde alle Frequenzbereiche
einmal durchgeht.
Beim aufgenommenen Signal tut man jetzt so, als sei das die Impulsantwort.</p>

<h4 id="quelle-filter-modell">Quelle-Filter-Modell</h4>

<p>Die Quelle erzeugt ein Schallsignal mit einem bestimmten Spektrum (stimmhafte Laute oder wei√ües Rauschen durch Atmen),
das bis zur Abstrahlung an den Lippen einen Resonator (den Vokaltrakt) durchl√§uft,
welcher das Signal mit einer bestimmten √úbertragungsfunktion verformt.
Da es sich auch hier um ein lineares System handelt, kann man das Spektrum des
abgestrahlten Sprachsignals dadurch erhalten, da√ü man das Spektrum der Quelle
mit der √úbertragungsfunktion des Filters multipliziert.
Ein anderer - aber mathematisch √§quivalenter - Weg ist die Faltung des
Quellesignals mit der Impulsantwort des Filters im Zeitbereich.</p>

<p>Dabei ist zu beachten, da√ü es sich bei der Quelle nicht nur um die stimmhaft
angeregte Glottis handeln mu√ü. Ebenso kann der Resonator, der Hohlraum im Vokaltrakt,
je nach Lage der Quelle sehr komplexe Formen annehmen (z.B. bei Ankopplung des Nasenraums).</p>

<p>Also:
Schallsignal $u_n$ wird erzeugt und durch Vokaltrakt v und Lippen r moduliert.
Das kann man als eine Faltung des Signals sehen.
Was wir dann messen am Mikrofon ist: $f_n = u_n * v_n * r_n$</p>

<p>Um an das Originalsignal zu kommen m√ºsste man die Faltung invertieren, was
schwer ist.</p>

<h2 id="7-vorlesung">7. Vorlesung</h2>

<p>schreiben</p>

<p>12.11.2018</p>

<h3 id="fourier">Fourier</h3>

<p>schreiben</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/spUNpyF58BY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="dft">DFT</h3>

<p>schreiben</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/h6QJLx22zrE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/mkGsMWi_j4Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="fft">FFT</h3>

<p>schreiben</p>

<p>Ein Teile-und-Hersche-Algorithmus zur Berechnung der DFT.
Wird oft in Hardware gegossen.</p>

<p>ToDo: Wie funktioniert es?</p>

<h2 id="8-vorlesung">8. Vorlesung</h2>

<p>schreiben</p>

<p>14.11.2018</p>

<p>Aufzeichnungen fehlen mir‚Ä¶</p>

<h2 id="9-vorlesung">9. Vorlesung</h2>

<p>schreiben</p>

<p>19.11.2018</p>

<h3 id="formanten">Formanten</h3>

<p>Die Resonanzfrequenz f des Helmholtz-Resonator h√§ngt von der Gr√∂√üe der √ñffnung S
und dem Gasvolumen V ab.</p>

<p>$ f = ~ sqrt{S}$
$ f = ~ { 1 \over sqrt{V} }$</p>

<p>Die verst√§rkten Resonanzfrequenzen nennt man <strong>Formanten</strong>. Sie sind als
starkes Auftreten der Oberschwingung sichtbar im Spektrogramm.
Sie entstehen durch eine Selbstverst√§rkung durch Inferenzen im Helmholtz-Resonator.</p>

<p>Die niedrigste Frequenz <strong>F1</strong> nennt sich erster Formant.
Tr√§gt man F1 und F2 f√ºr die amerikanischen Vokale auf, dann bekommt man das
<strong>Vokaldreieck</strong>. Es hei√üt so, da sich alle Vokale in einem Dreieck befinden.
Das sollte man nicht verwechseln mit dem Vokalviereck. Die Bestimmung von F1 und
F2 ist manuell geschehen.
Die Frage war dann damals, ob man Vokale anhand F1 und F2 erkennen kann.
Sie sind zwar geclustert, aber nicht 100% scharf abgetrennt und zudem ist die automatische
Identifikation von F1 und F2 schwer, da man bei einer falschen F1-Erkennung
einen hohen Fehler erh√§lt.</p>

<h3 id="lpc">LPC</h3>

<p>Linear Predictive Coding also lineare Vorhersage.</p>

<p>Nach dem Quelle-Filter-Modell kann man Sprache als LTI sehen:</p>

<script type="math/tex; mode=display">s[n] = - \sum_{k=1}^{p}{a_k s[n-k] + e[n]}</script>

<p>Das hei√üt, dass sich das zuk√ºnftige Signal als Kombination der alten Werte und
einem Fehler zusammensetzt.</p>

<p>Die Z-Transformierte der Impulsantwort des LTI:</p>

<script type="math/tex; mode=display">H(z) = {1 \over 1 - \sum_{k=1}^{p}{a_k z^{-k}}}</script>

<p>Es handelt sich um einen Quotienten aus Polynomen, wobei der Z√§hler nur eine
eins ist. Deswegen gibt es keine Nullstelle.
Der Nenner kann Polstellen hervorbringen. Da das Modell nur Pole und keine Nullstellen
hat, nennt es sich <strong>All-Pole-Modell</strong>.</p>

<p>Wie findet man die $a_k$?
Daf√ºr muss man den Fehler minimieren: $e[n] = s[n] - \widetilde{s}[n]$</p>

<p>Wir setzen $a_o = 1$. So kann man die LPC-Koeffizienten $a_k$ finden.
Diese werden gerne in der Spracherkennung als Merkmale verwendet.</p>

<p>Die Werte der Z-Transformierten auf dem komplexen Einheitskreis approximieren
das Spektrum. Das Problem: Nasale haben Interferenzen zwischen Abstrahlern, die
sich eliminieren k√∂nnen, dadurch entstehen sich ausl√∂schende Schwingungen.</p>

<p>Das ist ung√ºnstig, da wir im All-Pole-Modell keine Nullstellen darstellen k√∂nnen.</p>

<p>Vergleicht man die LPC-Koeffizienten mit DFT-Koeffizienten, dann sind man, dass
LPC eine Gl√§ttung des Spektrums bewirkt.</p>

<p>Es gibt zwei Probleme damit:</p>

<ul>
  <li>Es gibt keine Nullstelle</li>
  <li>Wie w√§hlt man die Aufl√∂sung richtig (Anzahl Koeffizienten)</li>
</ul>

<h3 id="cepstrum">Cepstrum</h3>

<p>Wortspiel mit Spektrum.</p>

<script type="math/tex; mode=display">Cep(f) = FT^{-1}(log FT(f))</script>

<p>Durch die einmalige Anwendung der Fouriertransformation gelangt man in den
Frequenzbereich. Durch die Anwendung der Inversen FT jedoch nicht wieder
zur√ºck in den Zeitbereich, sondern in den Cepstral-Bereich.</p>

<p>Durch Anwendung des Kontinuierlichen und monotonen Logarithmus wird aus der
Multiplikation eine Addition.</p>

<p>Unter der Annahme, dass das Zeitsignal eine Faltung unterschiedlicher Signale
ist, kann man das Cepstrum ‚Äúentfaltend‚Äù nennen, wodurch man auf das urspr√ºngliche
Signal kommt.</p>

<script type="math/tex; mode=display">f = e * h
FT(f) = FT(e) \cdot FT(h)
log(FT(f)) = log(FT(e)) + log(FT(h))
FT^{-1}(log(FT(f))) = FT^{-1}(log(FT(e))) + FT^{-1}(log(FT(h)))</script>

<p>Wenn e also sch√§tzbar, dann kann man es abziehen. Daf√ºr muss man die Impulsantwort
von e errechnen. e ist aber eher unbekannt.</p>

<p>Das Cepstrum ist eigentlich eine komplexe Funktion. Das darstellbare Cepstrum
(‚Äúreal‚Äù) kann man als FT des Leistungsspektrums sehen. Die Einheit der
x-Achse nennt man Quefrenz.</p>

<p>Vielfaches der Grundfrequenz wiederholt sich immer wieder. Das ist sichtbar
im Quefrenz-Bereich.</p>

<p><strong>Wichtig:</strong> Die Signalenergie flie√üt nur in die Berechnung des 0-ten Koeff. ein.
Daher wird er h√§ufig durch die Signalenergie ersetzt. Das hei√üt, dass das
Signal bis auf den 0-ten Koeff. <strong>lautst√§rkenagnostisch</strong> ist.</p>

<h4 id="liftering">Liftering</h4>

<p>Die unteren Koeffizienten des Cepstrums beschreiben die Makrostruktur des Signals.
Die oberen die Mikrostruktur. Die oberen Koeffizienten setzt man auf Null und
nennt das in Anlehnung an ‚Äúfiltering‚Äù <strong>liftering</strong>.</p>

<p>Meistens w√§hlt man 13 Koeffizienten zur Weiterverarbeitung.</p>

<p><strong>H√§ufiger Verarbeitungsablauf:</strong></p>

<p>Mel-skalierte Cepstral-Koeffizienten:</p>

<ul>
  <li>Signal</li>
  <li>-&gt; FT</li>
  <li>Spektrum</li>
  <li>-&gt; Spektrum-Mel-Skalierung</li>
  <li>Mel-Spektrum</li>
  <li>-&gt; $FT^{-1} (log(\cdot))$</li>
  <li>Mel-Cepstrum</li>
</ul>

<p>Man spricht auch von Mel-Frequez-Cepstrum und den Mel-Frequnz-Cepstrum Coeffients (MFCC)</p>

<h3 id="dct">DCT</h3>

<p>In der Praxis wird statt der $FT^{-1}$ eine <strong>Diskrete Cosinus-Transformation (DCT)</strong>
verwendet.</p>

<p>DCT-II ist definiert als:</p>

<script type="math/tex; mode=display">C[k] = \sum_{n=0}^{N-1}{x[n]cos(\pi k (n + 1/2)/N)} \text{ mit } 0 \leq k \le N</script>

<p>Die Inverse der DCT-II ist die DCT-III:</p>

<script type="math/tex; mode=display">x[n] = {1 \over n} \{  { C[0] + 2 \sum_{k=1}^{N-1}{C[k]cos(\pi k (n + 1/2)/N)}} \}</script>

<p>Bei JPEG findet dieses Verfahren ebenfalls Anwendung. Man entfernt ebenfalls
die hohen Frequenzanteile und kann dann mit einer d√ºnnbesetzten Matrix
ein Huffman-Coding.</p>

<h3 id="mel-filterbank">Mel-Filterbank</h3>

<p>Durch Anwendung einer Mel-Filterbank auf ein Spektrogramm, die die h√∂heren Frequenzen ungenauer
erfasst als die niedrigen, reduziert man die Anzahl der Koeffizienten von
zum Beispiel 8000 auf 40.</p>

<p>Durch das Cepstrum mit Liftering entsteht eine Art Gl√§ttung im
rekonstruierten Leistungsspektrum.</p>

<h3 id="typ-vorverarbeitung">Typ. Vorverarbeitung</h3>

<ul>
  <li>Digitales Signal (16 kHz Abtastrate, 16 Bit Aufl√∂sung)</li>
  <li>Fensterung (Hamming-Fenster mit 16ms Breite, 10ms Verschiebung) -&gt; pro Fenster (Frame) 256 Abtastwerte/Samples</li>
  <li>DFT: 256 komplexe Werte</li>
  <li>Betragsspektrum: 256 reelle, symmetrische Werte -&gt; nur die H√§lfte n√∂tig: 128 reelle Werte</li>
  <li>Mel-Skalierung auf 24-40 Werte</li>
  <li>Logarithmus: 24 - 40 Werte</li>
  <li>DCT: 24-40 Werte</li>
  <li>Liftering: 13 Werte</li>
</ul>

<h3 id="dynamische-merkmale">Dynamische Merkmale</h3>

<p>Spektrum und Cepstrum bilden nur statische Eigenschaften ab, aber der Verlauf
der Formanten verl√§uft einer Dynamik. Wohin bewegt sich ein Formant?</p>

<p>Daf√ºr approximiert man zum Beispiel die 1. und 2. Ableitung im Diskreten durch
Differenz zu vorherigem Frame. Es werden also aus 13 Mel-Cepstral-Koeffizienten
39 Werte.</p>

<h4 id="autokorrelation">Autokorrelation</h4>

<script type="math/tex; mode=display">A(t) = \sum_{i}^{}{s[t] - s[i-t] }</script>

<p>t beschreibt die Verschiebung des Signals. Mit diesem verschobenen Signal vergleicht man das Signal selbst.
Bei t=0 liegt die obere Grenze, weil das Signal mit sich selbst maximal √§hnlich ist.
Dieser Wert ist proportional zur Leistung des
Signals. Es treten Spitzen auf, wo Wiederholungen sind</p>

<p>Die Grundfrequenz macht sich im Signal durch Oberschwingungen mit der Frequenz
bemerkbar. Die erste Spitze ist die Grundfrequenz, aber nicht so gut erkennbar.
Wenn man hier bei der Entfernung einen Fehler macht, dann einen gro√üen.
Dagegen hilft Gl√§ttung, weil menschliche Stimme nicht enorm springt.
Warum will man die Grundfrequenz haben? Zum Beispiel wichtig f√ºr <strong>tonale Sprachen</strong>
und <strong>Prosodieanalysen</strong>.</p>

<h4 id="nulldurchgangsrate">Nulldurchgangsrate</h4>

<p>Wie oft wechselt das Signal das Vorzeichen?
Weil bei Stille eine geringere Nulldurchgangsrate als bei Sprache vorliegt,
kann man so die beiden voneinander trennen.</p>

<h3 id="dim-red">Dim Red</h3>

<p>Dimensionanalit√§t reduzieren</p>

<h4 id="pca">PCA</h4>

<p>Hauptkomponentenanalyse: Projiziere Merkmale in einen niederdimensionalen Raum
und erhalte dabei so viel Varianz wie m√∂glich.
Zum Beispiel durch Berechnung der Eigenvektoren der Kovarianzmatrix.
Diese Eigenvektoren nutzt man dann als neue Hauptachsen.</p>

<p>Die Kovarianzmatrix wird diagonalisiert bei der PCA, was den Effekt hat, dass die
Merkmalsvektoren dekorreliert werden.</p>

<p>Ein Paradebeispiel f√ºr einen Datensatz, der durch die PCA nicht nicht besser
trennbar wird, ist das <strong>Adidas-Problem</strong>.</p>

<h4 id="lda">LDA</h4>

<p>Dieses Problem kann durch die <strong>Lineare Diskriminanz Analyse</strong> gel√∂st werden.
Bei dieser Methode sind die Klassenzugeh√∂rigkeiten der Datenpunkte bekannt und
es wird angestrebt die einzelnen Klassen maximal voneinander und in sich
kompakt zu halten.</p>

<p>Die Klassenseparabilit√§t einer Klasse
<script type="math/tex">S = { \sigma_{between}^2 \over \sigma_{within}^2 }</script>
soll maximiert werden.
Danach <strong>Kullback‚ÄìLeibler-Divergenz</strong> maximieren. Es handelt sich um ein Ma√ü f√ºr
die Unterschiedlichkeit zweier Wahrscheinlichkeitsverteilungen.</p>

<p>Wird in der Praxis verwendet.</p>

<h4 id="nn">NN</h4>

<p>Vorverarbeitung durch Neuronale Netze. Interpretation der Ausgabe als posteriori
Wahrscheinlichkeit von Phonemen, Polyphonen oder Sub-Phoneme.</p>

<p>Man nutzt dabei ein gr√∂√üeres Fenster (200-500 ms) einer herk√∂mmlichen Vorverarbeitung,
zum Beispiel Cepstrum.</p>

<p>Also auch wieder als eine Vorverarbeitung. Und dann wieder alle Daten zusammenf√ºhren
in einem gro√üen Vektor. Darauf kann man dann wieder LDA anwenden.</p>

<h4 id="bottleneck-features">Bottleneck Features</h4>

<p>Man kann das Neuronale Netz auch benutzen, um eine niederdimensionale Repr√§sentation
erhalten. Daf√ºr f√ºhrt man einen Flaschenhals in die Topologie ein und trainiert
weiterhin auf Klassifizierung in Phoneme etc.
Dann nutzt man die Aktivierungen im Flaschenhals als Feature.</p>

<h3 id="klassifikation">Klassifikation</h3>

<p>Einteilung von <strong>Pattern-Recognition</strong> verfahren:</p>

<ul>
  <li>Statistical
Antworten haben eine Wahrscheinlichkeit. Nimm z.B. das Maximum als Antwort.
    <ul>
      <li>unsupervised (z. B. Gaussian-Mixture)</li>
      <li>supervised (z. B. k-means)
        <ul>
          <li>parametric (Wahrscheinlichkeit einer Dichtefunktion)</li>
          <li>nonparametric (z. B. Parlson-Fenster)
            <ul>
              <li>linear</li>
              <li>nonlinear</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Knowledge/Connectionism (z.B. Automat)
Sammle Wissen √ºber die L√∂sung eines Problems und repr√§sentiere es (z. B. Entscheidungsbaum, Regeln)
 und triff dann eine ‚Äúharte‚Äù Entscheidung</li>
</ul>

<h2 id="10-vorlesung">10. Vorlesung</h2>

<h3 id="klassifikation-1">Klassifikation</h3>

<h4 id="bayes">Bayes</h4>

<p>Das Standardbeispiel f√ºr ein √ºberwachtes, parametrisches Lernverfahren ist der
Bayes-Klassifikator. Damit ist Dr. St√ºker aber unzufrieden, weil Bayes erstmal
keine Parameter ben√∂tigt. F√ºr gew√∂hnlich verwendet man dieses Klassifikator mit
einer Gau√ü-Verteilung, daher kommen die Parameter.</p>

<p>Man kann sagen, dass ein heutiges Spracherkennungssystem ein Bayes-Klassifikator
mit komplizierter Wahrscheinlichkeitsdichtefunktion (bzw -verteilung), Faltung
und Rekurrenz ist.</p>

<p>Er nutzt die a-priori W‚Äôkeit und klassenbedingte W‚Äôkeit um posteriori
W‚Äôkeit zu berechnen.</p>

<p>Die klassenbedingte Wahrscheinlichkeit bezeichnet man auch als <strong>Likelihood.</strong></p>

<script type="math/tex; mode=display">P(\omega_i \| x) = { P(x \| \omega_i) P(\omega_i) \over P(x)}</script>

<p><strong>Minimum Fehler Regel</strong>
Der naive Bayes-Klassifikator will Wahrscheinlichkeit f√ºr Fehler minimieren.
Das hei√üt, man will den Z√§hler maximieren, um posteriori zu maximieren.</p>

<script type="math/tex; mode=display">P(x) = \sum_i P(x\| \omega_i) P(\omega_i)</script>

<p>Man sucht mit diesem Verfahren eine optimale Trennlinie zwischen Verteilungen.
Diese hei√üt auf Englisch <strong>Optimal Decision Boundary</strong>.</p>

<h4 id="parzen-fenster">Parzen-Fenster</h4>

<p>Dies ist ein Beispiel f√ºr ein nicht parametrische Sch√§tzung einer Wahrscheinlichkeitsfunktion.
Zu jedem Punkt im Raum z√§hle die Samples in einem gew√§hlten Volumen, teile sie
durch die Gesamtanzahl der Punkte und durch das Volumen.</p>

<h3 id="asr-pattern">ASR Pattern</h3>

<p>Spracherkennung mit Musterklassifikation. Der Gedanke dabei ist, dass man zum
Beispiel f√ºr jedes Wort ein Referenzmuster hat und dann eine Mustererkennung
auf den Eingabedaten durchf√ºhrt.</p>

<p>Die Reihenfolge der Merkmalsvektoren ist wichtig, weil Sprache linear in der
Zeit l√§uft. W√§hle das Referenzmuster mit der geringsten Fehlerdistanz.</p>

<p>Naiver Ansatz: Normalisiere die Zeit auf die k√ºrzeste √Ñu√üerung. Jetzt ist lineare
Zuordnung m√∂glich. Sobald die Sprechgeschwindigkeit nicht konstant war funktioniert
das wieder nicht.</p>

<p>Sprache ist aber nicht von gleicher L√§nge. Es variiert die Sprechgeschwindigkeit.
Sprache ist bestenfalls ‚Äúst√ºckweise‚Äù linear.</p>

<p>Das hei√üt, die Abbildung, die wir suchen ist nicht bijektiv, nicht injektiv und
auch nicht surjektiv. Vergleiche Alignment aus Maschinellem √úbersetzen. Das ist
auch nur eine Relation.</p>

<h3 id="time-warp">Time Warp</h3>

<p>Gegeben: Sequenzen $x_1 ‚Ä¶ x_n$ und $y_1 ‚Ä¶ y_m$
Gesucht: Relation R mit (i, j) in R wenn $x_i \to y_j$ abbildet.</p>

<p>Das hei√üt, die Zeitachsen von Referenz und Muster werden auf eine gemeinsame
Zeitachse gebracht.</p>

<p><img src="/assets/images/dtw.gif" alt="" />
Figur 11.1 aus <a href="http://web.science.mq.edu.au/~cassidy/comp449/html/ch11s02.html#dtw.figure">Comp449</a></p>

<p>F√ºr einen gegebenen Pfad R(i,j) ist die Summe aller lokalen Distanzen seine
Distanz zwischen x und y. Wie findet man aber den Pfad?</p>

<p>√Ñhnlich der minimalen Editierdistanz zusammen mit Dynamischer Programmierung.
Daf√ºr ben√∂tigt man eine DP-Matrix, die die minimalen Editierschritte von $x_1 ‚Ä¶ x_n$
zu $y_1 ‚Ä¶ y_n$ enth√§lt. F√ºr jeden Zustand merkt man sich, welcher der beste
vorherige Zustand war (<strong>Backpointer</strong>) und dann nutzt man backtracking, um die
Sequenz der Editierschritte zu erhalten.</p>

<h3 id="dtw">DTW</h3>

<p>Dynamic Time Warp kann √§hnlich gel√∂st werden. Der Zielpfad soll aber einige
Einschr√§nkungen haben, die wir aus der Sprache ableiten:</p>

<ul>
  <li>Der Pfad darf keine signifikanten Teile am Anfang und Ende auslassen</li>
  <li>Da Sprache zeitlinear ist, muss der Pfad monoton steigen</li>
  <li>Lokale Kontinuit√§t: Sprache springt nicht zu stark</li>
  <li>N√§he zur Diagonalen</li>
  <li>Pfad sollte glatt verlaufen, da die Sprechgeschwindigkeit nicht beliebig ist</li>
</ul>

<p>Welche Schritte sind denkbar?</p>

<ul>
  <li>symmetrische DTW-Schritte (wie bei Editierdistanz)</li>
  <li>Bakis: vertikal +2 und horizontal +1 -&gt; Sprung</li>
  <li>Itakura: Verbot zweier horizontaler Schritte</li>
</ul>

<p>Es ist ziemlich viel denkbar‚Ä¶</p>

<p><strong>Diagonalfenster</strong>
Gem√§√ü der Regeln, dass man Anfang und Ende nicht auslassen soll und dass N√§he
zur Diagonalen gew√ºnscht ist, kann man den <strong>Suchraum</strong> zum Beispiel auf ein
Parallelogramm eingrenzen, das die Diagonale beinhaltet.
Und dann gestattet man einfach keine Schritte, die von au√üen nach innen gehen w√ºrden.
Problem damit: Wenn Stille in der Mitte des Wortes, dann macht man einen Knick nach
oben und gelangt vielleicht hinaus. Und bei Start k√∂nnte der Suchraum zu klein sein.
Vorteil: O(n) statt O(n¬≤)</p>

<p>Um die Probleme zu beheben: <strong>Strahlsuche</strong>
Idee: Ziehe nur diebesten Zust√§nde in Betracht sortiert nach kumulativem Gewicht..
Dabei kann man die Grenze unterschiedlich ziehen:</p>

<ul>
  <li>konstanter Strahl: nimm die besten k-Pfade</li>
  <li>relativer Strahl: nimm alle, die nur x% schlechter sind als bester Pfad</li>
  <li>absoluter Strahl: nimm alle, die um weniger als x schlechter sind als bester Pfad</li>
  <li>Kombination aus beiden: Maximale Grenze nach oben und abh√§ngig vom aktuell besten</li>
</ul>

<p>Eine gro√üe Frage ist, wie beschreiben wir die Distanz zweier Vektoren?</p>

<p>$d(x,y)$ mit</p>
<ul>
  <li>L2 $\sqrt(\sum_{i=1}^{n}{(x_i-y_i)^2})$</li>
  <li>L1 $\sum_{i=1}^{n}{| x_i - y_i |}$</li>
  <li>Mahalanobis-Distanz: x und y werden als mehrdimensionale Vektoren auf einer
Zufallsvariablen mit Varianz S interpretiert. Wenn S Einheitsmatrix -&gt; normale L2-Distanz,
ansonsten: $\sqrt{(x-y)^{T} S^{-1}(x-y)}$</li>
</ul>

<p>Eine wichtige Frage ist also, ob das Distanzma√ü Varianz innerhalb einer Dimension und
zwischen den Dimensionen betrachtet.</p>

<p><strong>DTW √ºberwacht?</strong> Ja.</p>

<p><strong>DTW parametrisch?</strong> H√§ngt vom Distanzma√ü ab.</p>

<p>Wenn man mehrere Trainingsbeispiele als Referenz hat, dann kann man entweder
die Trainingsbeispiele vermischen oder alle benutzen und beim Mustervergleich mitteln.</p>

<p>DTW wurde f√ºr die Spracherkennung bei alten Handys oder im Embedded Bereich verwendet.
Die Wahl der Parameter muss gesucht werden. Auch das kann als Lernverfahren
betrachtet werden (Grid Search).</p>

<h2 id="11-vorlesung">11. Vorlesung</h2>

<p>26.11.2018</p>

<p>DTW ist Sprecherabh√§ngig und nur f√ºr kleines Vokabular.</p>

<h3 id="dtw-osdp">DTW OSDP</h3>

<p>One Stage Dynamic Programming ist die Erweiterung von DTW auf eine Sequenz von
W√∂rtern. Das hei√üt, es gibt dann in-word und cross-word transitions.
Daf√ºr muss die zu testende Aufnahme nicht segmentiert werden.</p>

<p>An der Y-Achse werden alle Referenzw√∂rter und an der X-Achse die Aufnahme angetragen.
Wenn die obere rechte Ecke eines Referenzwortes erreicht wurde, dann darf zu den
Anf√§ngen von allen Referenzw√∂rtern gesprungen werden (inklusive dem Anfang der
  gleichen Box).</p>

<p>Ein positiver Nebeneffekt ist, dass die Wortsegmentierung quasi nebenbei entsteht.</p>

<p>F√ºr die optimale L√∂sung muss man einige DP-Matrizen berechnen: $O(N \cdot M^{k_{max}} \cdot K)$.</p>

<p>In der Praxis geringer wegen Strahlsuche oder Einschr√§nkung des Suchraums.</p>

<p>Den Speicherbedarf kann man von R√ºckzeigern und Distanzen jeder Spalte reduzieren,
weil der genaue Verlauf der Pfade unwichtig sein kann, sondern nur die Wortsequenz.</p>

<p>Man braucht eigentlich nur zwei Spalten und eine Liste von genommenen √úberg√§ngen
 (‚ÄúR√ºckzeiger bei Wort√ºberg√§ngen‚Äù) .</p>

<p>Syntaktische Einschr√§nkung:
Jedes Wort hat eine Menge erlaubter Vorg√§nger. Das reduziert auch den Suchaufwand.
Und wenn man die Vorg√§ngermenge abh√§ngig machen m√∂chte vom aktuellen Zustand,
dann dupliziere W√∂rter bis man wieder nur eine Vorg√§ngermenge braucht.</p>

<p>Man h√§tte damals komplizierte Grammatiken aufbauen k√∂nnen, aber man hat sich
lieber auf die stochastische Abstandsbemessung konzentriert.</p>

<h3 id="gau√ü">Gau√ü</h3>

<p>Die Gau√üverteilung wird auch Normalverteilung genannt.
Sie ist wie folgt definiert:</p>

<script type="math/tex; mode=display">p(x) = {1 \over \sqrt{ 2 \pi \sigma^2 } e^{- 1 (x - \mu)^2 \over 2 \sigma^2}</script>

<p>Mittelwert (1. Moment) ist $\mu$
Varianz (2. Moment) ist $\sigma^2=E((X-\mu)^2)$, wobei $\sigma$ Standardabweichung genannt wird</p>

<p>Wenn man Trainingsdaten X betrachtet und annimmt, dass sie gau√üverteilt sind, also
$X ~ \mathcal{N}$, dann kann man den Mittelwert und die Standardabweichung
sch√§tzen, indem man sie auf den Trainingsdaten berechnet.</p>

<p>In realen Anwendungen sind die Merkmale jedoch h√∂herdimensional
-&gt; <strong>multivariate Gau√üverteilung</strong></p>

<p>Wenn n Trainingsdaten X d-dimensional, dann:</p>

<script type="math/tex; mode=display">p(x) = {1 \over \sqrt{ (s \pi)^d det(\Sigma)}} e ^ { - 1 (x - \mu)^T \Sigma^{-1} (x - \mu) \over 2 }</script>

<p>Wobei $\Sigma$ die Kovarianzmatrix ist: <script type="math/tex">Cov(X) = (Cov(X_i, X_j))</script> f√ºr i, j= 1‚Ä¶n</p>

<ul>
  <li>sie ist symmetrisch und positiv (semi-)definit</li>
  <li>wird als Ellipsoid dargestellt</li>
  <li>die Diagonaleintr√§ge sind f√ºr ‚Äúechte‚Äù Normalverteilungen gr√∂√üer 0</li>
  <li>Wenn alle Werte au√üerhalb der Diagonalen Null, dann sind die Achsen des Ellipsoiden
parallel zu den Hauptachsen und das hei√üt, dass die Dimensionen von X dekorreliert sind</li>
</ul>

<h3 id="gmm">GMM</h3>

<p>Gaussian Mixture Model bzw. Gau√ü-Mischverteilung</p>

<p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture">Visualisierung</a></p>

<p>Viele reale Prozesse sind aber nicht normalverteilt. Menschliche Sprache auch nicht.</p>

<p>Daf√ºr n√§hert man die gesucht Wahrscheinlichkeitsverteilung durch ein gewichtetes
Aufaddieren von Gau√üglocken an.</p>

<script type="math/tex; mode=display">p(x) = \sigma_{i=1}^{k}{c_i \mathcal{N}(\mu_i, \Sigma)}</script>

<p>Um Parameter und Rechenzeit zu sparen, werden nur Diagonalelemente in $\Sigma$
verwendet. Das hei√üt, dass wir eine Modellannahme machen und zwar, dass die
Ellipsoiden nicht gedreht sind.
Diesem Nachteil kann man durch mehrere Mischungen von Glocken entgegenwirken.</p>

<p>Man rechnet mit $log p(x)$, da der log monoton, entf√§llt dadurch die Auswertung
der kostspieligen e-Funktion (Taylorreihenentwicklung), aber die wichtigen
Eigenschaften von p bleiben bestehen. Meistens wird sowieso nur min und max betrachtet.
Der log ist auch numerisch stabiler.</p>

<p>Diese GMM entspricht der Mahalanobis-Distanz zwischen einem gegeben Vektor und
dem Mittelwertsvektor. log-skalierte Mahalanobis-Distanz</p>

<p>Auf GMM kann log leider nicht direkt angewandt werden, weil es sich um eine Summe
handelt.</p>

<p>Wie findet man Mittelwert und Kovarianzmatrix?</p>

<h3 id="vektorquantisierung">Vektorquantisierung</h3>

<p>VQ: Vektorquantisierung</p>

<p>Zwischenspiel. Eigentlich wollen wir f√ºr unsere Rechnungen die Varianz und das
Kontinuierliche wieder loswerden. Und wir wollen nicht mit dem gesamten Merkmalsraum
rechnen.</p>

<p>Abbildung gesucht: $R^n \to N$, um reelle Vektoren auf Index in Repr√§sentantenvektoren
zu beziehen.</p>

<p>Das hei√üt, wir suchen f√ºr jeden Merkmalsvektor x den ‚Äún√§chsten‚Äù Referenzvektor $\mu_i$:
<script type="math/tex">I(X) = arg_i min d(X, \mu_i)</script>
Dabei entsteht zwangsl√§ufig ein Quantisierungsfehler, den wir minimieren wollen.</p>

<p>Welche Distanz?
Wenn man Euklid w√§hlt, dann teilt man den Raum in <strong>Voronoi-Regionen</strong> ein.
Jede Region enth√§lt nur einen Referenzvektor, dieser wird dann f√ºr alle Punkte genutzt,
die in diese Region fallen.
Nachteil: Liefert unintuitive Ergebnisse f√ºr Menschen, weil nicht die Varianz
der Daten betrachtet wird.</p>

<p>Deswegen nutzen wir die <strong>Mahalanobis-Distanz</strong>, weil sie die Streuung der Daten
mitber√ºcksichtigt. Diese Streuung muss aber im Vorhinein gesch√§tzt werden.
F√ºr diese Sch√§tzung muss also klar sein, welchen Referenzvektoren die Trainingsdaten x
zugeordnet werden.</p>

<p><strong>Klassifikationsproblem</strong>: Gegeben Vektor x finde passenden Referenzvektor $\mu_i$.</p>

<p>Probleme:</p>
<ul>
  <li>Ausrei√üer in den Trainingsdaten verzerren Sch√§tzung der Repr√§sentanten</li>
  <li>Immer nur einen Repr√§sentanten bis jetzt betrachtet. Wenn ein Punkt aber
genau zwischen zwei Repr√§sentanten liegt, dann w√ºrde man lieber mehr betrachten.</li>
</ul>

<p><strong>K-N√§chste Nachbarn</strong>: Bestimme f√ºr einen neuen Vektor die Klassen der k
n√§chsten Nachbarn. Nimm die h√§ufigst auftretende Klasse f√ºr die Klassifikation des
Vektors.</p>

<p>Bei vielen Referenzvektoren (16‚Ä¶1024) f√ºr viele Klassen (10e4 ‚Ä¶ 10e5) und
einer Dimensionalit√§t der Vektoren (16‚Ä¶48), hat man zu hohen Rechenaufwand.</p>

<p>-&gt; Beschleunigung n√∂tig!</p>

<ul>
  <li>Keine garantierte Beschleunigung und immernoch korrektes Ergebnis: <strong>Las Vegas</strong></li>
  <li>Garantierte Beschleunigung aber nicht unbedingt korrektes Ergebnis: <strong>Monte Carlo</strong></li>
</ul>

<p><a href="https://yourbasic.org/algorithms/las-vegas/">Las Vegas und Monte Carlo Beispiel</a></p>

<p>Standardbeispiel beim Finden eines Minimums:
Randomisiertes Ziehen (Sampling) ist abh√§ngig von Annahme √ºber Verteilung der Daten</p>

<p><strong>Beschleunigung von knn:</strong></p>

<p>Early abortion:
Breche Distanzberechnungen ab, sobald betrachteter Vektor aktuelles Maximum √ºberschritten hat.
Garantiert korrektes Ergebnis und Beschleunigung au√üer Liste war aufsteigend sortiert.</p>

<hr />

<p>Organisiere Merkmalsraum in Baumstruktur. An jedem Knoten liegt Entscheidung
in Form einer Hyperebene in einer Dimension vor. Wenn Vektor x links, dann links absteigen etc.
Die Bl√§tter enthalten dann nur noch weniger n√§chste Nachbarn.</p>

<p>Die Entscheidungsknoten werden entlang der gr√∂√üten restlichen Varianz getroffen.
Berechne Richtung der gr√∂√üten Varianz und teile sie in links und rechts.</p>

<p><strong>Beschleunigung GMM</strong>:</p>

<ul>
  <li>Nur den Wert der n√§chsten Gau√üglocke w√§hlen</li>
  <li>Baumstruktur</li>
  <li>Early abortion</li>
</ul>

<h2 id="12-vorlesung">12. Vorlesung</h2>

<p>28.11.2018</p>

<p>Woher kommen die Quantisierungsstufen?</p>

<p>Wir wollen sie an die Daten anpassen indem wir Referenzvektoren finden.</p>

<p>Keine gute Idee: Unterteile Raum in Gitter. Das ist , wenn die Daten nicht gleichverteilt</p>

<h3 id="k-means">k-means</h3>

<p><strong>Un√ºberwachte k-Mittelwerte</strong>:</p>

<p>Gesucht sind die k Referenzvektoren.</p>

<ol>
  <li>Initialisiere zuf√§llig k Referenzvektoren</li>
  <li>Ordne jedem Trainingsvektor seinen n√§hsten Referenzvektor zu</li>
  <li>Durchschnitt einer Nachbarschaft ist neuer Referenzvektor</li>
</ol>

<p>Abbruch nach fester Anzahl Iterationen oder Distanz aller Vektoren unter Schwellwert
oder durch Kreuzvalidierung mit gelabelten Trainingsdaten.</p>

<h3 id="learning-vq">Learning VQ</h3>

<p>√úberwacht</p>

<p>Gesucht: Referenzvektoren</p>

<p>Bewege Referenzpunkt hin zu Objekten seiner Klasse und weg von den anderen.
Wie gro√ü soll die Bewegung sein?</p>

<p>Das erinnert an die Backpropagation, bzw. davon an die Bewegung auf der Fehleroberfl√§che.</p>

<p><strong>LVQ2</strong>: Bestimme die beiden n√§chstgelegenen Repr√§sentaten. Wenn beide unterschiedlich,
dann verschiebe Referenz.</p>

<p><strong>LVQ3</strong>: Auch Update wenn beide Repr√§sentanten gleich weit weg</p>

<p>LVQ kann als ein sehr einfaches neuronales Netzwerk interpretiert werden.</p>

<p>Unter dem Strich kann k-means f√ºr die Initialisierung von GMM genutzt werden.
Wir brauchen daf√ºr: $\mu, \sigma$ und die Gewichtung der einzelnen Glocken.</p>

<h3 id="stochastik-asr">Stochastik ASR</h3>

<p>Wir modellieren die Variabilit√§t der Sprache mit Statistik.
F√ºr die Zeitvariabilit√§t hatten wir DTW eingef√ºhrt.</p>

<p>F√ºr die unterschiedliche Auspr√§gung der Merkmale die Mahalanobis-Distanz.
-&gt; Bei einem gro√üen Vokabular (400.000 W√∂rter) ben√∂tigen wir f√ºr jedes Wort
eine Referenz. Und von jedem Wort am besten 1000 Aufnahmen um der Variabilit√§t
gerecht zu werden. Das w√§re 4 x 10e8 W√∂rter. <strong>Schwer zu sammeln!</strong>
Und die Grammatik fehlt da auch noch.</p>

<p>Stattdessen h√§tten wir lieber kleine Bauteile, die zu beliebigen W√∂rtern zusammengesetzt
werden k√∂nnen.</p>

<h3 id="fundamentalformel">Fundamentalformel</h3>

<p>Dr. St√ºker erwartet, dass man diese Formel nachts um 3:00 aufsagen kann:</p>

<p>Naiver Bayes-Klassifikator:
<script type="math/tex">\hat{W} = arg_w max P(W|X) =</script></p>

<p>Anwendung der Bayes-Formel:</p>

<script type="math/tex; mode=display">arg_w max { P(X|W) \cdot P(W) \over P(X)}</script>

<p>Weil es um die Maximierung geht, k√∂nnen wir nur den Z√§hler betrachten</p>

<script type="math/tex; mode=display">arg_W max P(X|W) \cdot P(W)</script>

<p>$\hat{W}$ ist die Sequenz von W√∂rtern. W ist alle m√∂glichen Sequenzen.
X ist die ‚ÄúAufnahme‚Äù nach der Vorverarbeitung.</p>

<p>Also: W√§hle $\hat{W}$ als wahrscheinlichste Wortfolge gegeben der Aufnahmedaten X.</p>

<p>Nebenspiel: Warum ist die Sequenz nicht unendlich gro√ü?</p>

<ul>
  <li>maximale Sprechgeschwindigkeit</li>
  <li>Diskretisierung (-&gt; mehr als ein Wort pro Frame)</li>
  <li>Inventar der Phoneme ist endlich</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>$P(W</td>
      <td>X)$ schwer zu modellieren, deswegen Bayes-Formel angewendet.</td>
    </tr>
  </tbody>
</table>

<p>Wir sind nicht interessiert, wie wahrscheinlich W ist, sondern wie sie aussieht.
-&gt; nur Z√§hler maximieren</p>

<p>$\hat{W}$ hei√üt <strong>Hypothese</strong> und X <strong>Merkmalsvektor</strong>.</p>

<p>P(X|W) ist da <strong>akustische Modell</strong>: Wie klingt die Wortfolge, wenn sie bei mir ankommt?
Das ist die klassenbedingte Wahrscheinlichkeit einer Aufnahme (a-priori)</p>

<p>P(W) ist <strong>Sprachmodell</strong>: Welche Wortfolgen sind wie wahrscheinlich?
Die Wahrscheinlichkeiten von Sequenzen (a-priori) -&gt; kann man zum Beispiel aus Textkorpora herausbekommen</p>

<p>Mit Bayes minimiert man die Wahrscheinlichkeit f√ºr einen Klassifikationsfehler.
Also die ‚ÄúSatzfehlerrate‚Äù (nur komplette Sequenzen). Sie sind entweder richtig oder nicht.
Was wir nicht minimieren ist soetwas wie eine Levenshtein-Distanz.</p>

<p>Klassifikatoren mit Wortfehlerrate zu bauen ist schwierig und wurde erst sp√§ter
realisierbar (kommt noch in der Vorlesung).</p>

<h3 id="mm">MM</h3>

<p>Markov-Modell</p>

<p>Annahme: Sprachproduktion ist ein stochastischer Prozess.</p>

<p>Gegeben ist ein Wahrscheinlichkeitsraum $( \Omega, F, P)$ mit:</p>

<ul>
  <li>Ergebnisse Omega</li>
  <li>Ereignisse F</li>
  <li>Wahrscheinlichkeiten von Ereignissen</li>
</ul>

<p>F Sigma-Algebra</p>

<p>Ereignismenge ist abgeschlossen gg√º. Vereinigung, Komplementbildung‚Ä¶</p>

<p>Definition Zufallsvariable‚Ä¶</p>

<p>F-Z-messbar‚Ä¶</p>

<p>Zust√§nde Z</p>

<p>Wenn t ganzzahlig, dann ist es ein zeitdiskreter stochastischer Prozess.</p>

<p>F√ºr ASR nutzen wir zeitdiskrete Markov-Ketten.</p>

<p>Eine Markov-Kette n-ter Ordnung: $X = (X_t)t=1..n$</p>

<p>Annahme: Zustands√ºbergang ist nur von den vorherigen n-1 Zust√§nden abh√§ngig.</p>

<p>F√ºr ASR nutzen wir noch einfachere Ketten: Markow-Kette erster Ordnung (also n=1)</p>

<ul>
  <li>Anfangsverteilung: $\mu$
Wahrscheinlichkeitsvektor der Anfangszust√§nde: $\pi = (\pi_1 .. \pi_n)$ mit jedem $\pi_i &gt;= 0$</li>
  <li>$\sum \pi_i = 1$</li>
  <li>√úbergangswahrscheinlichkeiten $p_{ij}(t) := P(X_{t+1}=s_j | X_t = s_i)$</li>
</ul>

<p>Wenn man die Abh√§ngigkeit von t entfernt, dann kommt man zur <strong>homogenen Markow-Kette</strong>:
$p_ij (t) = p_ij  \forall t$</p>

<p>Diese ist vollst√§ndig beschrieben durch:</p>

<ul>
  <li>Zustandsfolge S</li>
  <li>Anfang $\mu$</li>
  <li>√úbergang $p_ij$</li>
</ul>

<p>Ein einfaches Beispiel daf√ºr ist ein Wettermodell.</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/NKoPwE2LQhc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Man kann dann auch Folgen von Zust√§nden zuf√§llig generieren. Siehe Namensgenerator.
Die Namen klingen richtig. Der Mensch scheint ebenfalls eine Modellierung dieser
Art zu machen. Das ist als Best√§tigung der These zu sehen, dass Sprache als
stochastischer Prozess modelliert werden k√∂nnen</p>

<p>Problem: Bei Sprache sieht man aber keine Buchstaben sondern Merkmalsvektoren.
Wenn man ein Wort ausspricht, dann durchl√§uft man im modell Zust√§nde.
Diese Zust√§nde k√∂nnten Phoneme sein.</p>

<p>Es ist aber nicht beobachtbar, welche Phonemzust√§nde durchschritten wurden.</p>

<h3 id="hmm">HMM</h3>

<p>Hidden Markov Model</p>

<p>Wir wissen nicht, welche Zust√§nde Beobachtungen produzieren.</p>

<p>Wenn M√ºnzwurf hinter Vorhang passiert, dann k√∂nnen wir nicht den Zustand beobachten.
Eine Person hinter dem Vorhang sagt uns aber, was das Ergebnis war.</p>

<p>Aufgabe an die Zuschauer ist nach einer Sequenz von beobachteten Merkmalen,
die Zustandsfolge herauszufinden -&gt; ASR</p>

<p>Dazu n√§chste Vorlesung mehr.</p>

<p>Literaturempfehlung f√ºr HMM:</p>

<p><a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/hmm.pdf">A tutorial on HMM and selected applications</a>
 Das Tutorial ist immernoch gut. Die Anwendungsf√§lle eher nicht mehr.</p>

<p><a href="https://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf">Slides dazu</a></p>

<p>Neuerdings gibt es folgenden Ansatz:
 P(W|X) direkt mit Neuronalem Netz und sehr vielen Daten sch√§tzen.</p>

<p>2000: Waren 80h gelabelte Trainingsdaten sehr viel
 2018: Heute sind es 140.000 h aus einer speziellen Dom√§ne. Nur die gro√üen Firmen
 haben die Rechenkapazit√§t und Daten f√ºr solches Training</p>

<p>F√ºr jede Dom√§ne, die nicht so viele Daten angesammelt hat, sind die HMMs
 weiterhin ein sehr guter Kompromiss.</p>
:ET