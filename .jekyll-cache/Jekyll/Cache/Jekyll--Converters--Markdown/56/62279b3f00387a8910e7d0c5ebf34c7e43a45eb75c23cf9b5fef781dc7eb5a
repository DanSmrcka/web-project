I"B{<div style="background-color: #EAEFF4; border: 1px solid #b5aeb1; border-radius: 3px;  padding: 10px; margin-right: 10px">
    <strong>Vorlesung:</strong> <a href="https://campus.studium.kit.edu/ev/mNDK-kwFSjqvAXvMGVhBJQ">Data and Storage Management</a>, Vorlesung, 4 ECTS <br />
    <strong>Dozent:</strong> Prof. Dr. Bernhard Neumair  <br />
   <strong>ILIAS:</strong> <a href="https://ilias.studium.kit.edu/goto.php?target=fold_885971&amp;client_id=produktiv">https://ilias.studium.kit.edu/goto.php?target=fold_885971&amp;client_id=produktiv</a> <br />   
   <strong>Vorlesungswebsite:</strong> <a href="https://www.scc.kit.edu/bildung/11923.php">https://www.scc.kit.edu/bildung/11923.php</a> <br />   
   <strong>Klausur:</strong> mündlich, daher nach Vereinbarung <br />
</div>

<h2 id="organisatorisches">Organisatorisches</h2>

<h3 id="vorlesungen">Vorlesungen</h3>
<ul>
  <li><strong>17.10.2018</strong>: nicht da gewesen</li>
  <li><strong>24.10.2018</strong>: nicht da gewesen</li>
  <li><strong>31.10.2018</strong>: Kapitel 2-25 bis Ende; Kapitel 3-1 bis 8</li>
  <li><strong>7.11.2018</strong>: Kapitel 3-9 bis 3-27</li>
  <li><strong>21.11.2018</strong>: Kapitel 3-28 bis Ende; Kapitel 4-1 bis 4-13</li>
  <li><strong>28.11.2018</strong>: Kapitel 4-14 bis 37</li>
</ul>

<h3 id="material">Material</h3>
<p>Das Material der Vorlesung besteht aus:</p>
<ul>
  <li><strong>Folien</strong>: Folien werden im ILIAS hochgeladen</li>
</ul>

<h3 id="klausur">Klausur</h3>
<p>Klausur findet mündlich statt</p>

<h2 id="vorlesungsinhalt">Vorlesungsinhalt</h2>
<h3 id="introduction">Introduction</h3>
<p>in immer größerem Umfang daten gesammelt, immer kostengünstiger herzustellen → viel größere Messungen, Experimente → Daten kostbares Gut</p>

<p><strong>Daten</strong>: “Collection of raw facts from which conclusions may be drawn”; more convenient from is digital data (increase processing capabilities;
lower cost of digital storage; affordable, faster communication technology)</p>

<p><strong>Kategorisierung von Daten</strong>:</p>
<ul>
  <li><em>Structured:</em> e.g. rows and columns</li>
  <li><em>Unstructured:</em> 80% of enterprise information, e.g. PDFs, Documents, Web Pages, Invoices</li>
</ul>

<p><strong>Information Lifecycle Management</strong>: effective data management:<br />
Create → Access → Migrate → Archive → Dispose (Vgl. Abb. Kap 1 - 8)</p>

<p>Enabling Data-Intensive Computing: Data Analysis, Visualization, Experiment, Data Creation, Publications, Archive, Simulation.</p>

<p><strong>Datenanstieg</strong>: Smarter System creating an information explosion; storage requirements growing 20-40% p.a., Information doubling ever 18-24 months, 
Storage butgets up 1-5% in 2010 → Information explosion meets budget reality; <br />
Clients struggling to keep up, top issues for storage managers:</p>
<ul>
  <li>Managing Storage Growth ~55%</li>
  <li>Proper Capacity Forecasting and Storage Reporting ~35%</li>
  <li>Managing Costs ~30%</li>
  <li>Backup Administration and Management 20%</li>
  <li>Managing Compolexity ~20%<br />
Top Storage Initiatives:</li>
  <li>Tired Store Build-out</li>
  <li>Consolidation</li>
  <li>Technology Refresh</li>
  <li>Backup Redesgin</li>
  <li>Vitualization Adoption</li>
  <li>Improving Performance</li>
  <li>Archiving</li>
</ul>

<p><strong>Key Requirements for Data Center Elements</strong>: Data Integrity, Availability, Security, Performance, Scalability, Capacity → Manageability</p>

<p><strong>Critical Success Factors</strong>: Managing Information Growth without Complexitiy:</p>
<ul>
  <li><em>Storage Efficiency:</em> Reduce Cost, improve performance, increase felxibility</li>
  <li><em>Data Protection:</em> maintain access to information</li>
  <li><em>Service Management:</em> improve service, beat expectations</li>
</ul>

<p><strong>Storage Efficiency</strong>: getting the most from storage resources</p>
<ul>
  <li><em>Tiered Storage:</em> automated tiered storage to maximize performance, reduce operation expenses; SSD to increase performance up to 300%;
cost reducing by migrating less critical data to less expensive media;</li>
  <li><em>Consolidated Storage:</em> reduce administrative costs and improce cycle time, Scale out storage to manage billions of files from single consolidated 
system</li>
  <li><em>Virtualized Storage:</em> quickly provision storage, increase flexibility; increate utilization of storage by up tp 30%; Reduce total cost of 
ownershipt (TCO) up to 66% by deploying automated, virtualized storage</li>
</ul>

<p><strong>Data Protection</strong>: Improving Service levels, beating Expectations</p>
<ul>
  <li><em>Visibility:</em> improve visibility to enable better forecasting, identify up to15% of allocated storage for reclamation, reuse</li>
  <li><em>Control and Automation:</em> improve cycle time, reduce errors; increase administrator productivity by simpifying disk provisioning</li>
  <li><em>Flexible Delivery Options:</em> flexible delivery options (e.g. cloud) an manages services; improve flexibility by moving processes and workloads to
external cloud environments</li>
</ul>

<p>Hard Disk Performance (per GB) drops alarmingly: Sustained IO Rate per GB and hard disk generation 10k RPM (2005)</p>

<p><strong>Storage Media</strong>: Predominant in datacenter:</p>
<ul>
  <li><em>Tape:</em> in robotic libraries</li>
  <li><em>SATA 7kRPM:</em> Serial ATA → “dumb” disk electronics</li>
  <li><em>3,5 FC 15kRPM:</em> Fibrechannel  → “intelligent” networkes SCSI disk controller</li>
  <li><em>Flash SSD:</em> Solid State Device, NAND Flash</li>
</ul>

<p>Trends Backup on SATA: 3,5” FC → 2,5” FC (FC = <a href="https://de.wikipedia.org/wiki/Fibre_Channel">FibreChannel</a>)</p>

<h3 id="storage-access-and-architectures">Storage Access and Architectures</h3>
<p><strong>Host</strong>: Applications run on host; hosts range from simple laptops to compley server cluster; Physical Components: CPU, Storage (Disk device, internal 
memory), I/O device (Host to host communications, e.g. Network Interface Card NIC), Host to storage device communications (Host Bus Adapter HBA)<br />
Logical Components:</p>
<ul>
  <li><em>Application:</em> Inferface between User and Host, three-tired architecture (UI, Logic, Databases), data access classification:
    <ul>
      <li>Block-Level-access: data stored/retrieved in blocks</li>
      <li>File-Level-access: data stored/retrieved by name and pathname</li>
    </ul>
  </li>
  <li><em>OS:</em> between application and hardware, controls environment</li>
  <li><em>LVM (Logical Volumen Manager):</em> <a href="https://wiki.ubuntuusers.de/Logical_Volume_Manager/">Logical Volume Manager</a>, creates/controlls host level logical
storage (physical view ⇔ logical view, physical data blocks ⇔ logical data blocks), part of OS or third party<br />
Components:
    <ul>
      <li>Physical Volumes: typically divided into contiguos equal-sized disk blocks</li>
      <li>Volume Groups: one or more physical volumes form volume groups, LVM manages it as single entity, physical volumes can be added/removed, at least
  one disk group per OS</li>
      <li>Logical Volumes</li>
    </ul>
  </li>
  <li><em>Device Drivers</em>: for device recognition, API to access, control devices, hardware dependent, OS specific</li>
  <li><em>File System</em>: file is collection of related records/data stored as a unit, file system ist hierarchical structure of files (e.g. FAT32, NFTS, UNIX FS)</li>
</ul>

<p><strong>Partitioning</strong>: Physical Volume splitted into many Logical Volumes</p>

<p><strong>Concatenation</strong>: Physical Volume united into one Logical Volume</p>

<p><strong>Files from/to Storage</strong>: User (manages/configures) → Files (Reside in) → Files System Files (Mapped by a file system to) →
File System Blocks (residing in) → LVM Logical Extents (Mapped by LVM to) → Disk Physical Extents (Managed by disk storage subsystem)</p>

<p><strong>Connectivity</strong>: Interaction between hosts, host and storage device; physical components: Bus, Port and cable</p>

<p><strong>Connectivity Protocol</strong>:</p>
<ul>
  <li><em>Tightly connected entities:</em> central processor to RAM, storage buffers to controllers</li>
  <li><em>Directly attached entities:</em> connected at moderate distance (e.g. host to storage)</li>
  <li><em>Network connected entities:</em> networked host (e.g. NAS or SAN)</li>
</ul>

<p><strong>Storage: Medias and Options</strong>:</p>
<ul>
  <li><em>Magnetic Tape:</em> low costs, long term data storage, limitations: sequential data access, single application access at a time, storage/retrieval overheads</li>
  <li><em>Optical Disk:</em> distribution medium in small, single-user computing environments, limited capacity and speed, e.g. CD-ROM, DVD-ROM</li>
  <li><em>Disk Drive:</em> most popular, large storage capacity, random read/write access, for performance intentive online application, 
$ Transfer Time = \frac{Block size}{Transfer rate} $</li>
</ul>

<p><strong>Hard Disk Drive (HDD) Basics</strong>: read/write cache hits ~1ms; physical disk I/O operations &gt;5ms; limited number of I/O operations per second by:</p>
<ul>
  <li><em>Average Seek Time [ms]</em>: head movement to required track, typical 4-10ms</li>
  <li><em>Rotation Latency [ms]</em>: disk platter spinning until first sector addressed passes under r/w heads; avg. time = half a rotation; typical: 2-4ms</li>
  <li><em>Transfer Time [ms]</em>: read/write data sectors, 1 sector = 512 Byte; typical &lt;16KiB</li>
</ul>

<p><strong>Fundamental Laws Governing Disk Performance</strong>:  arrival rate a</p>
<ul>
  <li><em>Little’s Law:</em> Relationship between number of requests N in queue an response time R: $ N = a * R $</li>
  <li><em>Utilization Law:</em> I/O controller utilization $ U = a*R_s $; $R_s$ is service time</li>
</ul>

<p><strong>Native Command Queueing (NCQ)</strong>: muss eingeschaltet werden; Frage: Wann geben wir commit, dass Schreiboperation erfolgreich war?</p>
<ul>
  <li>wenn File im Cache → schlecht für Performance</li>
  <li>wenn File auf Platte → Leistung von Hersteller, bessere Performance, Head Verhalten in Hardware, paar 1000 Umdrehungen pro Minute</li>
</ul>

<p><strong>Short Stroking</strong>: Increasing HDD Performance, Approach to achieve maximum possible performance from HDD by limiting overall head movement and
average seek time; Implementation: only small portion of overall capacity, tracks on outer edge with higher data density; Disadvantage: large number of 
HDDS invoveld, onyl small portion os storage capacity used; typical for applications with high access densities with high random I/O rates, 
low response times but small amount of data</p>

<p><strong>Transaction Processing</strong>: Buchungsprozesse, online, tausende Nutzer → Daten schlecht vorhersehbar, z.B. durch Sitzplatzbuchung, nur wenige bytes
die sich ändern; vorsicht beim Buffern z.B. Telefonie; Antwortzeit bei DBs typischerweise 2s → sehr niedrige response time, aber gleichzeitig bei
wenigen Datan hohe I/O Raten; OLTP = Online Transaction Process;<br />
<em>Latenz wichtig, Durchsatz wichtig!</em></p>

<p><strong>Batch Jobs</strong>: Nachts ausführbar, z.B. Berechnungen für Buchungen können nachts ausgeführt werden (Terminüberweisungen); ABER: Bewegen von vielen Daten, 
seriell geschrieben → I/O auch noch hoch, aber durch caching/prepatching handlebar → Durchsatz aber begrenzt
<em>Buffering möglich, Latenz egal!</em></p>

<p><strong>Application I/O – Workload Performance Characteristics</strong>:</p>
<ul>
  <li><em>Basic Workload Performance:</em> I/O rate [IOps] (transactions) or data rate [MBps] (throughput); Random access or sequential access;
Read:write ration; average I/O request size</li>
  <li><em>Additional Workload Performance:</em> Read cache hit ratio, average response time</li>
</ul>

<p><strong>Enterprise Flash Drives</strong>: highest possible throughput per drive, no spinning magnetic media, no mechanical movement (→ no latency), Solid State enables
consisten I/O performance; very low latency per I/O, energy efficient storage design; based on Flash Solid State; 30 greater IOPS, less than 1 ms service time, 
better reliability</p>

<h3 id="storage-infrastructures">Storage Infrastructures</h3>

<p><strong>Redundant Array of Independent Disks (RAID)</strong>: <a href="https://de.wikipedia.org/wiki/RAID">Wikipedia</a>: “Ein RAID-System dient zur Organisation 
mehrerer physischer Massenspeicher zu einem logischen Laufwerk, das eine höhere Ausfallsicherheit oder einen größeren Datendurchsatz erlaubt 
als ein einzelnes physisches Speichermedium.”; performance limitation of disk drive, individual drive has certain life expectancy → RAID; Ziel von
RAID: geringe Ausfallwahrscheinlichkeit für Festplatten mit möglichst geringem Overhead, Erhöhung von Performance; Raid provides: increase capacity, 
higher availability (Ausfallsicherheit), increased performance<br />
<em>RAID Array Components:</em></p>
<ul>
  <li>RAID Controller</li>
  <li>Physical Array</li>
  <li>Logical Array</li>
  <li>Hard Disk</li>
</ul>

<p><em>RAID implementations:</em></p>
<ul>
  <li>Hardware: usually specialized disk controller card; controls attached drives, arrays appear
to host as regular disk drive</li>
  <li>Software: “Firmware”, runs on OS, performance CPU workload dependent, not supporting all RAID
levels</li>
</ul>

<p><em>RAID Levels:</em></p>
<ul>
  <li>0: Redundanz fehlt, nur Striped array, no fault tolerance, Beschleunigung ohne Redundanz, gesteigerte Transferraten, Platten in zusammenhängende Blöcke 
gleicher Größe aufgeteilt, Blöcke “reißverschlussartig” zu einer großen Platte angeordnet, Zugriffe auf Platten parallel möglich (→ Striping), chunk size 
meist 64kB</li>
  <li>1: Disk mirroring, alle Daten auf zwei Festplatten gespiegelt, Kapaziät max so groß wie kleinste Platte, sehr einfach,</li>
  <li>Nested RAID 0+1: Striping und Mirroring</li>
  <li>Nested RAID 1+0: Mirroring und Striping, Ein RAID-10-Verbund ist ein RAID 0 über mehrere RAID 1, Eigenschaften der beiden RAIDs kombiniert: 
Sicherheit und gesteigerte Schreib-/Lesegeschwindigkeit, mindestens vier Festplatten</li>
  <li>3: Parallel access array with dedicated parity disk, parity disk enthält Summeninformation zum Errechnen der verorenen Bits auf Datenplatte</li>
  <li>4: Striped array, independent disks, dedicated parity disk; größere Datenblöcke der Paritätsinformationen, Datenübertragungsgeschwindigkeit durch
Datenübertragungsgeschwindigkeit der Paritätsplatte begrenzt, fest definierte Paritätsplatte</li>
  <li>5: Striped array, independent disks, distributed parity, Nutzdaten auf alle Platten verteilt, Paritätsdaten gleichmäßig verteilt, Parallelisierung nicht
möglich</li>
  <li>6: Striped array, independent disks, dual distributed parity, survives 2 erasures, parity block in different place each stripe</li>
</ul>

<p><em>RAID definitions:</em></p>
<ul>
  <li>Strip size (Chunk, Stripe): e.g. 256 KiB</li>
  <li>Stripe (Stride) size: Capacity of full stripe</li>
  <li>Spare drive: Standby drive, in case of disk failure in an array</li>
  <li>Rebuild Time: duration of exposure until spare has migrated into failing array</li>
</ul>

<p><em>RAID 5 vs. RAID 10:</em> RAID 5, RAID 10 comparable peroformance regarding read operations, RAID5 better for large-block seuential writes, RAID 10 better
for small-block random writes; RAID 5 good choice for high availability, fewer writes than reads, RAID 10 fault-tolerant, performance critical, write sensitive
but high random write percentage</p>

<p><em>Ende Vorlesung vom 7.11.2018</em></p>

<p><strong>Components of Intelligent Storage Systems</strong>: wenig “intelligent”, unterstützt dabei große Anzahl an physical disks in unterschiedliche
RAID Systeme abzubilden; Vorteile: increased capacity, improved performance, easier data management, improved data availability/protection</p>
<ul>
  <li><em>Front End:</em> Configuration Richtung Netzwerk/Host</li>
  <li><em>Cache:</em> write operations:
    <ul>
      <li>Write-through Cache: direktes Schreiben in den HS (“am Cache vorbei”), langsamer, in billigeren Systemen</li>
      <li>Write-back Cache: Daten nicht direkt in den Speicher geschrieben, sondern in den Cache, Daten in HS und Cache sind inkonsistent, commit wird
   gegeben, wenn Daten im Cache sind, aufwändigere Technik (teurere Systeme) aber bessere Performance</li>
    </ul>
  </li>
  <li><em>Back End:</em> Configuration zu physical disks</li>
  <li><em>Physical Disks:</em></li>
</ul>

<p><strong>Caching Algorithmen</strong>: Least Recently Used (LRU)/ Least Recently Written (LRW), bessere Performance durch write-cache management</p>

<p><strong>Cache Data Protection</strong>: protecting cache data against failure:</p>
<ul>
  <li><em>Cache mirroring:</em> writes to cache are held in two different memory locations on two different memory cards</li>
  <li><em>Cache vaulting:</em> cache exposed to risk of uncommitted data → power failure: uncommitted data is dumped to dedicated set of drives (= vault
drives)</li>
</ul>

<p><strong>Logical Units</strong>: physical disks combined to logical units and deliver to host as unit; LUN = logical unit number; man 
“sieht” nach außen nur LU und nicht physikalische Disks, einfach um im laufenden system RAID systeme auszutauschen; 
LUN Masking Zugriffsrechtschutz; Zugriffe auf LUNs immer in Blöcken unabhängig von Repräsentation nach außen, “hinten” auf Disks immer Standard</p>

<p><strong>High-end Storage Systems</strong>: for enterprises, large storage capacity, huge cache to service host I/Os, fault tolerance architecture, high scalability
Active-Active Configuration: Storage Array with two connections to host; Storage Array alles gedoppelt: Controller, Platten, Stromanschlüsse 
→ sehr hohe Ausfallsicherheit</p>

<p><strong>Mid-end Storage Systems</strong>: less scalable, eine Verbindung active (darüber geht Host I/O) und eine passiv, welche dafür da ist, wenn aktive Verbindung
failed</p>

<blockquote>
  <p>high-end SS und mid-end SS Preisunterschied kommt hauptsächlich von teurer Software (Firmware)</p>
</blockquote>

<h3 id="storage-connectivity-and-networking">Storage Connectivity and Networking</h3>
<p><strong>Direct Attached Storage (DAS)</strong>: <a href="https://de.wikipedia.org/wiki/Direct_Attached_Storage">Wikipedia</a>: “an einen einzelnen 
Rechner angeschlossene Festplatten, die sich in einem separaten Gehäuse befinden”<br />
<em>Vorteile:</em> lokale Datenhaltung, schnelles Entwickeln fürkleine Umgebungen, einfacher Deploy, Zuverlässigkeit, 
niedrige Kosten und niedrige Komplexität<br />
<em>Probleme:</em> begrenzte Skalierbarkeit, Downtime bei Wartung, limitiertes Ressourcensharing</p>

<p><strong>Small Computer System Interface (SCSI)</strong>: <a href="https://de.wikipedia.org/wiki/Small_Computer_System_Interface">Wikipedia</a>: “Familie von 
standardisierten Protokollen und Schnittstellen für die Verbindung und Datenübertragung zwischen Peripheriegeräten und Computern”<br />
<em>Adressierung:</em> SCSI initiator port and SCSI target port has SCSI port indetifier, logical unit is assigned a LUN within SCSI 
target device, LU has also LUN, SCSI target device with SAS target ports has device name<br />
→ <em>Address = Bus : Target ID : LUN</em><br />
<em>Commands:</em></p>
<ul>
  <li>Non-data commands (N): no data transferred</li>
  <li>write commands (W): data transferred from initiator to target, write data = Data-Out</li>
  <li>read commands (R): data transferred from target to initiator, read data = Data-In</li>
  <li>bidirectional commands (B): data transferred in both directions, Data-In and Data-Out</li>
</ul>

<p><em>Ende der Vorleseung vom 21.11.2018</em></p>

<p><strong>Serial ATA (SATA)</strong>: <a href="https://de.wikipedia.org/wiki/Serial_ATA">Wikipedia</a>: “Computer-Schnittstelle für den 
Datenaustausch mit Festplatten und anderen Speichergeräten”, Serial connectivity: 
Superior RAID volume creation and rebuild performance, future investment protection for increased
network bandwith; Serial ATA is replacement for ATA (+ additional superset capabilities “hot plug”) <br />
<em>Topology:</em></p>
<ul>
  <li>four-wire replacement for physical layer of ATA</li>
  <li>“Star” topology (point-to-point, no hubs): each device full bandwidth, no bus arbitration/collision overhead, simpler RAID implementation</li>
</ul>

<p><em>Schnittstellenvervielfachung:</em> connect to more than one device, concept of a simple hub, no hard drive changes needed
<em>Verfügbarkeitsmechanismen:</em> 2 host controller can connect to one SATA hard drive, eliminates SATA host controller as single point of
failure, also used for static load balancing</p>

<p><strong>Serial Attached SCSI (SAS)</strong>: <a href="https://de.wikipedia.org/wiki/Serial_Attached_SCSI">Wikipedia</a>, device and near cabinet interface (not a 
network interface)</p>
<ul>
  <li>four-wire replacement for physical layer of parallel SCSI</li>
  <li>“Star” topology (point-to-point, no hubs)</li>
</ul>

<p><strong>Storage Area Network (SAN)</strong>: dedicated high spee network of servers and shared storage devices, provide block level data access,
resource consolidation (centralized storage and management), scalability (theoretical ~15mio devices, performance), secure access  <br />
<em>Components</em>: any-to-any connectivity for resources in fabric, any server can potentially talk to any storage device</p>

<p><strong>Fiber Channel</strong>: Netztechnologie speziell für SANs; Glasfaserkabel (Front End), Kuperkabel (Back End); Zugriff auf Speicherblöcke;
effiziente Nutzung der Bandbreite (mit geringem Protokoll Overhead)  <br />
<em>Protokolle</em>:</p>
<ul>
  <li>FC 1: Codierung/Decodierung</li>
  <li>FC 2: Seqgemtierung, Reassembling, Framing, Flusssteuering, Credit-Mechanismus</li>
  <li>FC 3: Broadcasting, Multicasting</li>
  <li>FC 4: Abbildung auf Upper Layer Protocol</li>
</ul>

<p><em>Topologien</em>:</p>
<ul>
  <li>Point-to-Point</li>
  <li>Arbitrated Loop</li>
  <li>Switched Farbic
    <ul>
      <li>vermascht</li>
      <li>mehrstufig</li>
    </ul>
  </li>
</ul>

<p><em>Port Typen</em>:</p>
<ul>
  <li>N-Ports: Endgeräte (“Nodes”)</li>
  <li>L-Ports: an Arbitrated Loops</li>
  <li>F-Ports: Ports ans Switches mit N-Ports verbunden (“Farbic”)</li>
  <li>E-Ports: Verbindung zwischen Switches, (“Edges”)</li>
</ul>

<p><em>Addressierungskonzept</em>: Port eindeutig über MAC ID identifiziert, 64 bit WWPN (World Wide Port Name) und WWNN (World Wide Node Name), 
Zuweisung einer N-Port-ID (24 bit) beim Einschalten (fabric login), Networking 24 bit Address Space: Domain ID (8 Bit), Area ID (8 Bit), 
Device/Node ID (8 Bit) (WWN - World Wide Name), Routing auf Basis von N-Port-IDs</p>

<p><em>Service Classes</em>:</p>
<ul>
  <li>Class 2: connectionless communication, end-to-end acks</li>
  <li>Class 3: connectionless communication, no end-to-end acks</li>
  <li>Class 1: reservation of dedicated path (origin-dest), dedicated links, similar to circiut switching, rarely used</li>
  <li>Class 4: like class 1, links may be shared</li>
  <li>Class 6: multicast</li>
</ul>

<p><em>Flow Control</em>: ensures that congeston does not result in packet loss in delivery of frame placed on fabric; used to end-to-end and link-level
flow control</p>

<p><em>Security</em>: Zones define which egress F-Ports are reachable from any ingress F-Port, authentication and access control at end devices, 
soft zoning (resticted notification and partitioning of WWN service), most SANs closed → no issues as in IP networks</p>

<p><strong>Inter Switch Links (ISL)</strong>: connects two or more FC switches using E-Ports, to transfer host-to-storage data and fabric management
traffic, scaling mechanism in SAN connectivity</p>

<p><strong>IP Storage</strong>: Fiber Channel or LAN (iSAN); Motivation: Availability/low costs, no additional network topology, single integrated 
network, remote data access, data replication; But: security considerations, performance, functionality</p>

<p><em>Ende VL 27.11.2018</em></p>

<p>IP Storage Netzwerke und Technologien, die auf Transportprotokollen wie IP/Ethernet aufbauen, dabei sind Koexistenzen möglich:</p>
<ul>
  <li><em>Fibre Channel over IP (FCIP):</em>  ist ein Transportprotokoll, welches Fibre Channel-Pakete in TCP einpackt (tunneling), 
um sie über standardmäßige TCP/IP-Netzwerke zu transportieren. Fibre Channel findet vor allem bei Speichernetzwerken Verwendung 
und zeichnet sich durch linksorientierte Flusskontrolle und Verlustfreiheit aus. FCIP ist sowohl für IP als auch für FC Netzwerke
transparent und verbindet geografisch verteilte SANs.
distributed SANs</li>
  <li><em>Internet Fibre Channel Protocol (iFCP):</em> ist eine IP Infrastruktur zum routing/switching von Fiber Channel frames. Es
ist ein gateway-to-gateway Protokoll das eine fibre channel fabric über ein TCP/IP Netwerk implementiert. Es verbindet
Fiber Channel Geräte mit Fiber Channel SANs über die IP-Infrastrukur.</li>
  <li><em>internet Small Computer System Interface (iSCSI):</em> ist ein Verfahren, welches die Nutzung des SCSI-Protokolls über 
TCP ermöglicht. SCSI commands werden über TCP/IP Netzwerke übertragen. Die IP-Infrastruktur wird verwendet für das 
routen/switchen von SCSI commands und die übertragenen Daten sind block-orientierte Speicherdaten.</li>
</ul>

<p>(img ip-storage-netzwerk.png)</p>

<p><strong>TCP/IP Offload Engines (TOE)</strong>: TCP/IP protocol stack is not efficient for SAN applications.
Protocol processing typically consumes too much processor time (multiple memory copies, too
many interrupts, checksum calculations). iSCSI wirespeed with 1 GbE typically requires TOE.
TOE is a network interface card (NIC), a host bus adapter (HBA) thath implements TCP/IP 
protocols. There are two alternatives available with respect to technology. The first
is ASICs which is required for iSCSI over 10 GbE, the sencond is GP Processor on TOE
which is an software implementation of TCP/IP. Some TOE implement also the iSCSI layer.</p>

<p><strong>Fiber Channel over Ethernet (FCoE)</strong>: is a newly proposed standard that is being developed.
The protocol specification maps Fibre Channel natively over Ethernet. It allows an evolutionary
approach toward I/O consolidation by preserving Fiber Channel protocols, maintaining the same
latency, security and traffic management attributes of Fibre Channel and preserving
investments in Fibre Channel tools, training and SANs. Fibre Channel has proven to be the 
dominant storage protocol in the data center. FCoE leverages FC and provides a viable I/O 
consolidation solution. Using Ethernet avoid creating a separate protocol for I/O 
consolidation.</p>

<p>Ethernet adapters (NICs) have unique MAC addresses. Fibre Channel N_Ports have 24-bit
addresses, called FC-ID. Since FCoE ports are Ethernet ports they have MAC addresses.</p>

<p><em>I/O Consolidation with FCoE:</em> (img fcoe-io.png)</p>

<p><em>FCoE Deployment:</em> FCoE is used in parallel SAN and LANs. It has a potentially less optimal
use of networks. 4 or more connections per server mean higher adapter and cabling costs, 
whicht adds capital expense and optertional expense. Each connection adds addtional points of
failure to the network. That also means slower server provisioning and additional 
redundancy due to separate physical infrastructure and seperate management realms.</p>

<p>(img fcoe-depl.png)</p>

<p>A single Converged Network Adapter (CNA) is used to connect the host to the CEE network.
 This results in less components to deploy with LAN traffic and storage traffic sharing
 the same hardware.</p>

<p>(img fcoe-depl2.png)</p>

<p><strong>Converged Enhanced Ethernet (CEE):</strong> FCoE is a new technology that enables Fibre Channel 
traffic to traverse a new generation of lossless Ethernet known as Converged Enhanced
Ethernet (CEE) also refered to as Data Center Ethernet (DCE) or Data Center Bridging (DCB). 
CEE provides a transport to converge all network data entering and leaving servers.
Lossless Ethernet enables I/O consolidation at the server by transporting storage and 
networking traffic over CEE using a single Converged Network Adapter (CNA).</p>

<p><em>Reasons for CEE</em>:</p>
<ul>
  <li>high latency and dropped frames in traditional Ethernet</li>
  <li>FC requires lossless, low latency and a low congestion transport medium</li>
  <li>CEE is a term for an Ethernet technology that has been enhanced by additional standards to 
meet the requirements for transporting fibre channel frames</li>
</ul>

<p><em>CEE and Protocol Stack</em>: CEE provides the data link transport that will carry FC frames 
encapsulated in FCoE headers.</p>

<p>(img cee-prot.png)</p>

<h3 id="storage-virtualization">Storage Virtualization</h3>
<p><strong>Virtualization</strong>: is a technique of abstracting physical resources into a logical view. It
increases the utilization and capability of IT resources and simplifies resource management 
by pooling and sharing resources. It can significantly reduce downtimes (planned or
unplanned) and improves performance of the IT resources.</p>

<p><em>Forms of Virtualization:</em></p>
<ul>
  <li><em>Virtual Memory:</em> Each application sees its own logical memory, independent of physical 
memory</li>
  <li><em>Virtual Networks:</em> Each application sees its own logical network, independent of physical
network</li>
  <li><em>Virtual Servers:</em> Each application sees its own logical server, independent of physical 
servers</li>
  <li><em>Virtual Storage:</em> Each application sees its own logical storage, indepentent of physical
storage</li>
</ul>

<p><strong>Storage Virtualization</strong></p>

<p>Storage Virtualization is used to consolidate
storage, migrate data without/with minimal downtimes, adding/removing disk space with 
minimum overhead/downtime, to ensure availability, performance and reduce unused disk space.</p>

<p>(img storage-virt.png)</p>

<p><em>Protocols and Layers</em>: (img sv-prot.png)</p>

<p><strong>Disk Virtualization</strong>: The physical data layout (disk on the right) is mapped to the 
logical data layout (Logical Block Addresses (LBA) on the left). ATA, SATA, SCSI, SAS
is used for this mapping.</p>

<p>(img disk-virt.png)</p>

<p><strong>Block Virtualization</strong>: Physical disks with a fixed size, bounded performance and the 
characteristic that they do break occasionally are virtualized to so called virtual disks.
These are variable in size and performance, can be reliable and have the ability to grow, shrink
or morph. Block virtualization happens on host side, in the storage device.</p>

<p>(img block-virt.png block-virt2.png)</p>

<p><em>In-Band:</em> The Aggregation device is in the data path and transparent from host perspective.
Implementation can be switch-based (optimized for little impact on performance) or 
server-based device (implement storage services flexibly and inexpensively).</p>

<p><em>Out-of-Band:</em> (img block-virt-out.png)</p>

<p><strong>File Syste Virtualization</strong>:</p>
<ul>
  <li>Network Attached Storage (NAS): (img nas.png)</li>
  <li>Directly Attached Storage (DAS): (img das.png)</li>
  <li>Storage Area Network (SAN Attached): (img san.png)</li>
  <li>Storage Area Network (iSCSI Attached): (img iscsi.png)</li>
</ul>
:ET