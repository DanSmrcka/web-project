I"·+<div style="background-color: #EAEFF4; border: 1px solid #b5aeb1; border-radius: 3px;  padding: 10px; margin-right: 10px">
    <strong>Vorlesung:</strong> <a href="https://campus.studium.kit.edu/ev/LSwjSOYfRp6QN-PKh4W42Q">Algorithmen II</a>, Stammmodul, 6 ECTS <br />
    <strong>Dozent:</strong> Peter Sanders  <br />
    <strong>√úbungsleitung:</strong> Sebastian Lamm, Demian Hespe  <br />
   <strong>Vorlesungswebsite:</strong> <a href="http://algo2.iti.kit.edu/AlgorithmenII_WS18.php">http://algo2.iti.kit.edu/AlgorithmenII_WS18.php</a> <br />   
   <strong>Klausur:</strong> 19.2.2019 13:00 - 15:00  <br />
   <strong>Nachklausur:</strong> (noch unklar)  <br />
  
</div>

<h2 id="organisatorisches">Organisatorisches</h2>

<h3 id="vorlesungen">Vorlesungen</h3>
<ul>
  <li><strong>15.10.2018</strong>: Organisatorisches und Foliensatz 1-1 bis 1-19</li>
  <li><strong>16.10.2018</strong>: Dozent krank, VL wird von √úbungsleiter gehalten (Foliensatz 2, hat Nummerierung 3): 3-1 bis 3-23</li>
  <li><strong>22.10.2018</strong>: 1-20 bis 1-32; Wdh. Foliensatz 2; (Foliensatz 3 hat Nummerierung 2) 2-1 bis 2-65</li>
  <li><strong>23.10.2018</strong>: 2-10 bis 2-27 und √úbung 1</li>
  <li><strong>29.10.2018</strong>: 2-28 bis Ende Kapitel 3</li>
  <li><strong>30.10.2018</strong>: 4-1 bis 4-22, √úbung 2</li>
  <li><strong>05.11.2018</strong>: 4-23 bis Ende Kapitel 4; 5-1 bis 5-25</li>
  <li><strong>06.11.2018</strong>: 5-12 bis 5-19, √úbung 3</li>
  <li><strong>12.11.2018</strong>: 5-20 bis 5-61</li>
  <li><strong>13.11.2018</strong>: 5-61 bis 5-68, √úbung 4</li>
  <li><strong>19.11.2018</strong>: 5-68 bis Ende Kapitel 5, 6-1 bis 6-2</li>
  <li><strong>20.11.2018</strong>: 6-3 bis Ende Kapitel 6; √úbung 5</li>
  <li><strong>26.11.2018</strong>: Kapitel 7</li>
  <li><strong>27.11.2018</strong>: Kapitel 8-1 bis 8-6; √úbung 6</li>
  <li><strong>03.12.2018</strong>: Kapitel 8-7 bis Ende, Kapitel 9-1 bis 9-7</li>
</ul>

<h3 id="material">Material</h3>
<p>Das Material der Vorlesung besteht aus:</p>
<ul>
  <li><strong>Folien</strong>: Folien mit einer Chilischote am oberen Rand sind <em>nicht pr√ºfungsrelevant</em></li>
  <li><strong>√úbungsbl√§ttern</strong></li>
  <li><strong>Buch</strong>: <em>Algorithms and Data Structures ‚Äî The Basic Toolbox</em>, K. Mehlhorn, P. Sanders, Springer 2008, macht ca. 40% der Vorlesung aus, vor allem empfehlenswert, 
wenn man Algorithmen I nicht am KIT oder gar nicht geh√∂rt hat.</li>
  <li><strong>Skript</strong>: erg√§nzende Informationen die nicht im Buch stehen</li>
</ul>

<h3 id="√ºbungen">√úbungen</h3>
<p>Die √úbungen finden jeweils in der zweiten H√§lfte der Dienstagsvorlesung statt. Die √úbungsbl√§tter erscheinen 14-t√§gig jeweils am Dienstag. 
Die Mustel√∂sung kommt 9 Tage sp√§ter und wird in den √úbungen nicht besprochen. Die Bearbeitung der √úbungsbl√§tter ist freiwillig und wird w√§rmstens Empfohlen (na klar).
Das erste Blatt erscheint am 23.10.2018.</p>

<h3 id="klausur">Klausur</h3>
<p>Die Klausurbearbeitung betr√§gt 120 Minuten und es darf ein doppelseitig handbeschriebenes DIN A4 Blatt mitgenommen werden.</p>

<h2 id="vorlesungsinhalt">Vorlesungsinhalt</h2>

<h3 id="algorithm-engineering">Algorithm Engineering</h3>
<blockquote>
  <p>Algorithm Engineering ist eine umfassenendere Sicht auf die Algorithmik und liefert eine bessere Kopplung zu Anwendungen, als die 
Algorithm Theory.</p>
</blockquote>

<p><strong>Algorithm Theory:</strong> Das Problem in der Umsetzung von Algorithmen liegt zwischen Theorie, Praxis und interdiziplin√§ren Felden.</p>
<ul>
  <li><em>Theorie</em>: Model -&gt; Design -&gt; Analysis -&gt; Performance Guarantees</li>
  <li><em>Practice</em>: Implementation -&gt; Applications</li>
  <li><em>Other Disciplines</em>: Publications? Money?
Die Schwierigkeit liegt darin, die gro√üen Kluften zwischen diesen Gebieten zu √ºberwinden.</li>
</ul>

<p><em>Beispiele:</em></p>
<ul>
  <li>Komplexe theoretische Algorithmen lassen sich oft durch ‚Äúeinfache‚Äù FOR-Schleifen implementieren</li>
  <li>fortgeschrittene Datenstrukturen lassen sich durch Arrays implementieren</li>
  <li>Kontanten werden in der Laufzeitberechnung vernachl√§ssigt, machen in Realit√§t aber 42% der Effizienz aus</li>
</ul>

<p><strong>Algorithmics as Algorithm Engineering</strong>: Den Zusammenhang von Theorie und Praxis kann man sich besser als Kreislauf vorstellen, 
bei welchem Model, Design, Analysis, Implementation und Experiment ineinander √ºbergehen und zus√§tzlich interdisziplin√§re Forschung mit einbeziehen.</p>

<p><strong>Realistic Models</strong>: Vorsichtige Definition von Modellen, die sowohl in der Theorie funktionieren, als auch auf realen Maschinen laufen k√∂nnen.</p>

<p><strong>Design</strong>: algorithm that work well in practice; simplicity, reuse, constant factors, easy instances</p>

<p><strong>Analysis</strong>: constant factors matter, beyond worst case analysis ‚Üí practical algorithms might be difficult to analyze</p>

<p><strong>Implementations</strong>: Sanity check for algorithms; Challenges in semantic gaps: abstract algorithms ‚Üî C++ ‚Üî hardware</p>

<p><strong>Experiments</strong>: sometimes good instead of analysis, rather too much than too little output data, reproducibility (10 years!); need a possible outcome 
that fasifies a hypothesis;</p>

<p><strong>Quality Criteria</strong>: new algorithms must be good (really good, not only with ‚Äútoy data‚Äù); show through comparision old - new algorithm: use same data, 
benchmarks, quality and speed, real world data if possible, many different inputs if possible</p>

<h3 id="fortgeschrittene-datenstrukturen">Fortgeschrittene Datenstrukturen</h3>
<p>Anmerkung: Foliensatz 02 tr√§gt die Nummerierung 3</p>

<p><strong>Priorit√§tslisten</strong>: <a href="https://de.wikipedia.org/wiki/Vorrangwarteschlange">Wikipedia</a>: ‚ÄúDen Elementen, die in die Warteschlange gelegt werden, 
wird ein Schl√ºssel mitgegeben, der die Reihenfolge der Abarbeitung der Elemente bestimmt.‚Äù, 
Operationen: insert, extractMin, remove, decreaseKey (=Priorit√§t erh√∂hen)</p>

<p><strong>Adressierbare Priorit√§tslisten</strong>: Elemente in Priorit√§tsliste k√∂nnen ver√§ndert werden* ‚Üí zur Identifikation des zu ver√§ndernden Elements adressierbar,
z.B. remove nicht nur mit oberstem Element m√∂glich; Operationen in Priorit√§tslisten: build, size, insert, min, deleteMin, remove<em>, decreaseKey</em>, merge* 
(*neu gg√º einfachen Priorit√§tslisten);
Anwendungen: Dijkstra, Greedy-Algorithmen (<a href="https://de.wikipedia.org/wiki/Greedy-Algorithmus">Wikipedia</a>: ‚Äúschrittweise den Folgezustand ausw√§hlen, 
der zum Zeitpunkt der Wahl den gr√∂√üten Gewinn bzw. das beste Ergebnis verspricht‚Äù)</p>

<p><strong>Heap-Eigenschaft</strong>: Wurzel eines Teilbaums muss immer kleiner sein als seine Kindknoten (‚Üí min-heaps; max-heaps gibt es aber auch)</p>

<p><strong>Wald heap-geordneter B√§ume</strong>: nicht nur einen Baum ‚Üí ganzer Wald = Menge an B√§umen; kein Bin√§rbaum, Knoten kann mehrere Kinder haben<br />
Operationen:</p>
<ul>
  <li><em>Cut:</em> Teilbaum eines Baumes, als eigenen Baum in den Wald einf√ºgen (Knoten der Wurzel werden soll: parent := null)</li>
  <li><em>Link:</em> Verkn√ºpfen zweier B√§ume a,b; wenn a &lt; b ‚Üí b Teilbaum von a: $ union(a,b): link(min(a,b), max(a,b)) $</li>
</ul>

<p><strong>Pairing Heaps</strong>: Operationen:</p>
<ul>
  <li><em>insertItem(h: Handle):</em> Einf√ºgen eines neuen Baums f√ºr Handle h; $ O(1) $</li>
  <li><em>newTree(h: Handle):</em> Einf√ºgen eines neuen Elements in den Wald mit Handle h, zur effizienten Ausgabedes kleinsten Elements Pointer aktualisieren, 
wenn eingef√ºgtes Element kleiner als bisheriger min</li>
  <li><em>decreaseKey(h: Handle, k: Key):</em> Update Key des handles, wenn sich key verkleinert, unklar ob Baum noch heap-Eigenschaft erf√ºllt, deshalb: 
ist Element Wurzel ‚Üí update und fertig, wenn <em>keine</em> Wurzel ‚Üí cut; unbewiesen: $ O(log log n) ‚â§ T ‚â§ O(log n) $ ‚Üí sehr schnell in Realit√§t</li>
  <li><em>cut(h: Handle):</em> Herausschneiden aus Baum: Entfernen des gesamten Teilbaum und einf√ºgen als neuen Baum in Wald (newTree(h)); 
update des minPointers (minPtr)</li>
  <li><em>deleteMin : Handle:</em> Element m auf das minPtr zeigt wird aus Wald entfernt ‚Üí muss root sein, das kleinste Element kann nicht unterhalb eines Baumes sein, 
alle Kinder von m werden als neue Teilb√§ume in Wald eingef√ºgt; paarweise Vereinigung der Wurzeln, Vergleiche Wurzeln von nebeneinanderstehenden B√§umen und
h√§nge das gr√∂√üere Element unten an; $ O(log n) $</li>
  <li><em>merge(c: addressablePQ):</em> aus zwei Heaps einen Heap bilden; kleinere minPtr wird f√ºr neuen Heap √ºbernommen; Vereinigung der W√§lder der Heaps; 
Ursprungswald := ‚àÖ; $ O(1) $</li>
</ul>

<p>Repr√§sentation: Wurzeln in doppelt verketteter Liste; zwei Varianten (Vgl. Foliensatz 2 Abb 3-10); Pointer auf linken bzw. rechten Sibling, weil sonst 
Parent alle Pointer auf Kinder halten m√ºsste</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Variante 1:</td>
          <td>parent</td>
          <td>data</td>
          <td>left sibling</td>
          <td>right sibling</td>
          <td>one child</td>
          <td>‚Üí einfacher, mehr Speicherplatz</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Variante 2:</td>
          <td>data</td>
          <td>parent or left sibling</td>
          <td>right sibling</td>
          <td>one child</td>
          <td>‚Üí spart Platz, etwas komplizierter</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>Fibonacci-Heaps</strong>: Wichtige Begriffe:</p>
<ul>
  <li><em>Rang:</em> direkte Anzahl Kinder</li>
  <li><em>Verinigung nach Rang:</em> Unnion nur f√ºr gleichrangige Wurzeln (Union by Rank)</li>
  <li><em>Markieren:</em> Knoten die ein Kind verloren haben</li>
  <li><em>Kaskadierende Schnitte:</em> Schneide markierte Knoten (mehr als 2 Kinder verloren)</li>
</ul>

<p>Armotisierte Komplexit√§t: $ O( o + d log n)$ f√ºr d=#deleteMin, o=#otherOps,  n = maxAnzahlKnoten</p>

<p>Repr√§sentation: wie Variante 1, plus Rank, Markierung zusammen mit data speichern</p>

<p>Operationen:</p>
<ul>
  <li><em>deleteMin : Handle:</em> Element m auf das minPtr zeigt wird aus Wald entfernt ‚Üí muss root sein, das kleinste Element kann nicht unterhalb eines Baumes sein, 
alle Kinder von m werden als neue Teilb√§ume in Wald eingef√ºgt; solange ein a,b ‚àà forest existiert, f√ºr das gilt rank(a) = rank(b) ‚Üí  Vereinigung von a und b,
inkrement Rang der verbliebenen Wurzeln, update  minPt und return minPtr; Analyse: $ O(maxRank) = O(log n)  $ (Beweis siehe Foliensatz 02 Abb. 3-16), 
maxRank logarithmisch, $ 2^k + 1 √ó insert, 1 √ó deleteMin ‚Üí rank k $, Binomialb√§ume haben Struktur, dass Teilb√§ume von allen R√§ngen darunter enthalten sind
‚Üí Problem: durch Rausschneiden ergeben sich kleine aber hochrangige B√§ume ‚Üí Kaskading Cuts in Fibonacci Heaps</li>
  <li><em>Union-by-Rank:</em> Roots haben Rang, Array mit L√§nge des gr√∂√üten Rangs, jedes Element wird im Array an Stelle seines Rangs geh√§ngt, ist Feld besetzt, 
werden Elemente vereint (der gr√∂√üe nach aneinander geh√§ngt), dadurch ver√§ndert sich Rang und Baum muss an anderer Stelle im Array eingesetzte werden, ist 
dieses wieder besetzt muss wieder Vereinigt werden,‚Ä¶ (vgl Foliensatz 02 Abb. 3-15); Analyse: $ O(AnzahlUnions + AnzahlForest)$</li>
  <li><em>Cascading Cuts/Kaskadierenden Schnitte:</em> in decreseKey kein normaler cut, sondern cascadingCut(h): wenn h keine Wurzel ist, p := parent(h), unmark wenn
markierung, cut, wenn p marked dann cascadingCut(p), else mark p; Beweis, dass kaskadierende Schnitte maxRank logarithmisch halten (vgl. Foliensatz 02: 3-19)</li>
</ul>

<p>Name Fibonacci-Heaps: Bekannt, dass i-te Fibonacci-Zahl $ ‚â• 1.618^i $, exponentiell in i ‚Üí ein Baum mit Rank i enh√§lt exponentiell viele Knoten ‚Üí nur
logarithmisch viele Ranks, z.B. v mit rank(v) = i enth√§lt $ ‚â• F^{i+2} = 1.618^i+2 $ Elemente ‚Üí logarithmische Zeit f√ºr deleteMin 
(Beweis auf Foliensatz 02: 3-21)</p>

<p><em>Ende der Vorlesung vom 16.10.2018</em></p>

<h3 id="fortgeschrittene-graphenalgorithmen">Fortgeschrittene Graphenalgorithmen</h3>
<p><strong>K√ºrzeste Wege</strong>: Graph mit Knoten V und Kanten E, Kostenfunktion c und Anfangsknoten s; Gesucht L√§nge $ \mu (v)$ des k√ºrzesten Pfads von s nach v ‚Üí
Pfad mit der kleinsten Summe aus den Kosten der Kanten; zwei Knotenarrays: d[v] (vorl√§ufige Distanz von s nach v;  $ d[v] ‚â• \mu (v)$), 
parent[v] (Vorg√§nger von v auf Pfad von s nach v);<br />
<em>Initialisierung:</em> $ d[s] = 0, parent[s] = s; d[v] = ‚àû, parent[v] = ‚ä• $ (‚ä• = Bottom Symbol, undefined)<br />
<em>Kanten relaxieren:</em> falls $ d[u] + c(u,v) &lt; d[v] ‚Üí d[v] := d[u] + c(u,v) $ und $ parent[v] := u  $
<em>Pseudocode von Dijkstra:</em></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">initialize</span> <span class="n">d</span><span class="p">,</span> <span class="n">parent</span>
<span class="nb">all</span> <span class="n">nodes</span> <span class="n">are</span> <span class="n">non</span><span class="o">-</span><span class="n">scanned</span>
<span class="k">while</span> <span class="err">‚àÉ</span> <span class="n">non</span><span class="o">-</span><span class="n">scanned</span> <span class="n">node</span> <span class="n">u</span> <span class="k">with</span> <span class="n">d</span><span class="p">[</span><span class="n">u</span><span class="p">]</span> <span class="o">&lt;</span> <span class="err">‚àû</span>
    <span class="n">u</span> <span class="p">:</span><span class="o">=</span> <span class="n">non</span><span class="o">-</span><span class="n">scanned</span> <span class="n">node</span> <span class="n">v</span> <span class="k">with</span> <span class="n">minimal</span> <span class="n">d</span><span class="p">[</span><span class="n">v</span><span class="p">]</span>
    <span class="n">relax</span> <span class="nb">all</span> <span class="n">edges</span> <span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">)</span> <span class="n">out</span> <span class="n">of</span> <span class="n">u</span>
    <span class="n">u</span> <span class="ow">is</span> <span class="n">scanned</span> <span class="n">now</span></code></pre></figure>

<p>d ist anschlie√üend optimale Entfernung und parent zugeh√∂rige Wege; v erreichbar, es wird irgendwann gescannt; v gescannt $ \mu (v) = d[v]$</p>

<p><em>Laufzeit:</em> $T_{Dijkstra} = O(m ¬∑ T_{decreaseKey} (n) + n ¬∑ (T_{deleteMin} (n) + T_{insert} (n)))$ mit 
<a href="https://de.wikipedia.org/wiki/Fibonacci-Heap">Fibonacci-Heappriorit√§tslisten</a> (Wikipedia: ‚ÄúElemente mit festgelegter Priorit√§t 
in beliebiger Reihenfolge effizient im Heap gespeichert werden k√∂nnen und stets ein Element mit h√∂chster Priorit√§t 
entnommen werden kann.‚Äù), wenn zudem m hinreichend gro√ü (m &gt; log log n), dominiert m die Laufzeit und hinterer Teil nichtmehr relevant
 $T_{DijkstraFib} = O(m ¬∑ 1 + n ¬∑ (log  n + 1)) =
 O(m + n  log  n)$ ‚Üí ABER: konstante Faktoren sind gr√∂√üer als bei bin√§ren Heaps</p>

<p><em>Laufzeit im Durchschnitt:</em> maximal m decreaseKey Operationen (eine pro Kante) ‚Üí Erwartungswert der Laufzeit ist Durchschnitt: 
$ E[Anzahl decreaseKey-Operationen] = O(n log \frac {m} {n}) $  ‚Üí $ E[T_DijkstraBHeap] = O(m + n log log \frac {m} {n} log n) $, bei
dichten Graphen sogar lineare Laufzeit ($ m = Œ©(n log n log log n)$)</p>

<p><em>Ende der Vorlesung vom 22.10.2018</em></p>

<p><em>Pr√§fixminimum:</em> Zum Beweis, dass die Anzahl decreseKey Operationen in $ O(n log \frac {m} {n}) $ laufen: Ansehen der eingehenden Kanten von v, 
 in der Reihenfolge in der sie gescannt wurden, decrese Key findet statt, wenn $ Œº(u_i) + c(e_i) &lt; min (Œº(u_j) + c(e_j))$; wenn aber $ Œº(u_i) ‚â• Œº(u_j) $,
 (Wege zu $u_i$ k√ºrzer sind als zu $u_j$) f√ºr j &lt; i muss das <em>Pr√§fixminimum</em> gelten: $ c(e_i) &lt; min_{j&lt;i} c(e_j) $ ‚Üí es findet decreaseKey bei 
 Bearbeitung von $e_i$ statt; nur Absch√§tzung (sonst sehr schwierig, da Reihenfolge der $u_i$ unbekannt), es kann z.B. garkein decreaseKey kommen;
 Da erstes Minimum zu insert f√ºhrt gilt: $ ‚â§ H_k ‚àí 1 ‚â§ (ln k + 1) ‚àí 1 = ln k $ erwartete decreaseKeys 
 <em>Pr√§fixminima einer Zufallsfolge:</em> Da Folge zuf√§llig permutiert, ist jede Zahl mit der gleichen Wahrscheinlichkeit die kleinste: bei i Zahlen $ \frac{1}{i}$</p>

<p>Ziel: Dijkstras Algorithmus schneller zu machen, daf√ºr bereits angesehen: Fibonacci-Heaps (schlecht bei decreaseKey, im average-Case nicht so wichtig; ABER:
auch Betrachtung des Worst-Case). Daher nochmal Priorit√§tslisten ansehen:</p>

<p><strong>Monotone ganzzahlige Priorit√§tslisten</strong>: bei ganzzahligen Schl√ºsseln gut und ausnutzen, dass deleteMin immer monoton steigende Werte liefert ‚Üí weitere
Verbesserung von Dikjstras Algorithmus m√∂glich; sehr viel eingesetzt in Praxis (nicht nur Dijkstra);</p>

<p><strong>Bucket-(Priority)-Queue:</strong> zahlen in sehr engem Bereich zu repr√§sentieren (Ring um settled nodes); zyklisches Array B mit L√§nge C+1, 
jeder Arrayeintrag enth√§lt doppelt verkettete Liste von prioriy-queue Eintr√§gen, alle Eintr√§ge in Bucket haben gleiche Priorit√§t, 
Knoten mit Distanz d[v] wird in B[d[v] mod(C+1)] gespeichert<br />
<em>Initialisierung:</em> C+1 leere Listen, min = 0
<em>Operationen in Bucket Queues:</em></p>
<ul>
  <li>insert(v): v wird in B[d[v] mod(C+1)] eingef√ºgt; Analyse: $O(1)$</li>
  <li>decreseKey(v): entfernen von v und neu einf√ºgen in B[d[v] mod(C+1)] mit neuem Key; Analyse: $O(1)$</li>
  <li>deleteMin: anfangen bei B‚Ä¶ (im moment das Minimum gespeichert), falls leer inkrement min, solange bis nicht leerer Bucket gefunden (sonst geht nicht weil
Queue leer) ‚Üí hier Monotonizit√§t wichtig: wenn Schl√ºsselwerte sinken k√∂nnen, m√ºsste man ‚Äúr√ºckw√§rts schauen‚Äù k√∂nnen ‚Üí extra Kosten; Analyse: im Worst-Case
muss man einmal durch die Queue also $ O(nC) $ oder $ O(n+maxPathLength) $</li>
</ul>

<p><strong>Radix-Heaps</strong>: Array von -1 bis k Buckets ($ K = 1 + ‚åälog_2 C‚åã $); min ist zuletzt aus Q entfernte Distanz; f√ºr jeden Knoten v gilt 
$ d[v] ‚àà [min, . . . , min +C] $; nun hat man C m√∂gliche Schl√ºssel, aber nur log C m√∂gliche Buckets ‚Üí wie abbilden? ‚Üí ‚ÄúBitfummelei‚Äù :D 
Beispiel: C=9, bin√§r 1001 ‚Üí K=4; Vergleich bin√§r min und bin√§r v ‚Üí suche erste Stelle an der sie sich unterscheiden. Spezialfall: bei keinem 
Unterschied in B[-1]; Unterscheiden sie sich an einer gr√∂√üeren Stelle als K ‚Üí B[K]; sehr schnell zu berechnen mit Maschinenbefehl XOR ‚Üí Unterschiede
werden zur 1, gleiche Zahlen zur 0 ‚Üí suche nun h√∂chstwertige 1 in Ergebnis (Operation <em>msd(a,b)</em> = ‚Äúmost significant difference‚Äù)</p>
<ul>
  <li><em>deleteMin:</em> wenn B[-1] nicht leer, steht min dort; sonst suche erstes nicht leere Bucket, suche in der Liste dieses Buckets B[i] das min und schiebe 
es nach B[-1] ‚Üí neu ‚Äúaufr√§umen‚Äù mit msd-Formel in B[i] (nur B[i], alle anderen nicht betroffen) ‚Üí return B[-1]; Analyse: $ O(K+|B[i]|)$</li>
</ul>

<p><em>Ende der Vorlesung vom 23.10.2018</em></p>

<p><em>Vergleich Djkstra:</em></p>
<ul>
  <li>Dijkstra: $ T_{Dijkstra} = O(m ¬∑ T_{decreaseKey} (n) + n ¬∑ (T_{deleteMin} (n) + T_{insert} (n))) $</li>
  <li>mit Radix-Heaps: $ T_{DijkstraRadix} = O(m + n ¬∑ (K + K)) = O(m + n ¬∑ log C) $</li>
</ul>

<p><strong>All-Pairs Shortest Paths</strong>: k√ºrzeste Pfade f√ºr alle Paare bestimmen, negative Kosten erlauben, aber keine negativen Kreise;
Verschiedene M√∂glichkeiten:</p>
<ul>
  <li>n mal Bellman Ford: $ O(n¬≤m) $</li>
  <li>Knotenpotentiale: $ O(nm+n¬≤logn) $</li>
</ul>

<p><strong>Knotenpotentiale</strong>: Knoten bekommen Potentiale (von ‚Äúpotentieller Energie‚Äù in Physik, z.B. mit Elektroauto bergab fahren hat negatives Kantengewicht 
‚Üí kann √ºber Potential ausgedr√ºckt werden); Potentiale definierten <em>reduzierte Kosten</em>, Kosten finden gleiche k√ºzeste Pfade (genaue
Definion der Potentiale egal): Durch aufsummieren der Potentiale (erst positiv, dann negativ) ergibt sich $ ÃÑc(e) = pot(u) + c(e) ‚àí pot(v) $ (+ und - hebt sich auf
es bleibt nurnoch erster und letzter Knoten);<br />
<em>Einf√ºgen eines Hilfsknotens s:</em> von Knoten s aus Kanten zu allen Knoten einf√ºhren, die Kosten 0 haben. Anschlie√üend <a href="https://de.wikipedia.org/wiki/Bellman-Ford-Algorithmus">Bellman-Ford</a>
 zur k√ºrzesten Pfade Berechnung; definiere $ pot (v) := Œº(v) $ f√ºr alle v ‚Üí nun alle Kosten <em>nicht negativ</em> und Dijkstra kann verwendet werden, warum sind alle Kanten positiv:</p>
<ul>
  <li>Keine negativen Kreise, pot(v) = wohldefiniert ‚Üí l√§sst sich durch Bellman-Ford ‚Äúherausschneiden‚Äù</li>
  <li>F√ºr beliebige Kanten gilt: $ Œº(u) + c(e) ‚â• Œº(v) $  ‚Üí $ cÃÑ(e) = Œº(u) + c(e) ‚àí Œº(v) ‚â• 0 $
<em>Analyse:</em> s hinzuf√ºgen $O(n)$; Postprocessing (zur√ºck zu urspr√ºnglichen Kosten): $ O(n¬≤) $; n mal Dijkstra dominiert  ‚Üí $ O(n(m + n log n)) = O()nm + n^2 log n) $<br />
<em>Distanz zu Zielknoten:</em></li>
  <li>Bidirektionale Suche: abwechselnd s und t, Vorw√§rtssuche auf G, R√ºckw√§rtssuche auf G‚Äô (= umgedrehte Kantengewichte), bei jedem Schritt k√ºrzeste
Distanz speichern: $ d[s,t] = min(d[s,t], d_{forward} [u] + d_{backward} [u]) $; Abbruchkriterium: Suche scannt Knoten, der in anderer Richtung bereits gescannt wurde: $ d[s,t] ‚áí Œº(s,t)$</li>
  <li>A*-Suche: Scanne Knoten m√∂glichst nah an t, Funktion $f(v)$ sch√§tzt $Œº (v,t)$: $ pot(v) = f(v)$ und $ cÃÑ(u,v) = c(u,v) + f(v) ‚àí f(u) $;<br />
 f(v) ben√∂tigt folgende Eigenschaften:
    <ul>
      <li>Konsistenz (reduzierte Kosten nicht negativ): $c(e) + f(v) ‚â• f(u) ‚àÄe = (u, v)$</li>
      <li>wird t aus Q entfernt wenn f(t) = 0 und $ f(v) ‚â§ Œº (v,t) ‚àÄv ‚àà $</li>
    </ul>
  </li>
</ul>

<h3 id="anwendungen-von-dfs">Anwendungen von DFS</h3>
<p><strong>Tiefensuche (Depth-First-Seacht, DFS)</strong>: <a href="https://de.wikipedia.org/wiki/Tiefensuche">Tiefensuche</a> ‚ÄúPfad vollst√§ndig in die Tiefe beschritten, bevor abzweigende Pfade beschritten werden‚Äù; 
Algorithmus:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">unmark</span> <span class="nb">all</span> <span class="n">nodes</span><span class="p">;</span> <span class="n">init</span>      <span class="o">//</span><span class="n">dfsPos</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="mf">1.</span><span class="o">..</span><span class="n">n</span><span class="p">;</span> <span class="n">finishingTime</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="mf">1.</span><span class="o">..</span><span class="n">n</span>

<span class="n">foreach</span> <span class="n">s</span> <span class="err">‚àà</span> <span class="n">V</span> <span class="n">do</span>
    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">marked</span> <span class="n">then</span>
        <span class="n">mark</span> <span class="n">s</span>          <span class="o">//</span> <span class="n">make</span> <span class="n">s</span> <span class="n">a</span> <span class="n">root</span> <span class="ow">and</span> <span class="n">grow</span>
        <span class="n">root</span> <span class="p">(</span><span class="n">s</span><span class="p">)</span>        <span class="o">//</span> <span class="n">a</span> <span class="n">new</span> <span class="n">DFS</span><span class="o">-</span><span class="n">tree</span> <span class="n">rooted</span> <span class="n">at</span> <span class="n">it</span><span class="p">;</span>  <span class="n">dfsNum</span> <span class="p">[</span><span class="n">s</span><span class="p">]:</span><span class="o">=</span> <span class="n">dfsPos</span> <span class="o">++</span>
        <span class="n">DFS</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

<span class="n">Procedure</span> <span class="n">DFS</span> <span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="p">:</span> <span class="n">NodeId</span> <span class="p">)</span>      <span class="o">//</span> <span class="n">Explore</span> <span class="n">v</span> <span class="n">coming</span> <span class="k">from</span> <span class="n">u</span><span class="o">.</span>
    <span class="n">foreach</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="err">‚àà</span> <span class="n">E</span> <span class="n">do</span>
        <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="n">marked</span> <span class="n">then</span> <span class="n">traverseNonTreeEdge</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="k">else</span>
            <span class="n">traverseTreeEdge</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>         <span class="o">//</span> <span class="n">dfsNum</span> <span class="p">[</span><span class="n">w</span><span class="p">]:</span><span class="o">=</span> <span class="n">dfsPos</span> <span class="o">++</span>
            <span class="n">mark</span> <span class="n">w</span>
            <span class="n">DFS</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
            <span class="n">backtrack</span> <span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>        <span class="o">//</span> <span class="k">return</span> <span class="k">from</span> <span class="n">v</span> <span class="n">along</span> <span class="n">the</span> <span class="n">incoming</span> <span class="n">edge</span><span class="p">;</span> <span class="n">finishingTime</span><span class="p">[</span><span class="n">v</span><span class="p">]:</span><span class="o">=</span> <span class="n">dfsPos</span> <span class="o">++</span></code></pre></figure>

<p><strong>Starke Zusammenhangskomponenten (SCC)</strong>: fundamental Eigenschaft von Graphen, zur Anwendung, Analyse und Vereinfachung von Graphen ben√∂tigt. Relation ‚Üî* besagt, u ist zusammenh√§ngend mit
v, wenn es einen Pfad von u nach v gibt und einen Pfad von v nach u ‚Üí man kann in beide Richtungen gehen, Relation ist √Ñquivalenzrelation, definiert Relationsklassen: jede Teilmenge ist wiederum
 stark zusammenh√§ngend, beliebige Knoten in Teilmenge k√∂nnen sich gegenseitig erreichen; bei <em>Zusammenschrumpfen</em> des Graphen (=jede Zusammenhangskomponente durch neuen Knoten ersetzen, 
 entstehender Graph = <em>Schrumpfgraph</em>), dann entsteht ein <em>gerichteter Azyklischer Graph</em></p>

<h3 id="maximum-flows-und-matchings">Maximum Flows und Matchings</h3>
<p><strong>Network</strong>: <a href="https://de.wikipedia.org/wiki/Fl%C3%BCsse_und_Schnitte_in_Netzwerken#Netzwerk">Network</a>; directed weighted graph, source node $s$ and sink node $t$, s has no incoming edges, 
t no outgoing edges, weight $c_e$ is nonnegative capacity of edge $e$</p>

<p><strong>Flows</strong>: <a href="https://de.wikipedia.org/wiki/Fl%C3%BCsse_und_Schnitte_in_Netzwerken#Fluss">Fluss</a>; Funktion f die Netzwerk einen nichtnegativen Flusswert zuweist<br />
Bedingungen:</p>
<ul>
  <li><em>Kapazit√§tskonformit√§t:</em> Flusswert auf Kante ist max. so gro√ü wie Kapazit√§t der Kante</li>
  <li><em>Flusserhalt:</em> in jeden Knoten muss genau so viel ‚Äúhineinflie√üen‚Äù wie ‚Äúherausflie√üen‚Äù</li>
</ul>

<p><strong>Cuts</strong>: <a href="https://de.wikipedia.org/wiki/Fl%C3%BCsse_und_Schnitte_in_Netzwerken#Schnitt">Schnitt</a>; echte Teilmengen S und T der Knoten in Netzwerk mit $s \in S$ und $t \in T$; 
Kapazit√§t des Schnittes ist Summe der Kapazit√§ten der Kanten von S nach T</p>

<blockquote>
  <p>Wert von Max-Flow = min. Kapazit√§t eines s-t-Cuts</p>
</blockquote>

<p><strong>Max-Flow Berechnung</strong>:</p>
<ul>
  <li><em>Linear:</em> Flussvariablen $x_e$ f√ºr jede Kannte, Fluss pro Kante hat max. Kapazit√§t, eingehende Fluss = ausgehender Fluss, maximiere ausgehende Kanten vom Startknoten s aus</li>
  <li><em>Augmenting Paths:</em> finde Pfad von s nach t, sodass Kante noch Kapazit√§t √ºbrig hat, s√§ttige Kante mit kleinster √ºbriger Kapauit√§t, passe Kapazit√§ten an (erstelle <em>Residualgraph</em>) und wiederhole</li>
</ul>

<p><strong>Residual Graph</strong>: <a href="https://de.wikipedia.org/wiki/Fl%C3%BCsse_und_Schnitte_in_Netzwerken#Residualnetzwerk">Residualnetzwerk</a>; Das Residualnetzwerk mit Residualgraphen $G_f$, Fluss $f$ und 
Residualkapazit√§ten $u_f$ zeigt restliche Kapazit√§ten des Netwerks an; Residualgraph besitzt gleiche Knotenmengen wie G und besteht aus den von $f$ nicht ausgelasteten Kanten und erg√§nzt
R√ºckkanten (f√ºr $f(e) &gt; 0$), und Kanten (wenn $f(e) &lt; c(e)$)</p>

<p><strong>Augmenting Paths</strong>: Pfad p von s nach t finden, sodass jede Kante $e$ nonzero residual capacity hat</p>

<p><strong>Ford Fulkerson Algorithmus</strong>: <a href="https://de.wikipedia.org/wiki/Algorithmus_von_Ford_und_Fulkerson">Algorithmus</a> zu Bestimmung des Maximalen Flusses; am 
Besten Beispiel auf Wikipedia anschauen</p>

<p><strong>Max-Flow-Min-Cut Theorem</strong>: <em>max-flow = min-cut</em>; offensichtlich: <em>any-flow ‚â§ max-flow ‚â§ min-cut ‚â§ any-cut</em>; Theorem gilt
f√ºr beliebige Flussnetzwerke, wenn alle ‚â§ zu = werden, Beweis: (S, T) flow = (S, T) cut capacity ‚áí (S, T) flow = max-flow = min-cut</p>

<p><strong>Blocking Flows:</strong> $f_b$ ist ein Blockierender Fluss, wenn alle Kanten $e$ von $f_b$ gleich der Kosten von $e$ sind: 
$‚àÄ paths p = \langle s,‚Ä¶,t\rangle: ‚àÉe ‚àà p : f_b(e) = c(e)$</p>

<p><strong>Dinitz-Algorithmus</strong>: <a href="https://de.wikipedia.org/wiki/Algorithmus_von_Dinic">Algorithmus</a>
<em>DinitzMaxFlow:</em> berechnet MaxFlow, R√ºckw√§rtsbreitensuche von t nach s, solange
ein Pfad von s nach t existiert, f√ºge Kante von u nach v in den Layer Graph L ein, mit der Distanz $d(v) = d(u)-1 $; ausgehend von t haben also alle Knoten
die direkt von t erreichbar sind die d = 1, t selbst 0, usw. dadruch entsteht ein layer graph (Schichtengraph), in dem alle Knoten mit dem selben 
Abstand zu t einer Schicht angeh√∂ren; finde einen Blocking Flow $f_b$ in L, verg√∂√üere den Fluss f um den eben gefundenen blocking flow $f_b$<br />
<em>blockingFlow:</em> langer Algorithmus, besteht aus 3 Teilen: breakthrough, extend, retreat, loop solange bis return Blocking flow $f_b$ wenn v=s</p>
<ul>
  <li>breakthrough: Œ¥ = minimale Residualkapazit√§t von Kanten im Graph</li>
  <li>extend: pushe Knoten w auf p</li>
  <li>retreat: delete last edge</li>
</ul>

<p>Analyse: $O(m+nm) = O(nm)$, d(s) erh√∂ht sich pro Runde min. um 1, streng polynomiell: O(mn¬≤), O(mn log n) min dynamic tree, at most 
$ 2 \sqrt{m} $BF computations $O((m+n)\sqrt{m}$</p>

<p><em>Unit Capacity:</em> min{indegree(v), outdegree(v)} = 1</p>

<p><em>Matching:</em> ungerichteter Graph G=(V,E) mit M ‚äÜ E, gdw. (V,M) max grad ‚â§ 1; M ist maximal wenn $\nexists e ‚àà E \ M : M ‚à™ {e} $ ist matching;
M hat maximale Kardinalit√§t $\nexists matching M‚Äô: |M‚Äô| &gt; |M|$</p>

<p><strong>Preflow</strong>: Fluss f√ºr den Eingangskanten gr√∂√üer sein d√ºrfen als Ausgangskanten: $ excess(v) := inflow - outflow ‚â• 0 $; Knoten (nicht s,t) ist
<em>aktiv</em> wenn excess &gt; 0</p>

<p><strong>Preflow-Push Algorithms</strong>: Familie von Algorithmen; maintain approximation d(v) of BFS distance from v to t in $G_f$</p>
<ul>
  <li>Invariante 1: $d(t) = 0$</li>
  <li>Invariante 2: $d(s) = n$</li>
  <li>Invariante 3: no steep edges $ ‚àÄ(v, w) ‚àà E_f: d(v) ‚â§ d(w) + 1 $</li>
</ul>

<p><em>Edge Directions:</em></p>
<ul>
  <li>steep: d(w) &lt; d(v) ‚àí 1</li>
  <li>downwards: d(w) &lt; d(v)</li>
  <li>horizontal: d(w) = d(v)</li>
  <li>upwards: d(w) &gt; d(v)</li>
</ul>

<p><em>Analyse:</em> $O(n¬≤m)$</p>

<p>*Ende Vorlesung vom 13.11.2018**</p>

<p><strong>FIFO Preflow push</strong>: Wie kann Anzahl nicht-saturierender Pushes verringert werden? ‚Üí FIFO besser als Stack  ‚Üí $O(n¬≥)$</p>

<blockquote>
  <p>Grunds√§tzlich gilt : $n ‚â§ m$ mit Knoten n und Kanten m, da ansonsten Knoten isoliert w√§ren und f√ºr den Fluss nicht betrachtet werden m√ºssten.</p>
</blockquote>

<p><strong>Highest Level Preflow Push</strong>: verbessert Dinitz weiter: Knoten, die m√∂glichst weit weg sind werden bearbeitet, ‚ÄúFluss sammelt sich unten an, bevor man ihn
wegschiebt‚Äù, select active nodes, that maximize $d(v)$, with bucket priority queue, not monotone, relabels pay for scan operations ‚Üí highest level preflow
push finds a max flow in $O(n¬≤\sqrt{n})$<br />
still better: with aggressive local relabeling and global relabelling and special treatment of nodes with $d(v) ‚â• n$ ‚Üí no node can connect to t across an empty
level</p>

<h3 id="randomisierte-algorithmen">Randomisierte Algorithmen</h3>

<blockquote>
  <p><strong>Randomisierte Algorithmen</strong> verwenden Zufall(sbits) zur Beschleunigung/Vereinfachung von Algorithmen</p>
</blockquote>

<p><strong>Las Vegas Algorithmen</strong>: Ergebnis immer korrekt, Laufzeit ist Zufallsvariable (z.B. quicksort, hashing)</p>

<p><strong>Monte Carlo Algorithmen</strong>: Ergebnis mit bestimmer Wahrscheinlichkeit p inkorrekt, k-fache Wiederholungen machen Fehlerwahrscheinlichkeit
exponentiell klein $p^k$</p>

<p><strong>Permutationseigenschaft</strong>: Sortiertheit</p>

<p><strong>Sortieren - Ergebnis√ºberpr√ºfung (Checking)</strong>: √úberpr√ºfung ob Vektor1 eine Permutation von Vektor‚Äô ist, durch Polynom berechnen  (Differenz der Werte 0) wenn
Vektoren permutiert; Polynom q kann h√∂chstens n Nullstellen haben, Auswertung an zuf√§lliger Stelle $x in F$, $p(q!= 0 und q(x) = 0) ‚â§ \frac{n}{|F|}$
‚Üí K√∂rper F muss m√∂glichst gro√ü gew√§hlt werden; Monte-Carlo ‚Üí Verkleinern von p durch mehrfache Durchf√ºhrung</p>

<p><em>Ende Vorlesung vom 19.11.2018</em></p>

<p><strong>Alternatives Checking mit Hash Funktion</strong>: Ist Folge $E$ Permutation von $E‚Äô$? ‚Üí h ist zuf√§llige Hash-Funktion mit
 Wertebereich $0..U-1$; Checker return $h(E) = h(E‚Äô)$; Zwei M√∂glichkeiten:</p>
<ul>
  <li>$E = E‚Äô$ ‚Üí Checker gibt definitiv gleichen Wert zur√ºck, das Hash Funktion auf gleiche Werte angewandt wurde</li>
  <li>$E \neq E‚Äô$ ‚Üí Wahrscheinlichkeit, dass $h(E) = h(E‚Äô) \leq \frac{1}{U}$</li>
</ul>

<p><strong>Perfektes Hashing</strong>: keine Kollisionen, Abbildung in Array L√§nge n bei n Elementen (injektiv)
 $\omega(n)$ bits nur f√ºr codierung der Hashfunktion</p>

<p><strong>Cuckoo Hashing</strong>: Hashtabelle mit L√§nge $(2+\epsilon)n$; jedes Element ist entweder durch erste Hashfunktion $h_1$ oder 
durch zweite Hasfunktion $h_2$ in bucket eingeordnet; ‚ÄúRausschmei√üen‚Äù bzw. ‚ÄúVerdr√§ngung‚Äù der Elemente und Neuordnung durch alternative 
Hashfunktion; schnell in Lookup und delete, konstant beim Einf√ºgen</p>
<ul>
  <li><em>Insert</em>: ist erfolgreicht, gdw der Graph nicht mehr Kanten als Knoten enth√§lt</li>
  <li><em>Rebuild</em>: notwendige rebuilds nicht mehr als $O(\frac{1}{n})$</li>
</ul>

<p><em>Ende Vorlesung vom 20.11.2018</em></p>

<h3 id="externe-algorithmen">Externe Algorithmen</h3>

<blockquote>
  <p>Im Extremfall: Rechenoperationen der CPU z√§hlen nichtmehr, nurnoch I/O Operationen</p>
</blockquote>

<p><strong>Sekund√§rspeichermodell</strong>: M = schneller Speicher der Gr√∂√üe M (Hauptspeicher), B = Blockgr√∂√üe (W√∂rter die auf einmal aus Hintergrund
speicher gelesen/geschrieben werden k√∂nnen) ‚Üí Analyse: Blockzugriffe z√§hlen</p>

<p><strong>Externe Stapel</strong>: Datei mit Bl√∂cken, 2 interne Puffer, pro Puffer je ein <em>Top-of-Stack</em>-Pointer (TOS); Warum zwei Puffer? ‚Üí bei nur 
einem Puffer, im Falle dass abwechselnd push &amp; pop aufgerufen werden w√ºrde ‚Üí Kreislauf ‚Üí alle 2 Operationen ein I/O Zugriff ‚Üí viel zu teuer</p>
<ul>
  <li><em>push</em>: falls Platz in Puffer, sonst Puffer 1 in Datei (push auf Blockebene) ‚Üí Puffer 1 und Puffer 2 tauschen Pl√§tze</li>
  <li><em>pop</em>: falls vorhanden, pop aus Puffer ‚Üí sonst lese Puffer 1 aus Datei (pop auf Blockebene)<br />
‚Üí $ O(\frac{1}{B}) $ I/Os pro Operation</li>
</ul>

<p><strong>Externes Sortieren</strong>: Eingabegr√∂√üe $n$, Gr√∂√üe $M$ des schnellen Speichers, Blockgr√∂√üe $B$; Vorgehen beim <em>externalMerge(a,b,c: File of Element)</em>
Elemente x = read(a) und y = read(b), von j=1 bis a+b: if x ‚â§ x ‚Üí write(c), x = read(a) else write(c), y = read(b)<br />
<em>Analyse:</em> $\leq 3 + 3 \frac{|a|+|b|}{B}$ ‚Üí Bedingung: 3 Pufferbl√∂cke: $M &gt; 3B$</p>

<p><strong>Run Formation</strong>: sortiere Eingabeportionen der Gr√∂√üe M; I/Os ca. $2\frac{n}{B}$</p>

<p><strong>Externe Priorit√§tslisten</strong>: Problem bei Binary Heaps: $\theta(log \frac{n}{M})$ I/Os pro deleteMin</p>

<p><strong>Minimale Spannb√§ume</strong>: Semiexterner Kruskal: Annahme: $M = \Omega(n)$ konstat viele Maschinenworte pro Knoten</p>

<p><em>Ende Vorlesung vom 26.11.2018</em></p>

<h3 id="approximationsalgorithmen">Approximationsalgorithmen</h3>

<blockquote>
  <p>Umgang mit NP-harten Probleme, fast alle interessanten Optimierungsprobleme sind NP-hart ‚Üí trotzdem optimale L√∂sungen suchen und riskieren, dass 
Algorithmus nicht fertig wird, wie gut ist L√∂sung? ‚Üí <strong>Approximationsalgorithmen</strong>: polynomielle Ausf√ºhrungszeit, aber L√∂sung die ‚Äúnah‚Äù am Optimum 
ist, also die <em>effiziente</em> Berechnung einer <em>vern√ºnftigen N√§herung</em>.</p>
</blockquote>

<p><strong>Scheduling</strong> unabh√§ngiger gewichteter Jobs auf parallelen Maschinen: identische Maschinen, unabh√§ngige Jobs, bekannte Ausf√ºhrungszeiten, 
offline:</p>
<ul>
  <li>$x(j)$: Maschine auf der Job $j$ ausgef√ºhrt wird</li>
  <li>$L_i$: $ \sum_{x(j)=i} t_j $ Last von Maschine i</li>
  <li>Zielfunktion: Minimiere Makespan $L_{max} = max_i L_i$</li>
</ul>

<p><em>$L_{max}$ bei vielen kleinen Jobs:</em></p>
<ul>
  <li><em>obere Schranke:</em> Falls $l$ der zuletzt beendete Job ist: $L_{max} ‚â§ \sum_j \frac{t_j}{m} + \frac{m-1}{m} t_l$</li>
  <li><em>untere Schranken:</em> $L_{max} ‚â• \sum_j \frac{t_j}{m}$ und $L_{max} ‚â• max_j t_j$</li>
</ul>

<p><strong>Approximationsfaktor</strong>: Der Approximationsfaktor ist das <em>G√ºtekriterium</em> eines Approximationsalgorithmuses. 
Ein Minimierungsalgorithmus ALG erzielt einen Approximationsfator $\rho$ bez√ºglich Zielfunktion $f$, 
falls er f√ºr alle Eingaben $I$, eine L√∂sung $ALG(I)$ findet, so dass: $ \frac{f(ALG(I))}{f(OPT*(I))} ‚â§ \rho $ wobei $OPT(I)$ die optimale
L√∂sung f√ºr die Eingabe $I$ bezeichnet.   <br />
‚Üí List Scheduling erzielt einen Approximationsfaktor von $2-\frac{1}{m}$
<em>Beispiel</em>: Ein Algorithmus ALG sch√§tzt die Distanz einer Strecke x auf die n√§chste Zweierpotenz $2^{\lceil log|x| \rceil}$.
Der Algorithmus OPT bestimmt die korrekte Distanz |x|: $\frac{w(ALG)}{w(OPT)} = \frac{2^{\lceil log|x| \rceil}{|x|}} 
‚â§ \frac{2^{log |x| + 1}}{|x|}} = \frac{2|x|}{|x|} = 2 = \rho$ <br />
F√ºr $|x| = 2^{10} + 1 ‚Üí \frac{2^{11}}{2^{10}+1} = s - \frac{2}{2^{10}+1} ‚âà 2 $</p>

<p><em>Ende Vorlesung vom 27.11.2018</em></p>

<p><strong>Traveling Salesman Problem (TSP)</strong>: ist ein NP-vollst√§ndiges Problem, bei dem eine Reihenfolge f√ºr den Besuch mehrerer 
Orte so zu w√§hlen ist, dass keine Station au√üer der ersten mehr als einmal besucht wird, die gesamte Reisestrecke des 
Handlungsreisenden m√∂glichst kurz und die erste Station gleich der letzten Station ist.</p>

<p><em>Nichtapproximierbarkeit des TSP:</em> Gegeben ein Graph G = (V,V √ó V), finde einen einfachen Kreis C = (v_1,v_2,‚Ä¶,v_n,v_1), 
so dass n = |V| und $\sum_{(u,v) \in C} d(u,v)$ minimiert wird. Es ist NP-hart TSP innerhalb irgendeines Faktors a zu approximieren. 
Aus diesem Grund werden nun Absch√§tzungen getroffen:</p>

<p><strong>Absch√§tzung durch Hamiltonkreis</strong>: HamiltonCycle $‚â§_p$ a-Approximation von TSP</p>

<p><em>Hamiltonkreis</em>: ist ein geschlossener Pfad in einem Graphen,der jeden Knoten genau einmal enth√§lt. Das Hamiltonkreisproblem NP-vollst√§ndig.</p>

<p><strong>2-Approximation durch minimalen Spannbaum (MST)</strong>: <em>Gesamtgewicht MST ‚â§ Gesamtgewicht jeder TSP-Tour</em></p>

<p><em>Euler-Kreis</em>: Ein Euler-Kreis ist ein Zyklus, der alle Kanten eines Graphen genau einmal enth√§lt. 
Ein beliebiger zusammenh√§ngender, ungerichteter Graph G = (V,E) und |E| = m hat einen Euler-Kreis genau dann, 
wenn G zusammenh√§ngend und alle Knoten einen geraden Grad haben. <br />
<em>Analyse:</em> Euler-Kreise lassen sich in O(|E| + |V|) finden.</p>

<p><em>Minimale Spannb√§ume</em>: Ein Spannbaum ist ein Teilgraph eines ungerichteten Graphen, der ein Baum ist und alle Knoten 
dieses Graphen enth√§lt. Spannb√§ume existieren nur in zusammenh√§ngenden Graphen. Ein Spannbaum ist<em>minimal</em>, wenn kein anderer 
Spannbaum in demselben Graphen mit geringerem Gewicht existiert.</p>

<p><em>3/2 Approximations Algorithmus f√ºr TSP:</em> Gegeben Graph G = (V,E) (vollst√§ndig, ungerichtet): 
(hier Abbildung TSP-graph.png einf√ºgen)</p>
<ol>
  <li>bestimmte MST T</li>
  <li>bestimmte Knoten U mit ungeradem Grad in T</li>
  <li>finde minimales perfektes Matching M auf (U,E) (Summe der Gewichte der Matchingkanten minimal)</li>
  <li>f√ºgen Kanten M zu T ‚Üí T‚Äô hinzu</li>
  <li>bestimme Eulerkreis EK auf T‚Äô (alle Knoten haben geraden Grad)</li>
  <li>wandle EK zu Hamiltonkreis</li>
</ol>

<p>(Hier alle 6 Abbildungen einf√ºgen TSP-i.png)</p>

<p><em>Beweis:</em> Zun√§chst folgende Absch√§tzung: w(HK) ‚â§ w(EK) = w(T‚Äô) = w(T) + w(M) ‚â§ w(OPT) + w(M).
Nun muss w(M) bestimmt werden:</p>
<ul>
  <li>Sei HK‚Äô ein Hamiltonkreis auf U (Knoten mit ungeradem Grad T) (erzeugt durch √úberspringen aller
Knoten V ohne den Knoten in U im OPT) (Abbildung beweis-tsp-hk.png)</li>
  <li>definiere alternierende perfekte Matchings $M_1, M_2$ auf HK‚Äô (muss existieren, da |U| gerade, HK‚Äô = $M_1 \cup M_2$)<br />
(Abbildungen beweis-tsp-m1.png und beweis-tsp-m2.png)  <br />
‚Üí $w(M) ‚â§ w(M_1), w(M) ‚â§ w(M_2)$ (M min. Matching!)<br />
‚Üí $2¬∑w(M) ‚â§ w(M_1) + w(M_2) = w(HK‚Äô) ‚â§ w(OPT)$  (√úberspringen gerader Knoten und Dreiecks-Ungleichung)<br />
Daraus ergibt sich die Absch√§tzung: $w(HK) ‚â§ \frac{3}{2} w(OPT)$</li>
</ul>

<p>(√úberschrift Approximationsklassen)</p>

<p><strong>Pseudopolynomielle Algorithmen (APX)</strong>: Eine M√∂glichkeit mit Problemen umzugehen, die ansonsten NP-hart sind sind <em>Pseudopolynomielle
Algorithmen</em>.  Ein Algorithmus A ist ein polynomieller Algorithmus wenn seine Laufzeit ein Polynom im numerischen Wert der Eingabe ist 
(A(n) ‚àà P(n)). Dabei ist n die Anzahl der Eingabebits, wenn alle Zahlen un√§r (Beispiel: 0: , 1: |, 4: ||||) kodiert werden.</p>

<p><strong>Polynomial Time Approximation Scheme (PTAS)</strong>: f√ºr eine eingegebene Instanz I sowie einen Fehlerparameter Œµ, bei
 einer Polynomiellen Zeit in |I|, ist ein Algorithmus A ist ein Polynomial Time Approximation Scheme (PTAS) f√ºr</p>
<ul>
  <li><em>Minimization Problem</em>, wenn  die Ausgabequalit√§t f(x) = (1+Œµ)opt.</li>
  <li><em>Maximization Problem</em>, die Ausgabequalit√§t f(x) = (1-Œµ)opt.</li>
</ul>

<p><strong>Fully Polynomial Time Approximation Scheme (FPTAS)</strong>: Wenn ein PTAS zudem polynomielle G√ºte 
1/Œµ ben√∂tigt, hei√üt er Fully Polynomial Time Approximation Scheme (FPTAS).</p>

<p><strong>√úbersicht</strong>:</p>
<ul>
  <li><em>Approximable (APX):</em> œÅ = konstant, T polynomiell in n (z.B. $T(n, Œµ) = n^{\frac{1}{Œµ}}, œÅ(n, Œµ) = 2$)</li>
  <li><em>Polynomial time approximation scheme (PTAS):</em> œÅ = 1 ¬± Œµ, bel. Œµ ‚àà ( 0 , 1 ) , T poly. in n  (z.B. $T(n, Œµ) = n^{\frac{1}{Œµ}}, œÅ(n, Œµ) = 1 - Œµ$)</li>
  <li><em>Fully Polynomial time approximation scheme (FPTAS):</em> œÅ = 1 ¬± Œµ, bel. Œµ ‚àà ( 0 , 1 ) , T poly. in n  (z.B. $T(n, Œµ) = \frac{1}{Œµ}n, œÅ(n, Œµ) = 1 + 2Œµ$)</li>
</ul>

<p>(Hier Bild approximationsklassen.png einf√ºgen)</p>

<p><em>Beispielschranken:</em></p>
<ul>
  <li>PTAS: $n+2^{1/Œµ}, n{log 1/Œµ}, n^{1/Œµ}, n^{42/Œµ¬≥}, n+2^{2^{1000/Œµ}}$</li>
  <li>FPTAS: $n¬≤+\frac{1}{Œµ}, n+\frac{1}{Œµ‚Å¥}, \frac{n}{3} $</li>
</ul>

<p>(Hier Bild approximationsklassen.png einf√ºgen)</p>

<p>(√úberschrift Beispielalgorithmen Approximationsklassen)</p>

<p><strong>Rucksackproblem als Polynomieller Algorithmus</strong>: Aus einer Menge n von Objekten, die jeweils ein Gewicht $w_i$ und einen Profit $p_i$ haben, soll eine Teilmenge x 
ausgew√§hlt werden, deren Gesamtgewicht eine vorgegebene Gewichtsschranke W nicht √ºberschreitet. Unter dieser Bedingung 
soll der Profit P der ausgew√§hlten Objekte maximiert werden. <br />
Die kleinste Kapazit√§t der Gegenst√§nde 1,‚Ä¶,i die einen Profit ‚â• P ergeben: $ ‚àÄ 1 ‚â§ i ‚â§ n : C(i, P) = min(C(i ‚àí 1, P),C(i ‚àí 1, P ‚àí p_i ) + w_i )$
<em>Analyse:</em> PÃÇ ist die obere Schranke f√ºr den Profit: O(nPÃÇ) pseudo polynomiell und Speicherbedarf PÃÇ + O(n) Maschinenworte plus PÃÇn bits</p>

<p><strong>FPTAS f√ºr das Rucksackproblem</strong>: maximaler Einzelprofit $P := max_i p_i$, Skalierungsfaktor 
$K := \frac{ŒµP}{n}$, skaliere Profiten $p_{i^{‚Äò}} := \lfloor \frac{p_i}{K} \rfloor$, x‚Äô := dynamicProgrammingByProfit(p‚Äô,w,C)
gibt x‚Äô aus
<em>Analyse</em>: p¬∑x‚Ä≤ ‚â• (1 ‚àí Œµ)opt und O(n¬≥/Œµ)</p>

<h3 id="fixed-parameter-algorithmen">Fixed-Parameter-Algorithmen</h3>
<p>Eine weiter M√∂glichkeit mit NP-harten Problemen umzugehen sind <em>Fixed Parameter Algorithmen</em>. Um diese ‚Äúschwierigen‚Äù
Probleme zu l√∂sen (z.B. Minimum Vertex Cover), welche f√ºr allgemeine Instanzen zu lange Berechnungszeiten haben, gibt es die
M√∂glichkeit mit parametrisierten Algorithmen diese Spezialf√§lle zu berechnen. Neben der Eingabegr√∂√üe gibt es nuneinen zweiten Parameter 
<em>k</em>. Falls die Komplexit√§t der Berechnung des Problems in diesem Parameter steckt, kann man dadzrch effiziente
L√∂sungen f√ºr ein konstantes k erreichen. Dieser kann beispielweise k = Ausgabegr√∂√üe gew√§hlt werden. 
Beispiel: Finde die maximale Anzahl an Leuten, so dass in einem Graph alle Leute direkt oder √ºber einen Freund erreicht werden bzw.
mit einem Parameter k: Finde maximal k Leute, so dass alle Leute direkt oder √ºber einen Freund erreicht werden k√∂nnen. Letzteres ist einfach
zu l√∂sen.</p>

<p><strong>Fixed Parameter Tractable</strong>: Formal ist ein Problem parametrisierbar (auch: fixed parameter tractable oder FPT), 
wenn ein Algorithmus existiert, der es mit einer Laufzeit von  $O(f(k) \cdot p(n))$ l√∂st, 
wobei f eine berechenbare Funktion, k der Parameter, p ein beliebiges Polynom und n die Eingabel√§nge ist. f ist unabh√§ngig von n
und p ist unabh√§ngig von k.</p>

<p><em>Beispiele:</em> $2^k n^k, k^{k!}n^333, n + 1.1^k$ <br />
<em>Gegenbeispiele:</em> $n^k, n^{log log k}$</p>

<p><strong>Vertex Cover</strong>: ist ein klassisches NP-hartes Problem und ein trivialer Brute-Force-Algorithmus $(O(n^(k+1))$. Ein Vertex Cover ist
zu einem gegebenen einfachen Graphen und einer nat√ºrlichen Zahl k eine Knoten√ºberdeckung der Gr√∂√üe von 
h√∂chstens k. In anderen Worten: Eine Teilmenge U, die aus maximal k Knoten besteht, ist ein Vertex Cover, wenn jede Kante des 
Graphen mit mindestens einem Knoten aus U verbunden ist.</p>

<p>(Hier Bild vertex-cover.png einf√ºgen)</p>

<p>Vertex Cover ist ein FPT bez√ºglich des Parameters der Ausgabekomplexit√§t. Mit Hilfe der Entwurfstechniken <em>Kernbildung</em> und <em>Suche mit
beschr√§nkter Tiefe</em> werden zwei Algorithmen zu L√∂sung von Vertex Cover Problemen entwickelt:</p>

<p><em>Kernbildung (Kernelization):</em> Reduktionsregeln reduzieren das Problem aus die Gr√∂√üe O(f(k)) (also wird p(n) wegreduziert). Die
Probleminstanz wird auf den (eigentlichen schwieirigen) Problemkern reduziert und dieser dann mit einer anderen Technik gel√∂st.  Die Beobachtung
hinter der Idee der Kernbildung ist, dass alle Knoten v gr√∂√üer sein m√ºssen als k. Daraus ergibt sich, dass v entweder in der L√∂sung
des Vertex Covers liegen muss oder es keine L√∂sung f√ºr das Vertex Cover Problem gibt. Insgesamt ergibt sich damit eine Laufzeit 
von $O(nk+2^kk^2)$ <br />
Kernbildung ist au√üerdem eine wichtige Vor/Zwischenverarbeitungsstrategie f√ºr Optimierungsprobleme und auch polynomiell l√∂sbar.</p>

<p>(Hier Bild kernbildung.png einf√ºgen)
(Hier Bild bsp-kernbildung.png einf√ºgen)</p>

<p><em>Systematische Suche mit beschr√§nkter Tiefe:</em> Der Parameter k beschr√§nkt die Tiefe, wodruch ersch√∂pfendes
 Aufz√§hlen und Testen aller M√∂glichkeiten durch einen geeigneten Suchbaum m√∂glich sind. F√ºr den Fall k = 0 bricht der Algorithmus ab und die Laufzeit
wird damit beschr√§nkt. Die Rekursionstiefe k f√ºhrt zu $O(2^k)$ rekursiven Aufrufen, also insgesamt zu einer Laufzeit von $O(2^k(n+m))$. Die L√∂sung der
Rekursion: T(k) = (n+m) + 2T(k-1).</p>

<p>(Hier Bild tiefenbeschr-suche.png einf√ºgen)</p>

<h3 id="parallele-algorithmen">Parallele Algorithmen</h3>
<p>Parallele Algorithmen werden eingesetzt, um eine bessere Performance zu erreichen.</p>
<ul>
  <li><em>Geschwindigkeitssteigerung:</em> Eine Anzahl von p Computern k√∂nnen ein Problem bis zu p mal so schnell l√∂sen. Allerdings sind
dabei gute Koordinationsalgorithmen notwendig.</li>
  <li><em>Energieersparnis:</em> Wenn zwei Prozessoren mit halber Taktfrequenz arbeiten, ben√∂tigen sie weniger Energie als ein voll getakteter Prozessor.</li>
  <li><em>Speicherbeschr√§nkung</em> von Einzelprozessoren</li>
  <li><em>Kommunikationsersparnis:</em> Wenn Daten anfallen, kann man sie auch parallel (verteilt) (vor)verarbeiten.</li>
</ul>

<p><strong>Modell Nachrichtengekoppelte Parallelrechner</strong>: In nachrichtengekoppelten Parallelrechner existieren P <em>Prozessoren</em>, die <em>RAMs</em> (PRAMs) sind.
Eine <em>asynchrone Programmabarbeitung</em> ist gew√§hrleistet, sowie <em>Interaktion</em> der Prozessoren durch <em>Nachrichtenaustausch</em>. Jeder
Prozessor kann gleichzeitig maximal eine Nachricht senden und empfangen (vollduplex). Dabei existiert eine <em>Punkt-zu-Punkt</em> Verbindung,
sowie eine <em>vollst√§ndige Verkn√ºfung</em>. F√ºr eine Nachrichtenl√§nge l dauert der Austausch der Nachricht $T_{comm}(l) = T_{start} + l T_{byte}$.
Wobei $T_{start}$ die Latenz ist und $T_{byte}$ die Kommunikationsbandbreite, welche stets gr√∂√üer als $T_{start}$ ist.</p>

<p>(Hier Bild modell-nachr-parallel.png einf√ºgen)</p>

<p><em>Bulk Synchronous Parallel Computers (BSP)</em>: bezeichnet ein Modell des massiv parallelen Rechners. Das BSP-Modell besagt, dass die 
Laufzeit eines parallelen Algorithmus nicht nur von dem Grad der sequentiellen Teile abh√§ngt, sondern von mehreren Parametern. 
BSP ber√ºcksichtigt die Kosten des kollektiven Nachrichtenaustausches aller Rechner in Abh√§ngigkeit zur Nachrichtenl√§nge.</p>

<p><em>Speicherkonflikte in PRAMs:</em> Beim zeitgleichen Zugriff mehrerer Rechner auf einen geteilten Speicher, kann es zu Speicherkonflikten
kommen. Diese lassen sich in Lese- (read (R)) bzw Schreibzugriffe (write (W)) und in gleichzeitigen (concurrent (C)) bzw. exklusiven
Zugriff (exclusive (E)) klassifizieren. Daraus ergeben sich die folgenden Zugriffsarten:</p>
<ul>
  <li>EREW: exclusive Read, exclusive Write</li>
  <li>ERCW: exclusive Read, concurrent Write (Schwachsinn!)</li>
  <li>CREW: concurrent Read, exclusive Write</li>
  <li>CRCW: concurrent Read, concurrent Write
    <ul>
      <li>common: Alle Prozessoren m√ºssen den gleichen Wert schreiben.</li>
      <li>arbitrary: Der Wert eines zuf√§lligen Prozessors muss geschrieben werden.</li>
      <li>priority: Der Wert des Prozessors mit der kleinsten Prozessor-ID wird geschrieben.</li>
      <li>combine: Die Aggregation der Werte aller Prozessoren wird geschrieben (z.B. Summe).</li>
    </ul>
  </li>
</ul>

<p><strong>Analyse paralleler Algorithmen</strong>: Im Gegensatz zur Analyse sequentieller Algorithmen, bei der die Laufzeit nur abh√§ngig
von der Eingabe I ist, ist die Laufzeit bei parallelen Algorithmen zus√§tzlich abh√§ngig von der Prozessorenanzahl p. Ziel ist es
die <em>Ausf√ºhrungszeit</em> $T(I,p)$ zu finden. <br />
<em>Work:</em> $W=pT(p)$ ist das Kostenma√ü des Algorithmus. Wird p klein genug, ist die Arbeit lediglich vond er Instanzgr√∂√üe I abh√§ngig. <br />
<em>Span:</em> $T_{\infty} = sup_p T(p)$ misst die Parallelisierbarkeit bzw. die Zeit die ben√∂tigt werden w√ºrde, wenn beliebig 
viele Prozessoren zum Einsatz kommen. Das bedeutet, dass alle Berechnungen, die gleichzeitig stattfinden k√∂nnen, auch tats√§chlich 
gleichzeitig stattfinden. <br />
<em>(absoluter) Speedup:</em> $S=T_seq/T(p)$ ist die Beschleuning die gegen√ºber des besten sequentiellen Algorithmus erreicht werden kann. <br />
<em>Effizienz:</em> $E = S/p$. mit dem Ziel, dass E ‚âà 1 bzw. E = Œò(1). Die <em>superlineare Beschleuningung</em> f√ºr E &gt; 1, ist nur bedingt
m√∂glich. Die parallele Maschine hat mehr Ressourcen und kann daher besser arbeiten, aufgrund des Emulationsoverhead ist ein 
solcher Speedup eher unwahrscheinlich.</p>

<p><strong>Reduktion am Beispiel Assoziativer Operationen</strong>: Ein assoziativer Operator, der in konstanter Zeit berechnen werden kann, 
l√§sst sich zerlegen und in der Zeit O(log p) berechnen. Kommutative Beispiele sind + , ¬∑ , max, min, ein nichtkommuatatives
Beispiel ist die Matrixmultiplikation.</p>

<p>(hier bild assoziation.png einf√ºgen)</p>

<p><em>Beispiel Summe:</em> Zur verteilten Berechnung einer Summe addieren benachbarte Prozessoren ihre lokale Summe, dabei f√ºhrt der
Prozessor mit geradem Index die Berechnung aus. Der Prozessor mit ungeradem Index schickt sein Ergebnis an den benachbarten Prozessor
mit gerader Summe. Dies wird mehrfach ausgef√ºhrt, am Ende berechnet Prozessor 0 die Gesamtsumme. <br />
<em>Analyse</em>: Bei n Prozessoreinheiten ben√∂tigt die Berechnung der Summe O(log n) Zeit. Der Speedup ist O(n/log n) und die Effizienz O(1/log n).</p>

<p>(hier bild verteilte-summe.png einf√ºgen)</p>

<p>F√ºr p Prozessoreinheiten addiert nun jeder Prozessor n/p Elemente sequentiell, ehe er dann die parallele Summe f√ºr p 
Teilsummen berechnet. <br />
<em>Analyse:</em> Daf√ºr wird insgesamt die Zeit $T_{seq}(n/p) + Œò(log p)$ ben√∂tigt. Die Effizienz ben√∂tigt 1/ (1 + Œò(p log(p)) /n) und wird
daher besonders gut (‚âà 1), wenn n gro√ü gegen p log p wird.</p>

<p>(hier bild verteilte-summe-inkl-sequentiell.png einf√ºgen)</p>

<p>Kapitel Verbindungsnetzwerke</p>

<p><strong>Vollverkabelung</strong>: Eine vollst√§ndige Verkabelung ist nur bei einer geringen Anzahl an Rechnern sinnvoll. F√ºr p Rechner sind die
Kosten einer Vollverkabelung $\frac{p \cdot (p-1)}{2}. Eine Vollverkabelung gibt es als Simplex (i ‚Üí j), Telefon (i ‚Üî j) und Duplex (i ‚Üí j, j ‚Üí i)</p>

<p><strong>Hyperw√ºrfel</strong>: Hyperw√ºrfel sind n-dimensionale Analogien zum Quadrat bzw. W√ºrfel. Sie dienen zur Kommunikation zwischen
Prozessoren und sind billiger als eine Vollvermaschung. Die Knoten sind jeweils mit dem Knoten verbunden, dessen Label in der Bin√§rrepr√§sentation
nur ein Bit unterschiedlich hat. Eine Verkabelung im Hyperw√ºrfel kostet p log p Verbindungen.<br />
<em>Analyse:</em> $T_{Prefix} = O((T_{Start} + l T_{byte}) log p) ist nicht optimal bei $lT_{byte} &gt; T_{start}$</p>

<p>(hier bild hypercube.png einf√ºgen)
(hier bild hyperw√ºrfelalg.png einf√ºgen)</p>

<p>Kapitel Sortieren</p>

<p><strong>Paralleler Quicksort</strong>: Der sequentielle Quicksort w√§hlt ein Pivotelement und ordnet alle Elemente so, dass 
d_0,‚Ä¶,d_{k‚àí1} ‚â§ v &lt; d_k,‚Ä¶,d_{n‚àí1}. Anschlie√üend werden f√ºr den ersten Teil ($d_0,‚Ä¶,d_{k‚àí1}$) und den 
zweiten Teil ($d_k,‚Ä¶,d_{n‚àí1}$) rekursive Aufrufe gestartet. Eine Parallelisierun, die eine Laufzeit von 
 $O(T_{start} log¬≤ p)$ ben√∂tigt, k√∂nnte beispielsweise so aussehen:</p>

<p>(hier Bild par-qs.png)</p>

<h3 id="geometrische-algorithmen">Geometrische Algorithmen</h3>
<p><em>Geometrische Algorithmen</em> besch√§ftigen sich mit der algorithmischen L√∂sung geometrisch 
formulierter Probleme. Ein zentrales Problem ist dabei die Speicherung und Verarbeitung 
geometrischer Daten. Im Gegensatz zur Bildbearbeitung, deren Grundelemente Bildpunkte 
(Pixel) sind, arbeitet die algorithmische Geometrie mit geometrischen Strukturelementen 
wie Punkten, Linien, Kreisen, Polygonen und K√∂rpern. Beipiele f√ºr Probleme geometrischer
Algorithmen sind Konnvexe H√ºlle, Kleinste einschlie√üende Kugel, Range Search.</p>

<p><strong>Elementare geometrische Objekte</strong> sind Punkte $x \in R^d$, Strecken $\overline{ab}:= { Œ± a + (1 ‚àí Œ± )b : Œ± ‚àà [0, 1]}$
und noch mehr wie z.B. Halbr√§ume, Ebenen und Kurven.</p>

<p><strong>Dimension d</strong>:</p>
<ul>
  <li><em>1:</em> oft trivial und gilt nicht als geometrisches Problem</li>
  <li><em>2:</em> Geographische Informationssystem (GIS), Bildverarbeitung,‚Ä¶</li>
  <li><em>3:</em> Computergrafik, Simulationen, ‚Ä¶</li>
  <li><em>‚â• 4:</em> Optimierung, Datenbanken, maschinelles Lernen, ‚Ä¶ (curse of dimensionality)</li>
</ul>

<p><strong>Typische Fragestellungen</strong>:</p>
<ul>
  <li>Schnittpunkte zwischen n Strecken</li>
  <li>Konvexe H√ºlle</li>
  <li>Triangulation von Punktmengen (z.B. Dalunaytriangulierung: Kein Dreieck enth√§lt weitern Punkt)</li>
  <li>Vornoi-Diagramme: Unterteilung von $M ‚äÜ R^d$ in n Voronoizellen</li>
  <li>Punktlokalisierung</li>
</ul>

<p><strong>Geometrische Datenstrukturen</strong>:</p>
<ul>
  <li><em>Baumstrukturen:</em>
    <ul>
      <li>1-dim: Interval Tree</li>
      <li>2-dim: Quad Tree, Wavelet-Tree</li>
      <li>3-dim: BinarySpacePartition Tree</li>
      <li>n-dim: k-d-Tree</li>
    </ul>
  </li>
  <li><em>Facetten:</em> Delaunay Triangulierung (), Voronoi Diagramm</li>
</ul>

<p><strong>Streckenschnitt (line segment intersection)</strong> <br />
Beim <em>Streckenschnitt</em> will man gegeben n Strecken S={s1,‚Ä¶,sn} Schnittpunkte zwischen den Strecken finden. Ein
allgemeiner Anwendungsfall sind <em>Plane-Sweep-Algorithmen</em> die unter anderem f√ºr die Kontruktion f√ºr konvexe H√ºllen
oder Voronoi-Diagrammen ben√∂tigt werden. Die Anzahl der ausgegebenen Schnitte ist k.</p>

<p>(img streckenschnitt.png)</p>

<p>Die <em>Sweep-Line</em> strukturiert die Abrarbeitung eines Problems. Dabei wird ausgenutzt, dass geometrisch nahe Objekte sich
gegenseitig beeinflussen und geometrisch weit entferne Objekte (nahezu) unabh√§ngig sind. Im Allgemeinen werden n-dim
auf (n-1)-dim reduziert.</p>

<p><em>Dominierung</em>: Ein Punkt hei√üt nicht-dominiert genau dann, wenn kein anderer Punkt in der Menge existiert, dessen Koordinaten
alle gr√∂√üer oder gleich der Koordinaten des Punktes sind. Formal dominiert a den Punkt b genau dann, wenn ‚àÄi : ai ‚â§ bi
und ‚àÉi : ai &lt; bi.</p>

<p>(img dom.png)</p>

<p><em>Beispiel: Berechnung einer Skyline:</em> Dabei sind nur H√∂hen√§nderungen relevant. Jede √Ñnderung definiert ein eindimensionales
Problem (Maximumsbildung). Bei der Berechnung des Skylines Problem m√ºssen <em>nicht dominierte Punktmengen</em> berechnet werden.</p>

<p>Das naive Vorgehen zur Suche der Schnittpunkte w√§re f√ºr jedes Streckenpaar die Schnittpunkte zu √ºberpr√ºfen, was
mit einer Laufzeit von Œò(n¬≤) zu langsam bei gro√üen Datenmengen ist.</p>

<p><em>Plane-Sweep-Algorithmen</em>: Eine (waagrechte) <em>Sweep-Line l</em> verl√§uft von oben nach unten. Die Invariante dieser 
Algorithmen ist, dass Schnittpunkte oberhalb von <em>l</em> bereits korrekt ausgegeben wurden. Der Ansatz ist jeweils Segmente, 
die l schneiden zu speichern und deren Schnittpunkte zu finden.</p>

<p>(img plane-sweep1.png)</p>

<p><em>Plane-Sweep f√ºr orthogonale Strecken</em> <br />
Ein m√∂glicher <em>Plane-Sweep-Algorithmus f√ºr den orthogonalen Streckenschnitt</em> ben√∂tigt die Gesamtlaufzeit von O(n log n + k).</p>

<p>(img plane-sweep-orth-pseudo.png)</p>

<p><em>Verallgemeinerung</em>
Nun wied der orthogonalen Plane-Sweep-Algorithmus unter der Annahme, dass keine horizontalen Strecken und keine
√úberlappungen existieren, sowie dass Schnittpunkte jeweils im inneren von genau zwei Strecken liegen, verallgemeinert. 
Dabei kann man beobachten, dass kleine zuf√§llige Perturbationen (= St√∂rungen) die allgemeine Lage produzieren. <br />
Der <em>Status T</em> ist definiert als eine nach x geordnete Folge der l schneidenden Strecken. Ein <em>Ereginis</em> ist definiert
als eine Status√§nderung der Sweep-Line (sie ber√ºhrt einen Startpunkt, Endpunkt oder Schnittpunkt). Der <em>Schnitttest</em> wird 
nur f√ºr Segmente ausgef√ºhrt, die an einem Ereignispunkt in T benachbart sind.</p>

<p>(img plane-sweep-verallg.png)</p>

<p>Die <em>Korrektheit</em> ergibt sich dadurch, dass zwei Geraden s,t, die einen Schnittpunkt (x,y) gemeinsam haben, ein Eregnis triggern
sodass die beiden Geraden auf l Nachbarn werden. Zu Beginn sind s,t nicht benachbart, bei einem y + Œµ sind s und t benachbart, das
hei√üt es muss ein Ereignis geben, bei dem s und t Nachbarn werden.</p>

<p>(img korrektheit-ps.png)</p>

<p><em>Implementierung</em></p>

<p>(img ps-impl1.png ps-impl2.png ps-impl3.png ps-impl4.png)</p>

<p><em>Beispiel</em></p>

<p>(img ps-bspi.png for i=1,..,16)</p>

<p>Dieser Algorithmus ben√∂tigt eine <em>Laufzeit</em> von O((n+k) log n).</p>

<p><strong>2D Konvexe H√ºlle</strong> <br />
Eine Menge hei√üt <em>konvex</em>, wenn f√ºr je zwei beliebige Punkte, die zur Menge geh√∂ren, auch stets deren Verbindungsstrecke 
ganz in der Menge liegt. Die <em>konvexe H√ºlle</em> einer Teilmenge ist die kleinste konvexe Menge, die die Ausgangsmenge enth√§lt.
Die Menge wird also komplett von der konvexen H√ºlle eingeschlossen.</p>

<p>Gegeben einer Menge P = {p1,‚Ä¶,pn} ‚àà R¬≤ wird ein konvexes Polygon K mit Eckpunkten in P (also P ‚äÜ K) gesucht.</p>

<p><em>Graham‚Äôs Scan</em> ist ein Algorithmus zur Berechnung der konvexen H√ºlle in O(sort(n)), also O(n log n). Dazu werden die
Punkte in P lexikographisch nach (x,y) (zuerst x, wenn x gleich, nach y) sortiert. (p1&lt;p2&lt;.. &lt;pn). Zudem wird nur die
obere H√ºlle berechnet.</p>

<p>(img grahams-scan.png grahams-scan-bsp.png)</p>

<p><strong>Kleinste einschlie√üende Kugel</strong> <br />
Die <em>Kleinste einschlie√üende Kugel</em> ist eine kugelf√∂rmige konvexe H√ºlle. Gegeben einer Menge P = {p1,‚Ä¶,pn} ‚àà R¬≤ wird 
die kleinste einschlie√üende Kugel mit Radius K in P (also P ‚äÜ K) gesucht.</p>

<p><em>Smallest Enclosing Ball (sEB)</em> ist ein Algorithmus zur Berechnung der kleinsten einschlie√üenden Kugel in O(n).</p>

<p>(img smallest-encl-ball.png)</p>

<p>Zur <em>Korrektheit</em> ist zu Zeigen, dass ein x $\notin$ B existiert, das x auf dem Rand von sEB(P) liegt. Durch Kontraposition
kann man zeigen, dass ein x, das nicht auf dem Rand von sEB(P) liegt, auf dem sEB(P{x}) liegen muss, was wiederum = B ist.
Dadurch muss x ‚àà B gelten.<br />
Smallest Enclosing Balls sind <em>eindeutig</em>. Zum Beweis wird angenommen, dass zwei sEBs B1, B2 existieren, welche nicht gleich
sind. Daraus folgt ‚Üí P ‚äÜ B1 ‚àß P ‚äÜ B2 und P ‚äÜ B1 ‚à© B2 ‚äÜ sEB(B1 ‚à© B2) =: B. Aber dann muss gelten radius(B) &lt; radius (B1), 
was ein Widerspruch zur Annahme, dass B1 ein sEB ist.</p>

<p>(img seb-korr.png)</p>

<p>Die <em>Analyse</em> der kleinsten einschlie√üenden Kugel ergibt, dass allgemein eine Zeit von T(p,0) ‚â• d!n zur Berechnung von sEB gilt.</p>

<p>(img seb-anal.png)</p>

<h2 id="√ºbung">√úbung</h2>
<h3 id="√ºbung-1">√úbung 1</h3>
<p><strong>Amortisierte Analyse:</strong> <a href="https://de.wikipedia.org/wiki/Amortisierte_Laufzeitanalyse">Amortisierte Laufzeitanalyse</a> wurde in Algorithmen I behandelt. Im
Gegensatz zur allgemeinen Laufzeitanalyse wird der Worst-Case in mehreren Durchl√§ufen des Algorithmuses analysiert. Ziel ist es die durchschnittliche 
obere Schranke im Worst-Case zu bestimmen, da in z.B. teure Operationen nur selten auftreten.</p>

<p><strong>Kontomethode:</strong> <a href="https://de.wikipedia.org/wiki/Account-Methode">Account-Methode</a>: Verfahrensweise der armotisierten Laufzeitanalyse; wichtig: bei 
Tokenerzeugung darf es keine Zyklen geben!</p>

<p><strong>Kontomethode mit Fibonacci-Heaps:</strong></p>
<ul>
  <li><em>Insert:</em> Erzeuge neuen Baum mit Element zum einf√ºgen, f√ºge ihn den Wurzeln hinzu, ggf. min Pointer anpassen; erstelle Union-Token 
(je Wurzel ein Union-Token); insert kostet konstante Zeit</li>
  <li><em>deleteMin:</em> siehe √úbungsfolie 1</li>
</ul>

<h3 id="√ºbung-2">√úbung 2</h3>
<p><strong>Spezielle Priority Queues</strong>: monoton, ganzzahlig; Bedingungen:</p>
<ul>
  <li>positive, ganzzahlige Schl√ºssel</li>
  <li>neue/ge√§nderte Schl√ºssel ‚â• minmale Schl√ºssel</li>
  <li>maximales Schll√ºsselinkrement C (bezogen auf min. Schl√ºssel)</li>
</ul>

<p><strong>Bucket Queues</strong>: zirkul√§re Liste aus Buckets, Variable min, init min = 0; C Schl√ºsselinkremente ‚Üí #Buckets |B| = C + 1</p>
<ul>
  <li><em>insert:</em> Einf√ºgen in B[key mod(C+1)]; Laufzeit: $O(1)$</li>
  <li><em>deleteMin:</em> Entfernen aus B[min mod(C+1)] oder aus erstem nicht-leeren Folgebucket; Laufzeit: $O(C)$</li>
  <li><em>decreaseKey:</em> Verschieben von altem in neues Bucket; Laufzeit $O(1)$ <br />
‚Üí Abstand min-Element und max-Element maximal C ‚Üí ansonsten geht einf√ºrgen nicht; vgl. <em>Spezielle Priority Queues</em>-Bedingungen; Problem sonst, dass man kleinste Element finden, obwohl noch
kleineres Element in Queue vorhanden</li>
</ul>

<p><strong>Radix Heaps</strong>:  Liste aus Buckets, Variable min; C Schl√ºsselinkremente ‚Üí  #Buckets $ |B| = K + 2 = (\lfloor log C \rfloor)  + 1 ) + 2 $ z.B. mit C = 9 ‚Üí 6 Buckets; maximal k√∂nnen pro Bucket[i]
$max(1,2^i)$ verschiedene Werte in Bucket stehen</p>
<ul>
  <li><em>Bucket[i]:</em> i = min(msd(min, key), K)</li>
  <li><em>Bucket[k]:</em> Overflow Bucket</li>
  <li><em>Bucket[-1]:</em> min<br />
Vorgehen:</li>
  <li><em>insert:</em> Einf√ºgen in Bucket B [min( msd(min, key), K)]; Laufzeit: $O(log C)$ ‚Üí da wenn min entfernt wird, alle Elemente verschoben werden m√ºssen, erstellen nach Kontomethode bei insert
bereits genug Token um verschieben zu ‚Äúbezahlen‚Äù</li>
  <li><em>deleteMin:</em> min = minimaler Schl√ºssel (aus Bucket i), Elemente aus Bucket i verschieben, Entfernen aus Bucket B [‚àí 1 ]; Laufzeit: $O(log C)$</li>
  <li><em>decreaseKey:</em> Verschieben von altem in neues Bucket; Laufzeit $O(1)$<br />
‚Üí Warum Formel [min( msd(min, key), K)]? ‚Üí bei min √Ñnderung potentiell alle Buckets zu ver√§ndern ‚Üí schlechte Laufzeit und man muss nicht alle Elemente verschieben, d.h. armotisierte 
Laufzeit Erkl√§rung funktioniert nicht</li>
</ul>

<p><strong>Average Case Analyse f√ºr Minimale-Spannb√§ume (MST)</strong>: Minimaler Spannbaum ist Baum aus Graph, der nur Kanten aus Graph verwendet, alle Knoten miteinander verbindet und Kanten sollen
minimales Gewicht haben; Jarnik-Prim Algorithmus:</p>
<ul>
  <li>W√§hle beliebigen Startknoten s</li>
  <li>F√ºre Knoten v zu MST mit kleinstem Abstand d[v]<br />
Analyse: m Kanten, mit zuf√§llig geordneten Gewichten ‚Üí wie oft wird decreaseKey im Durchschnitt ausgef√ºhrt? ‚Üí Nachbarn durchnummerieren, in Reihenfolge in der sie zu MST hinzugef√ºgt
wird, wann muss Kante relaxiert werden ‚Üí Distanz an v ist gr√∂√üer als Kante die sich nun ergeben hat ‚Üí DecreaseKey wird dann ausgef√ºhrt, wenn d[v] &gt; d[√ºber neu hinzugef√ºgte Kanten]<br />
Wie oft passiert das? ‚Üí $ E( M_k ) = H_k $ ‚Üí $ O ( n ln \frac{m}{n}) $</li>
</ul>

<h3 id="√ºbung-3">√úbung 3</h3>
<p><strong>Dijkstras Algorithmus</strong>: nicht-negative Kantengewichte, vorl√§ufige Distanzen in Priority Queue d, unterscheidet zwischen
erreichten Knoten und gescannten Knoten</p>

<p><strong>Bidirektionale Suche</strong>: Beschleunigung, zweimal Dijkstra ausf√ºhren (Vorw√§rtssuche s ‚Üí t auf G, R√ºckw√§rtssuche von t ‚Üí s auf $G^r$ mit umgekehrten
 Kanten); Vorgehen: einen Schritt Vorw√§rtssuche, einen Schritt R√ºckw√§rtssuche, Abbruch wenn ein Knoten von beiden Suchen gescannt (nicht nur erreicht);</p>

<p><strong>A*-Suche</strong>: Dijkstra in Richtung t gesteuert, Potentialfunktion pot ordnet jedem Knoten reele Zahl zu; Vorgehen entwerder reduzieren der
Kantengewichte $$ \bar{c}(u, v) := c(u, v) + pot(v) ‚àí pot(u)$ <strong>oder</strong> modifizieren der Schl√ºssel $\bar{d}[v] : = d[v] + pot(v)$; g√ºltige Potentialfunktion:</p>
<ul>
  <li>untere Schranke f√ºr Distanz zum Ziel t: $ pot(u) ‚â§ \mu(u,t) $</li>
  <li>nicht-negative reduzierte Kantengewichte: $ \bar{c}(u, v) := c(u, v) + pot(v) ‚àí pot(u) ‚â• 0 $</li>
  <li>t hat potential 0
Woher Potentialfunktionen? Sollen Sch√§tzung f√ºr tats√§chliche Distanz sein, z.B. Manhattan-Distanz, Euklidischer Abstand
A*-Suchen M√∂glichkeiten:</li>
  <li>Suche mit reduzierten Kantengewichten</li>
  <li>Suche mit ge√§nderten Schl√ºsseln in Priority Queue</li>
  <li>Suche mit Landmarks: Kompormiss, berechne Potentiale f√ºr einige Knoten, bei konkreter Anfrage w√§hle Landmark hinter dem Ziel, Landmarks immer
korrekt dank Dreicksungleichung (oben definierte Potentialfunktionseigenschaften m√ºssen gegeben sein), Algorithmus kann langsam sein (z.B. Suche
in  falscher Richtung)</li>
</ul>

<p><strong>Starke Zusammenhangskomponenten</strong>: 
Nomenklatur:</p>
<ul>
  <li><em>oNodes:</em> Stack, offene Knoten</li>
  <li><em>oReps:</em> Stack, Repr√§sentaten der offenen Komponenten</li>
  <li><em>aktive Knoten:</em> Knoten die w√§hrend dfs markiert, aber noch nicht finished, kein Backtracking</li>
  <li><em>offene SCCs:</em> enth√§lt nur aktive Knoten und keine abgeschlossenen Knoten</li>
  <li><em>abgeschlossene SCCs:</em> alle Knoten finished (markiert), backtracking gemacht</li>
  <li><em>Repr√§sentanten der SCC:</em> Knoten mit kleinster dfsNumber (Reihenfolge der Knotenabarbeitung bei Tiefensuche)  in SCC</li>
</ul>

<p>Invarianten:</p>
<ul>
  <li><em>Invariante 1:</em> Keine Kanten von geschlossenen in offene Komponenten.</li>
  <li><em>Invariante 2:</em> Offene Komponenten liegen auf Pfad.</li>
  <li><em>Invariante 3:</em> Repr√§sentanten partitionieren offene Komponenten bzgl. dfsNum.</li>
</ul>

<p><strong>Floyd-Warshall Algorithmus</strong>: kubischer Algorithmus, berechnet transitive H√ºlle</p>

<p><strong>Floyd-Warshall Algorithmus und SCCs</strong>: Transitive H√ºlle einer SCC ist ein vollst√§ndiger Graph, betrachte Schrumpfgraph, 
deutlich schneller: #SCC¬≥ &lt; n, Eintrag pro SCC</p>

<h3 id="√ºbung-4">√úbung 4</h3>
<p><strong>Residualgraph</strong>: Verwalten von Restkapazit√§ten, Modellierung und Erkennung von Gegenfl√ºssen, keine 0 Gewicht Kanten, Fl√ºsse
√ºber Kanten und Gegenkanten erlaubt</p>
<ul>
  <li><em>Restkapazit√§t:</em> $c^f(e) = c(e) ‚àí f(e)$</li>
  <li><em>Fluss:</em>  $c^f(e^{rev}) = f(e)$</li>
</ul>

<p><strong>Dinitz Distanz Label</strong>: geben Distanz im Residualgraphen zur Senke t an, R√ºckw√§rtsgerichtete Breitensuche ‚Üí Layered Graph</p>

<p><strong>Dinitz Blocking Flow</strong>: ‚ÄúAuf jedem Weg durch den Graphen mindestens  eine Kante bis zur maximalen Kapazit√§t ausgelastet ist‚Äù,
Berechnung auf Schichtgraph, kein R√ºckfluss m√∂glich; Berechnung basiert auf Tiefensuche von s aus:</p>
<ul>
  <li>extend: gehe einen Knoten n√§her ans Ziel (Schichtgraph)</li>
  <li>retreat: Sackgasse gefunden, gehe zur√ºck, l√∂sche Kante</li>
  <li>breakthrough: Tiefensuche hat Senke erreicht, l√∂sche saturierte Kanten
<em>Analyse</em>: $O(nm)$</li>
</ul>

<h3 id="√ºbung-5">√úbung 5</h3>
<p><strong>Potentialmethode</strong>: Stack mit Operationen:</p>
<ul>
  <li>push(v): Element v oben auf Stack legen $O(1)$</li>
  <li>pop: oberstes Element aus Stapel entfernen $O(1)$</li>
  <li>multipop(k): oberste k Elemente aus Stapel entfernen $O(n)$
Beweis: Warum $O(n)$ und nicht $O(n¬≤)$ wie vermutet f√ºr n Operationen im worst-case
Anzahl Elemente $\phi$ auf Stack $\phi = |Elemente auf Stack| \geq 0$; Push erh√∂ht $\phi$, pop bzw. multipop verringert $\phi$;
bei n Operationen $|push| \leq n$ maximal n Erh√∂hungen, Verringerungen max n ‚Üí alle √Ñnderungen $\leq 2n$ ‚Üí maximal O(n)</li>
</ul>

<p><strong>Preflow Push Algorithmus</strong>: Bezeichnungen:</p>
<ul>
  <li><em>Aktiver Knoten:</em> Knoten $v$ aktiv, wenn $excess(v) = inflow(v) - outflow(v) &gt; 0$</li>
  <li><em>g√ºltige Kante:</em> Kante $(v,w) \in G^{f}$ g√ºltig, wenn Level $d(v) = d(w) + 1 $<br />
+
Ablauf:
    <ol>
      <li>w√§hle <em>aktiven Knoten</em> $v$</li>
      <li>falls <em>g√ºltige Kante</em> $(v,w)$ existiert push (schiebe Fluss entlang $(v,w)$, vorausgesetzt: d(u) = d(v) + 1)</li>
      <li>ansonsten <em>relabel</em> (erh√∂ht Level eines Knoten, erh√§lt $d(u) \leq d(v) + 1 in G^{f}$, nur wenn push vom Knoten nicht m√∂glich)</li>
    </ol>
  </li>
</ul>

<p>Oft nicht klar, welche Knoten, Kanten etc, daher Heuristiken:<br />
Auswahl des aktiven Knotens:</p>
<ul>
  <li><em>generic preflow push:</em> $O(n¬≤m¬≥)$</li>
  <li><em>FIFO preflow push:</em> $O(n¬≥)$ Typisch f√ºr Ausf√ºhrung per Hand ‚Üí Klausur</li>
  <li><em>highest-level preflow-push:</em> $O(n¬≤\sqrt{m})$</li>
</ul>

<p>Unterschiedliches relabel:</p>
<ul>
  <li><em>aggressive local relabeling:</em> erh√∂he in einem Schritt so, dass g√ºltige Kante existiert</li>
  <li><em>global relabeling:</em> setze Levels nach: (Berechnung der Distanzen √ºber Breitensuche)
    <ul>
      <li>$\mu(v,t)$: falls t von v erreichbar</li>
      <li>$n+\mu(v,s)$: sonst, falls s von v erreichbar</li>
      <li>$2n-1$: sonst</li>
    </ul>
  </li>
  <li><em>gap heuristic:</em> wird Level durch relabel(v) leer, setze Level von v und aller von v erreichbaren Knoten auf $d(w) = max{d(w), n}$</li>
</ul>

<p>Knotenauswahl:</p>
<ul>
  <li><em>two-phase approach:</em>
    <ul>
      <li>Phase 1: w√§hle nur Knoten Level $d(v) &lt; n$ aus ‚Üí erzeuge maximum preflow ‚Üí korrekter Fluss in t</li>
      <li>Phase 2: nur noch Knoten mit Level $d(v) \geq n$ aus ‚Üí Fluss nur nach s m√∂glich</li>
    </ul>
  </li>
</ul>

<h3 id="√ºbung-6">√úbung 6</h3>
<p><strong>Randomisierte Algorithmen</strong>
<strong>Las Vegas Algorithmus</strong>: immer korrekte/optimale L√∂sung, Laufzeit ist Zufallsvariable (erwartete Laufzeit $E[T]$))</p>

<p><strong>Monte Carlo Algorithmus</strong>: falsche/suboptimale L√∂sung mit Wahrscheinlichkeit p, deterministische Laufzeit</p>

<p><strong>Monte Carlo Simulation</strong>: <strong>! ‚â† Monte Carlo Algorithmus !</strong>, Zufallsexperimente sehr oft ausf√ºhren, je l√§nger/√∂fter
ausgef√ºhrt, desto besser Ergebnis</p>

<p><strong>Las Vegas ‚Üí Monte Carlo</strong>: gegeben Las Vegas Algorithmus mit erwarteter Laufzeit $E[T] = f(n)$, gesucht Monte Carlo
Algorithmus mit Laufzeit $O(f(n))$ und Fehlerrate p <br />
<em>Idee:</em> Abbruch nach Zeit $\alpha f(n)$, Ausgabe Falsch wenn Algorithmus abgebrochen
wurde, Markov Ungleichung: $P[T &gt; \alpha f(n)] ‚â§ \frac{1}{\alpha}</p>

<p><strong>Monte Carlo ‚Üí Las Vegas</strong>: Monte Carlo Algorithmus mit Laufzeit $O(f(n))$ und Fehlerrate p Korrektheit in $O(g(n))$ √ºberpr√ºfbar,
 gesucht Las Vegas Algorithmus mit erwarteter Laufzeit $E[T] = f(n)$ <br />
 <em>Idee:</em> Wiederhole Monte Carlo bis korrektes Ergebnis gefunden, Laufzeit $T ‚â§ i * O(f(n) + g(n))$, $E[T] = \frac{1}{1‚Åªp}$</p>

<p><strong>Probability Booting</strong>: Matrix Multiplikation, nur false postives m√∂glich, bei p ‚â§ 0.5 schnelle Konvergenz, Laufzeit $O(kn¬≤)$, 
Wiederhole Test k mal mit unterschiedlicher Wahl von r:</p>
<ul>
  <li>liefert Test FALSCH: AB ‚â† C</li>
  <li>liefern ALLE Tests Korrekt: mit $p ‚â§ 0.5^k$ false positive</li>
</ul>

<p><strong>Coupon Collector</strong>: Ziel n versch. Sammelkarten in M√ºslipackungen: Erwartungswert $n H_n ‚â§ n ln n + n$</p>

<p><strong>Speichermodell</strong>
<strong>Parallel Disk Model (PDM)</strong>: Speicherzugriff in Bl√∂cken, Blockzugriffe minimieren (Datenlokalit√§t), Muster
wiederholt sich in Speicherhierarchie immer wieder</p>

<p><strong>I/O-effizientes Design</strong>: nicht nur relevant bei Disk I/O, z.B. Dijkstra ‚Üí Strukturierter Zugriff wichtiges Designprinzip  <br />
<em>Blockgr√∂√üen</em>:</p>
<ul>
  <li>$T_{seek}$: Positionierungszeit</li>
  <li>$W_{max}$: maximale Bandbreite<br />
‚Üí Lesedauer: $T = T_{seek} + \frac{B}{W_{max}}$  <br />
<em>Grundlegende Techniken</em>:</li>
  <li>Zugriffsmuster:
    <ul>
      <li>Random Access: $O(n)$ I/Os</li>
      <li>Linearer Scan: $O(\frac{n}{B})$ I/Os</li>
    </ul>
  </li>
  <li>Sortieren: $ sort(n) = O(\frac{2n}{B}(1+\lceil log_{M/B} \frac{n}{M}))$ I/Os</li>
  <li>Priorit√§tswarteschlangen: sort(n)I/Os</li>
</ul>

:ET