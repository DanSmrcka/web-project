I"ß!<div style="background-color: #EAEFF4; border: 1px solid #b5aeb1; border-radius: 3px;  padding: 10px; margin-right: 10px">
    <strong>Vorlesung:</strong> <a href="https://campus.studium.kit.edu/ev/vACNcrMpRvCbVANs7qJ92Q">Analysetechniken f√ºr gro√üe Datenbest√§nde</a>, Vorlesung, 5 ECTS <br />
    <strong>Dozent:</strong> Klemens B√∂hm  <br />
   <strong>ILIAS:</strong> <a href="https://ilias.studium.kit.edu/goto_produktiv_crs_883316.html">https://ilias.studium.kit.edu/goto_produktiv_crs_883316.html</a> <br />   
   <strong>Vorlesungswebsite:</strong> <a href="http://dbis.ipd.kit.edu/2629.php">http://dbis.ipd.kit.edu/2629.php</a> <br />   
   <strong>Videos:</strong> <a href="https://opencast.informatik.kit.edu/engage/ui/index.html?e=1&amp;p=1&amp;epFrom=a4ded4cb-24cf-4372-aa1f-7ae4c7f16091">https://opencast.informatik.kit.edu/engage/ui/index.html?e=1&amp;p=1&amp;epFrom=a4ded4cb-24cf-4372-aa1f-7ae4c7f16091</a> <br />   
   <strong>Klausur:</strong> m√ºndlich, daher nach Vereinbarung <br />
   <strong>Einordnung:</strong> Vertiefungsfach "Informationssysteme", Profil "Datenintensives Rechnen" <br />
</div>

<h2 id="organisatorisches">Organisatorisches</h2>

<h3 id="vorlesungen">Vorlesungen</h3>
<ul>
  <li><strong>16.10.2018</strong>: Organisatorisches und Foliensatz 1-1 bis 2-36</li>
  <li><strong>23.10.2018</strong>: Foliensatz 2-36 bis 3-8</li>
  <li><strong>30.10.2018</strong>: Foliensatz 3-9 bis 3-40, 4-1 bis 4-35, 5-1 bis 5-31</li>
  <li><strong>06.11.2018</strong>: Foliensatz 5-31 bis Ende, Kapitel 6, 7-1 bis 7-37</li>
  <li><strong>27.11.2018</strong>: Kapitel 8, 9-1 bis 9-20</li>
</ul>

<h3 id="material">Material</h3>
<p>Das Material der Vorlesung besteht aus:</p>
<ul>
  <li><strong>Folien</strong>: Folien werden im ILIAS hochgeladen</li>
  <li><strong>√úbungsbl√§ttern</strong><br />
Inhaltlich sind keine Ver√§nderungen zum Vorjahr geplant, unter Umst√§nden aber durchaus m√∂glich</li>
</ul>

<h3 id="√ºbungen">√úbungen</h3>
<p>Es gibt insgesamt 5 √úbungen, haupts√§chlich schreiben von Programmen in R, √úbungsbl√§tter gibt es zwei Wochen vor
dem √úbungstermin zum Download, Musterl√∂sung wird in VL besprochen, √úbungsaufgaben sind sehr wichtig</p>

<h3 id="praktikum">Praktikum</h3>
<p>Es gibt im Sommersemester das zugeh√∂rige Praktikum ‚ÄúAnalysetechniken f√ºr gro√üe Datenbest√§nde‚Äù, die VL wird
vorausgesetzt, zwei Datens√§tze anhand eine eigene Analyse der Daten stattfinden soll, wenn man teilnehmen m√∂chte
muss man das letzte √úbungsblatt bearbeiten und abgeben -&gt; sozusagen Anmeldung f√ºr Praktikum wenn das gut bearbeitet wurde</p>

<h3 id="klausur">Klausur</h3>
<p>Die Klausur soll m√ºndlich stattfinden (wenn Kursgr√∂√üe das zul√§sst), Pr√ºfungstermin kann man im Sekretariat des Lehrstuhls 
durch ausf√ºllen eines Formulars ausmachen, dabei gibt man den pr√§ferierten Monat der Pr√ºfung an (z.B. April 2019).
Inhaltliche Fragen: SQL, Frage oft ‚ÄúNenne ein Beispiel, aber nicht aus der Vorlesung‚Äù</p>

<h2 id="vorlesungsinhalt">Vorlesungsinhalt</h2>
<h3 id="einleitung">Einleitung</h3>
<p><strong>Worum geht es in der Vorlesung?</strong><br />
Erkennen von Zusammenh√§ngen in Datenbest√§nden, die:</p>
<ul>
  <li><strong>nichttrivial</strong>: nicht offensichtlich</li>
  <li><strong>implizit</strong>: belegbar durch Daten, Daten sollen Zusammenhang implizieren</li>
  <li><strong>potentiell n√ºtzlich</strong>: keine/kaum √úberlappung von Ergebniselementen/zuvor nicht bekannt</li>
</ul>

<p>Absicht hinter dem Herausfinden der Datenbest√§nde entweder <em>wissenschaftlicher
(Publikationen)</em> oder <em>kommerzieller (Geld) Natur</em>. Herausforderung also <em>neue, n√ºtzliche</em> Zusammenh√§nge 
herauszufinden. Dabei sind die Datenbest√§nde oft <em>sehr gro√ü</em> und mit <em>komplexer innerer Struktur</em>.</p>

<p><strong>Wichtige Data-Mining Probleme</strong></p>
<ul>
  <li><strong>Association Rules</strong>: Suche nach starken Regeln, Assoziationsanalyse z.B. im Warenkorb von Kunden 
(finden von <em>Frequent Item Sets</em>) mit dem Ziel eine Vorhersage zu treffen; Probleme: Performance, 
zu viele (√§hnliche) Association Rules -&gt; quantitative Angaben (wie oft wird ein Produkt 
mit einem anderen Produkt zusammen gekauft), zeitlichen Verlauf (nur wenn Item unmittelbar nacheinander gekauft 
wurde, nicht z.B. 3 Monate sp√§ter) ber√ºcksichtigen, aber auch geeignete Sprache verwenden, um das ganze zu 
Beschleunigen.</li>
  <li><strong>Clustering, Outlier Detection</strong>: Identifizieren von Gruppen in Datenmengen, die √§hnliche Eigenschaften
besitzen, nah beieinanderliegen z.B. Kundengruppen; nicht zu einem Cluster geh√∂rende Datenobjekte bezeichnet 
man als Outlier, d.h. Datenobjekte, die sich graphisch dargestellt, weit von den ‚Äúgeh√§uften‚Äù Datenpunkten 
entfernt sind. Attribute unterscheiden sich z.B. Zahlen, Aufz√§hlungstyp (Gruppen wie z.B. Haarfarbe) oder 
Mengenwertigkeit. Cluster k√∂nnen in einem Datenbestand unterschiedliche Form, Dichte und Gr√∂√üe besitzen.<br />
<em>Unsupervised:</em> vorab nicht bekannt welche Cluster es geben wird und wo ihre Grenzen verlaufen 
-&gt; abh√§ngig vom Datensatz<br />
<em>Supervised:</em> Klassenzugeh√∂rigkeit des Trainingsdatenbestandes vorher bekannt</li>
  <li><strong>Klassifikation</strong>:  Einteilung von Datenobjekten in Klassen mit dem Ziel anhand von n Attributen das
n+1te Attribut vorherzusagen. Als Grundlage Trainingsmenge von welchen alle n+1ten Attribute bekannt sind. <br />
Viele unterschiedliche Ans√§tze, unter anderem:<br />
<em>Linearer Classifier:</em> Gegeben n Attribute x, Schwellenwert T (Score), n Gewichte w, Datensatz ist in Klasse, 
gdw. <em>x * w &gt; T</em>, Gewichtung entscheidet √ºber Klasse, gro√üe Gewicht = wichtige Attribute, 
kleine Gewichte = unwichtige Attribue, hat ein Attribut keine Auswirkung auf die Klassifizierung kann es mit 
negativen Gewichten versehen werden; sinnvoll f√ºr reelwertige, boolsche Attribute<br />
<em>1-Rules:</em> sehr einfaches Vorhersageverfahren, betrachtet jedes Attribut f√ºr sich, anfangs auch jeden 
Attributwert, h√§ufigste Klasse ist diejenige, die vorhergesagt wird. Attribut mit kleinster Fehlerquote wird
als Grundlage f√ºr Vorhersage gew√§hlt. (Vgl. 1 - 29), einfaches, schlichtes Modell mit hoher Fehlerrate, reicht
f√ºr manche Anwendungsf√§lle aus<br />
<em>Overfitting</em>: zu kleinteiliges Modell, das zu sehr auf den Trainingsdatenbestand zugeschnitten ist und daher
im Allgemeinen schlecht performt</li>
</ul>

<p><strong>Frage: Geben Sie Beispiele f√ºr Anwendungsgebiete, in denen Clustering anwendbar ist?</strong><br />
Typen von Studierenden (z.B. flei√üige, faule, gute Studenten, schlechte Studenten)</p>

<p><strong>Frage: Skizzieren Sie ein Szenario aus den Naturwissenschaften, in dem Klassifikation 
sinnvoll einsetzbar ist.</strong><br />
Beispiel aus Vorlesung <em>Predictive Maintenance</em>, Vorhersage f√ºr Motoren bzw. alle komplexen technischen
Systeme, wann/welcher Schadensfall eintritt, dabei false positives oder false negatives m√∂glich. Zwei 
Herangehensweisen: <br />
<em>datenorientiert</em>: wenn aktueller Systemstand kristischem Zustand √§hnelt, dann Alarm; Voraussetzung, dass 
System sich in Zukunft so verh√§lt, wie in Vergangenheit (d.h. nicht unter menschlichem Einfluss stehen), 
ausreichend Trainingsdaten vorhanden sind, zusammenbringen von Messdaten und vorherzusagenden Daten, 
Infrastruktur muss relevante Ph√§nomene erfassen.<br />
<em>modellorientiert</em>: Experte formuliert ‚ÄúAlarmzust√§nde‚Äù per Hand
Bei Betrachtung der Testdaten wird unterschieden:<br />
<em>statisch:</em> jeder Zeitpunkt wird isoliert betrachtet <br />
<em>dynamisch:</em> zeitliche Entwicklung wird ber√ºcksichtigt -&gt; <em>Change Detection</em> entweder univariat (Zeitreihe
eindimensional) oder multivariat (von zwei Dimensionen abh√§ngig, Vektor)</p>

<p>Im Allgemeinen ist es nicht ausreichend Analyseverfahren zu verwenden, sondern zudem die Daten aufzubereiten
und in die Umgebung einzubetten. Au√üerdem ist es nciht offensichtlich, welche Daten wie zu analysieren sind.</p>

<p><em>Ende der 1. Vorlesung vom 16.10.2018</em></p>

<p><strong>Pr√ºfungsfrage: Was ist Overfitting?</strong>
Zu feinteilige Betrachtung der Trainingsdaten -&gt; performen, funktionieren nicht in Realit√§t</p>

<p><strong>Pr√ºfungsfrage: Was ist Change Detection? Wie unterschieden sich multivariate 
von univariaten Datenstr√∂men?</strong>
Bei dynamischer Betrachtung von Daten, d.h. Zeitpunkte werden in relation zueinander betrachtet, ist Change
Detection die Festellung, ob von Zeitpunkt t1 zu Zeitpunkt t2 eine Ver√§nderung der Attributwerte 
stattgefunden hat. Dabei ist univariate Change Detection eindimensional und multivariate Change Detection
mehrdimensional (z.B. ein Vektor)</p>

<h3 id="statistische-grundlagen">Statistische Grundlagen</h3>
<h4 id="beschaffenheit-der-daten">Beschaffenheit der Daten</h4>
<p><strong>Kategorisierung der Daten</strong><br />
Kategorische Werte (Qualitative Konzepte)</p>
<ul>
  <li><em>Nominal:</em> Keine nat√ºrliche Anordnunge der Werte (z.B. Farben, Namen)</li>
  <li><em>Ordinal:</em> Anordnung existiert (z.B. klein &lt; mittel &lt; gro√ü)</li>
</ul>

<p>Numerische Werte (Zahlen)</p>
<ul>
  <li><em>Diskret:</em> endliche Anzahl m√∂glicher Werte (z.B. ganze Zahlen aus [0,100])</li>
  <li><em>Kontinuierlich:</em> unendlich viele m√∂gliche Werte (z.B. reelle Zahlen aus [0,100])</li>
</ul>

<p><strong>Dimensionalit√§t der Daten</strong></p>
<ul>
  <li><em>eindimensional (univariat)</em> z.B. Alter 10, 51, 38</li>
  <li><em>multidimensional (multivariat)</em> z.B. x-y-Koordinaten (1;0), (4;8)</li>
  <li><em>hochdimensional</em> z.B. Kundenrepr√§sentation in e-Commerce Firma</li>
  <li><em>Daten ohne Dimension</em> z.B. Strings, Mengen</li>
</ul>

<p><strong>Metrische Daten</strong>: Daten ohne Dimension, deren Objektabstand sich aber berechnen l√§sst. F√ºr diese 
Daten gilt in metrischen R√§umen gegeben mit Dom√§ne M, Metrik d f√ºr alle Objekte p,q,r aus M:</p>
<ul>
  <li>Symmetrie: $d(p,q) = d(q,p)$</li>
  <li>Definitheit: $d(p,q) = 0 ‚áî p = q$</li>
  <li>Dreiecksungleichung: $d(p,r) ‚áê d(p,q) + d(q,r)$</li>
</ul>

<h4 id="einfache-deskriptive-statistiken">Einfache deskriptive Statistiken</h4>
<p><strong>Aggregate</strong>: Kombinieren alle Werte eines Attributs zu einem skalaren Wert, z.B. count, sum, min, max, avg, 
manche Aggregate sind parallelisierbar, andere nicht. Eine Aggregatsfunktion hei√üt <em>self-maintainable</em>, wenn
nach einer √Ñnderung der Daten, der neue Wert der Aggregatsfunktion aus dem Alten berechnet werden kann. 
z.B. min; min war 3, jetzt ist kommt 2 -&gt; ist 2 &lt; 3 -&gt; dann ist 2 neues minimum. ABER: min ist nicht
self-maintainable bez√ºglich des L√∂schens. Wenn 2 gel√∂scht wird, kann ich nichtmehr sagen, was das kleinste
Element danach war. Besseres Beispiel daher: count(). 
Klassifizierung von Aggregatsfunktionen:</p>
<ul>
  <li><em>distributiv:</em> Funktion F kann erst auf Teildaten angewandt werden und anschlie√üend existiert eine 
Funktion G mit der aus den Teilergebnissen, das Endergebnis berechnet werden kann. z.B. min, max, count 
(Definition 2 - 9)</li>
  <li><em>algebraisch:</em> Funktion G kann auf Teildaten angewandt werden, und liefert ein M-Tupel zur√ºck, 
das wiederum mit einer anderen Funktion H am Ende das gleiche liefert wie eine Funktion F direkt 
auf die gesamten Daten angewandt. z.B. avg. <em>Beispiel:</em> Zum Berechnen des Durchschnitts einer
Datenmenge, kann man auch aus Submengen ein Tupel mit der Anzahl der Elemente (count) und deren Summe
(sum) zur√ºckgeben. Aus diesem Tupel kann man nun den Durchschnitt berechnen. (Definition 2 - 9)</li>
  <li><em>holistisch:</em> keine Parallelisierung m√∂glich, keine Beschr√§nkung des Speicherbedarfs f√ºr Sub-Aggregate,
z.B. h√§ufigsterWert(), median(); Beispiel: h√§ufigsterWert() ist holistisch, da die einzelnen h√§ufigsten
Werte pro Submenge nicht aussagekr√§ftig sind, sondern nur eine Liste Auskunft dar√ºber gibt, welcher Wert
<em>insgesamt</em> am h√§ufigsten Auftritt, L√§nge der Liste abh√§ngig von Anzahl der Daten.</li>
</ul>

<p><strong>Frage: Warum ist h√§ufigster Wert holistisch?</strong><br />
siehe oben</p>

<p><strong>Frage: Welcher Aggregatsfunktion l√§sst sich Trucated Average (Richter, h√∂chste und niedrigste Note wird
getrichen und daraus der Durchschnitt berechnet)</strong><br />
Algebraisch. Man berechnet mit Funktion G vier Werte: Anzahl Richter, Summe der Noten, h√∂chste Note pro
Scheibe, niedrigste Note pro Scheibe. Daher das Zwischenergebnis ist ein M-Tupel mit M=4. Aus diesem
Zwischenergebnis kann die Endnote berechnet werden. Aus unterschiedlichen Schichten kann nun der gesamt 
h√∂chste/niedrigste Wert gestrichen werden und anschlie√üend den Trucated Average berechnen.</p>

<p><strong>Pr√ºfungsfrage: Was ist der Zusammenhang zwischen Aggregatsfunktionen und self-maintainability?</strong>
?</p>

<p><strong>Pr√ºfungsfrage: Warum ist Median holistisch?</strong>
Weil der Median der Wert ist, der in der Mitte steht ist. Das hei√üt es m√ºsste jeweils eine
 sortierte Liste ausgegeben werden, und anschlie√üend aus der Anzahl der Elemente der mittlere
 Wert berechnet werden. Da die einzelnen Schichten aber nur intern sortiert sind kann man
 keinen Schluss darauf ziehen, wie der Median des gesamten Datensatz aussieht.</p>

<p><strong>(gewichtetes) arithmetisches Mittel</strong>: ugs. ‚ÄúDurchschnitt‚Äù, $Summe / Anzahl$, Klassifikation <em>algebraisch</em>, 
nur f√ºr numerische Daten</p>

<p><strong>Midrange</strong>: Mitte des tats√§chlichen Wertebereichs, $(max - min) / 2$</p>

<p><strong>Median</strong>: Mittlerer Wert, d.h bei sortierten Werten, der Wert der in der Mitte liegt. Bei ungerader Anzahl
der Durchschnitt der beiden Werte in der Mitte, Klassifikation <em>holistisch</em>, auf numerische und ordinale Daten
anwendbar (z.B. klein - mittel - gro√ü).</p>

<p><strong>Modalwert / Modus</strong>: Wert der im Datenbestand am h√§ufigsten Vorkommt, f√ºr nominale, ordinale Daten,
schwierig ei kontinuierlichen Daten. Wenn jeder Wert nur einmal vorkommt ist Modus nicht definiert.</p>

<p><strong>Quartile</strong>: Q1 = oberste 25%, Q3 = oberste 75%, IQR (‚ÄúInter-Quartile Range‚Äù) = Q3 - Q1</p>

<p><strong>Au√ürei√üer</strong>: Werte die mehr als 1,5 * IWR von Q1 nach unten bzw von Q3 nach oben abweichen.</p>

<p><strong>Boxplots</strong>: Zur Darstellung von min, Q1, median, Q3 und max</p>

<p><strong>Varianz</strong>: Ma√ü f√ºr die Gr√∂√üe der Abweichung von einem Mittelwert (Definition 2 - 18)</p>

<p><strong>Standardabweichung</strong>: Quadratwurzel der Varianz</p>

<p><strong>Histogramme</strong>: Zeigt H√§ufigkeit mit der die einzelnen Werte auftreten, Kompression in ‚ÄúBuckets‚Äù,
Probleme: ungeeigent f√ºr viele Dimensionen, keine Antwortverfeinerung (Antwortverfeinerung = 
je l√§nger Daten betrachtet werden, desto genauer ist deren Auswertung)</p>
<ul>
  <li><em>Equi-Width-Histogramm:</em> Breite aller Buckets gleich, Breite = Intervall der Attributwerte</li>
  <li><em>Equi-Depth-Histogramm:</em> Tiefe aller Buckets gleich, Tiefe = Summe der H√§ufigkeiten</li>
</ul>

<p><strong>Entropie</strong>: gibt an, wie zuf√§llig die Daten verteilt sind, Ma√ü f√ºr Unordnung (Definition 2-24),
negative Summe aus relativer H√§ufigkeit und statistischer Signifikanz, gibt an wie √ºberraschend
 ein Ergebnis ist, p=1 keine √úberraschung (minimal), maximal wenn pi=pj; Entropie ist 
 abh√§ngig von der Anzahl der Werte d.h. nicht normalisiert
 kleine Zahl = kleine Entropie = kleine √úberraschung
 gro√üe Zahl = gro√üe Entropie = gro√üe √úberraschung
 Joint-Entrpoie f√ºr mehr als eine Variable</p>

<h4 id="wahrscheinlichkeitstheorie">Wahrscheinlichkeitstheorie</h4>
<p><strong>Wahrscheinlichkeitsma√ü</strong>: P erf√ºllt die folgenden Axiome:</p>
<ul>
  <li><em>Nichtnegativit√§t:</em> $P(a)‚â•0$</li>
  <li><em>Triviales Ereignis:</em> $P(Œ©)=1$</li>
  <li><em>Additivit√§t:</em> F√ºr alle $a,b ‚àà F$ und $a ‚à© b = ‚àÖ$ (disjunkt): $P(a ‚à™ b) = P(a) + P(b)$</li>
</ul>

<p><strong>Multivariate Verteilung</strong>: Wahrscheinlichkeitsverteilun bei mehrdimensionalen 
Zufallsvariablen; $P(X = a, Y = b)$ = Wahrscheinlichkeit, P(X,Y) = multivariate 
Wahrscheinlichkeitsverteilung, P(X) und P(Y) Randverteilungen</p>

<p><strong>Unabh√§ngigkeit</strong>: Verteilung einer Zufallsvariable ist nicht anders, wenn Wert einer
anderen Zufallsvariable bekannt ist</p>

<p><strong>Erwartungswert</strong>:  Zahl, die die Zufallsvariable im Mittel annimmt (Definition 2 - 36)</p>

<p><strong>Varianz</strong>: mittlere quadratische Abweichung vom Erwartungswert, Streuungsma√ü (Definition 2 - 36)</p>

<p><em>Ende der 2. Vorlesung vom 16.10.2018</em></p>

<p><strong>Kovarianz</strong>: Nicht normierter Wert um Zusammenh√§nge zwischen Variablen festzustellen $Cov(X, Y) := E[(X ‚Äì E(X))¬∑(Y ‚Äì E(Y))]$, 
Kovarianz ist Verallgemeinerung von Varianz
Veranschaulichung:</p>
<ul>
  <li><em>X,Y abh√§ngig:</em> Punkte auf Gerade, wenn X gro√ü dann Y gro√ü (oder mit negativem Vorzeichen: X gro√ü, Y klein)</li>
  <li><em>X,Y unabh√§ngig:</em> Punkte auf horiziontaler Geraden, Y fix, unabh√§ngig von X, zweiter Faktor stets 0</li>
</ul>

<h4 id="statistische-tests">Statistische Tests</h4>
<p><strong>Chi-Quadrat Test</strong>: hier: Unabh√§ngigkeitstest; erwartete Werte oben, tats√§chlich eingetretene Werte unten; $ \chi ^{2}$
gro√ü wenn starke Korrelation, klein wenn keine Korrelation; Vorgehen: Pr√ºfgr√∂√üen berechnen und Abweichungen der Einzelereignisse aggregieren; 
Zur√ºckweisung der Hypothese, dass Verteilung unabh√§ngig wenn $ p(\chi ^{2}) ‚â§ \alpha $ (Schwellwert $\alpha$ h√§ngt von ‚Äúrisikofreude‚Äù ab); bei kleinem
Chi-Quadrat nich sicher, dass Hypothese zur√ºckgewiesen werden kann ‚Üí nur, dass einige F√§lle existieren, in denen keine Aussage getroffen werden kann;
Wenn z.B. Wahrscheinlichkeit von $ \chi ^{2} ‚â§ \alpha$ dann Zur√ºckweisung der Hypothese ‚Üí $X_1$ und $X_2$ abh√§ngig</p>

<p><strong>Kolmogororv-Smirnov-Text</strong>: √úberpr√ºfung ob zwei Wahrscheinlichkeiten √ºbereinstimmen, d.h. gleiche Verteilung bestimmen oder ob Zufallsvariable
zuvor angenommene Verteilung annimmt (muss keiner Normalverteilung folgen); (vgl. Abb 2-52); H√∂he der Stufen 1/8 (da 8 Werte/Messungen) = 0,125, 
bei Abweichung eintragen, Tabelle sagt welche Abweichung von Normalverteilung noch ok sind (anhand maximaler Distanz zw. kumulativen H√§ufigkeisverteilungen);
Beispiel: Frauen verdienen gleich viel, M√§nner gro√üer Spread, Verteilungen unterschiedlich, Verdienen M√§nner mehr als Frauen? Qualitativ: Stimmen Mittelwerte
√ºberein? Vorne nur Frauen, hinten nur M√§nner ‚Üí Mediane stimmen nicht √ºberein</p>

<p><strong>Wilcoxon-Mann-Whitney Test</strong>: Ist Abweichung der Mediane <a href="https://de.wikipedia.org/wiki/Statistische_Signifikanz">statistisch signifikant</a>; 
<em>Rangsummenstatistik</em> (R√§nge aufsummieren, Kennzahlen berechnen, mit Tabelle vergleichen), Test liefert Wahrscheinlichkeit dass Hypothese zutreffend; bei 
Zur√ºckweisung keine Aussage m√∂glich ‚Üí z.B. bei kleiner Stichprobe nicht m√∂glich zu sagen, dass Gegenteil der Hypothese eintrifft</p>

<p><strong>Bernoulli-Experiment</strong>: N Datenobjekte, Erfolgswahrscheinlichkeit p eines Experiments (z.B. korrekte Klassifizierung), Anzahl erfolgreicher Experimente S,
Beobactete Erfolgsquote $ f= \frac{S}{N}$ ist Zufallsvariable; $Varianz = \frac{p<em>(1-p)}{N} $ ‚Üí je mehr Experimente desto kleiner die Varianz; <br />
*log-likelihood Funktion:</em> logarithmisierte Funktion von Wahrscheinlichkeit f√ºr bestimmte Folge von Ausg√§ngen eines Bernouilli-Experiment: 
$ \prod _{i=1} ^{n} p^y_i¬∑(1-p) ^1-y_i $ einfacher Logarithmen aufzuaddieren, als wiederholt zu multiplizieren: 
$ \sum _{i=1} ^{n} (1-y_i)¬∑log (1-p) + y_i¬∑log p $</p>

<p><em>Vorteile Statistischer Tests:</em> nur ein Werkzeug f√ºr Anwendungsetwickler, Performanz, gute Kombinationsm√∂glichkeit mit anderen DB-Features</p>

<h4 id="datenreduktion">Datenreduktion</h4>
<blockquote>
  <p>Repr√§sentation des Datenbestands, weniger Platz ben√∂tigt, (fast) gleiche Analyseergebnisse</p>
</blockquote>

<p><strong>Numerosity Reduction</strong>: Reduzierung der Datenobjekte</p>
<ul>
  <li><em>parametrische Verfahren:</em> Annahme Datenverteilung folgt best. Modell; sch√§tzen der Modellparameter und lediglich diese speichern, ggf. mit Ausrei√üern</li>
  <li><em>nichtparametrische Verfahren:</em> Wichtige Auspr√§gungen speichern
    <ul>
      <li>Sampling: Arbeit mit repr√§sentativem Ausschnit des Datenbestands, kan Komplexit√§t der Analysealgorithmen reduzieren</li>
      <li>Feature Selection: Auswahl einer Teilmenge der Menge der Attribute; Vorhersagekraft der Attribute ermitteln, Schrittweise Auswahl von Attributen, 
  Schrittweise Eliminierung von Attributen</li>
      <li>Hauptachsentransformation: N k-dimensionale Datenobjekte ‚Üí finde orthogonale Vektoren (‚ÄúHauptachsen‚Äù), die Datenbestand am besten repr√§sentieren</li>
    </ul>
  </li>
</ul>

<p><strong>Dimensionality Reduction</strong>: Reduzierung der Attributanzahl, Weglassen der kleinstedn Diagonalelemente und absteigende Sortierung der Reihenfolge der 
Achsen</p>

<p><strong>Diskretisierung</strong>: Reduktion der m√∂glichen Werte pro Attribut, Verg√∂berung; Wo liegen Intervallgrenzen?</p>
<ul>
  <li><em>entropiebasierte Diskretisierung:</em> (erst n√§chste VL)</li>
</ul>

<p><strong>Pr√ºfungsfrage: Kategorisierung von Daten?</strong>
<strong>Pr√ºfungsfrage: Gegeben Aggregatsfunktion X, ist sie distributiv/algebraisch/holistisch/self-maintainable?</strong>
<strong>Pr√ºfungsfrage: Allgmeiner Zusammenhang zwischen distributiv/algebraisch/holistisch und self-maintainable?</strong>
<strong>Pr√ºfungsfrage: Wie gro√ü ist Entropie, wenn alle Klassen gleich h√§ufig?</strong></p>

<h3 id="r√§umliche-indexstrukturen">R√§umliche Indexstrukturen</h3>
<p><strong>Index</strong>: Seitenweise Anordnung der Daten, m√ºssen im Hauptspeicher vorliegen um damit Rechnen zu k√∂nnen, Seiten = Einheiten des
Zugriffs, <em>Problem der Zugriffsl√ºcke</em>: Zugriff/Laden der Seite viel Zeitaufwendiger als anschlie√üendes Rechnen ‚Üí Zugriffszeit linear mit 
gr√∂√üe der Daten; Index f√ºr mehrere Attribute m√∂glich, Reihenfolge wichtig, erstes Element bestimmt Sortierung, dann zweites;</p>

<p><em>Ende der Vorlesung vom 23.10.2018</em></p>

<p><strong>Normalisierung der Attribute</strong>: bei unterschiedlichen Einheiten pro Dimension ‚Üí Normalisierung: $ a_i = \frac{v_i - min v_j}{max v_j - min v_j}$, min = 0, max = 1</p>

<p><strong>Manhattan Distance</strong>: euklidischer Abstand (direkte Verbindung von Punkten) nicht immer gut ‚Üí Manhattan Distance: Gerade pro Dimension (vgl. Skizze Vl-Mitschrift), sozusagen 
‚ÄúStra√üen gehen‚Äù wie Manhattan; nicht normalisiert ‚Üí je mehr Dimensionen, desto gr√∂√üer der Wert</p>

<p><strong>kd-Baum</strong>: <a href="https://de.wikipedia.org/wiki/K-d-Baum">Wikipedia</a>: ‚Äú unbalancierter Suchbaum zur Speicherung von Punkten aus dem  $ \mathbb {R} ^{k}$. 
Er bietet √§hnlich dem Bereichsbaum die M√∂glichkeit, orthogonale Bereichsanfragen durchzuf√ºhren.‚Äù</p>

<p><strong>Nearest Neighbor</strong>: Kreis um Anfragepunkt; Einsparung, da nur Rechtecke inspiziert werden m√ºssen, die in Kreis liege</p>

<p><strong>Bereichsanfrage f√ºr Objekte mit r√§umlicher Ausdehnung</strong>: Rechtecke (vierdimensionaler Punkt: links, rechts, unten, oben), Anfragepunkt (x,y):</p>
<ul>
  <li>Fall 1: $x &lt; x_1 $ nur links absteigen</li>
  <li>Fall 2: $x ‚â• x_1 $ in beide Teilb√§ume absteigen</li>
</ul>

<p><strong>Pr√ºfungsfrage: Welche Rechtecke √ºberlappen (Anfrage-)Rechteck?</strong> <br />
<strong>Pr√ºfungsfrage: Welche Rechtecke sind in (Anfrage-)Rechteck enthalten?</strong> <br />
<strong>Pr√ºfungsfrage: Welche Rechtecke enthalten (Anfrage-)Rechteck?</strong></p>

<p><strong>kdB-Baum</strong>: <a href="https://en.wikipedia.org/wiki/K-D-B-tree">Wikipedia</a> Kombination von kd-Baum und B*-Baum, √Ñnderungen:ohysischer Knoten enth√§lt mehrere logische Regionen, physische Knoten nicht mehr pro Split-Dimension;
Vorteile: Daten-Wurzel-Abstand immer gleich, genau eine Speicherseit pro physischem Knoten, effizienter Zugriff; Nachteil: komplexe Reorganisation</p>

<p><strong>R-Baum</strong>: <a href="https://de.wikipedia.org/wiki/R-Baum">Wikipedia</a>; Bl√§tter sammeln Datenobjekte in Rechtecken, V√§ter sammeln Rechtecke der Kinder wiederum in Reckecken</p>
<ul>
  <li><em>Einf√ºgen:</em>
    <ul>
      <li>Datenpunkt in Zone eines Kind-Knotens ‚Üí kein Problem</li>
      <li>Datenpunkt f√§llt in √úberlappung von Zonen ‚Üí Knoten, dessen Fl√§che am wenigsten vergr√∂√üert werden muss</li>
      <li>Datenpunkt f√§llt in keine Zone eines Kinde-Knotens</li>
    </ul>
  </li>
</ul>

<p><strong>Instanzbasiertes Lernen</strong>: zur Laufzeit Suchde des n√§chsten Nachbarn im Trainingsdatenbestand ‚Üí Nachbarn bestimmen Klassifikation des gesuchten Datenpunktes; Unterst√ºtzung durch Suchbaum;
bei nominalen Attributen: Distanz ist 0 wenn Attributwerte gleich, ansonsten 1; Noise verschlechtert Ergebnisse</p>

<p><strong>Pr√ºfungsfrage: Warum kann man f√ºr r√§umliche Anfragen nicht ohne weiteres auswerten, wenn man f√ºr jede Dimension separat einen B-Baum angelegt hat?</strong> <br />
<strong>Pr√ºfungsfrage: Wie ist der R-Baum aufgebaut?</strong> <br />
<strong>Pr√ºfungsfrage: Wie funktioniert die Suche nach dem n√§chsten Nachbarn mit dem R-Baum?</strong> <br />
<strong>Pr√ºfungsfrage: Was √§ndert sich, wenn die Objekte eine r√§umliche Ausdehnung haben?</strong> <br />
<strong>Pr√ºfungsfrage: St√∂ren uns √úberlappungen von Knoten des R-Baums? Wenn ja, warum?</strong> <br />
<strong>Pr√ºfungsfrage: Wie unterscheiden sich R-Baum, kD-Baum und kDB-Baum?</strong> <br />
<strong>Pr√ºfungsfrage: Wie funktioniert Einf√ºgen in den R-Baum, inklusive Split?</strong> <br />
<strong>Pr√ºfungsfrage: Was f√ºr Anfragen unterst√ºtzen die diversen r√§umlichen Indexstrukturen?</strong> <br />
<strong>Pr√ºfungsfrage: Warum werden bei der NN-Suche nur genau die Knoten inspiziert, deren Zonen die NN-Sphere √ºberlappen?</strong><br />
<strong>Pr√ºfungsfrage: Welche Classifier kennen Sie?</strong></p>

<h3 id="klassifikation">Klassifikation</h3>
<p>Ziel neue Tupel richtig klassifizieren ‚Üí Annahme: Zuk√ºnftige Daten √§hneln vergangenen</p>

<p><strong>Feature</strong>: Funktion, die Attributwert auf einen (m√∂glicherweise hilfreichen) Wert abbildet; Vorgehen: man √ºberlegt sich alle Features, die n√ºtzlich sein k√∂nnten, Classifier ‚Äúsucht aus‚Äù</p>

<p><strong>Bin√§re Entscheidungsb√§ume</strong>: Anzahl m√∂glicher Entscheidungsb√§ume riesig ‚Üí wie vorgehen? ‚Üí Split-Attribute so w√§hlen, dass Datenpunkte in Knoten dasselbe Risiko tragen ‚Üí Split finden,
der Entropie minimiert (m√∂glichst wenig √úberraschung)</p>

<p><strong>Pruning</strong>: Overfitting ‚Üí wenn Entscheidungsbaum zu sehr auf Trainingsdatenbestand zugeschnitten</p>
<ul>
  <li><em>Prepruning:</em> beim Aufbauen des Baumes √ºberpr√ºfen, ob Split etwas bringt ‚Üí Knoten als Blatt belassen, wenn nicht</li>
  <li><em>Postpruning:</em> Entscheidungsbaum erst aufbauen und dann zur√ºckschneiden ‚Üí besser ‚Üí Baum nur einmal aufbauen</li>
</ul>

<p><strong>Holdout-Daten</strong>: verf√ºgbare Daten, die nicht explizit f√ºrs Training verwendet werden, Standardvorgehensweise, aber kleinerer Trainingsdatenbestand</p>

<p><strong>Pr√ºfungsfrage: Wie baut man einen Entscheidungsbaum auf?</strong> <br />
<strong>Pr√ºfungsfrage: Wie kann man Overfitting beim Aufbau eines Entscheidungsbaums ber√ºcksichtigen?</strong> <br />
<strong>Pr√ºfungsfrage: Wie kann Aufbau des Entscheidungsbaums ber√ºcksichtigen, dass unterschiedliche Fehlerarten unterschiedlich schlimm sind?</strong></p>

<h3 id="evaluation">Evaluation</h3>
<p><strong>Crossvalidierung</strong>: repeated holout Methode, wiederholte Aufteilung in Trainings- und Testdaten ‚Üí Klassifizierung ‚Üí Wiederholung; 10-fold cross validation ist Standard</p>

<p><strong>Stratification</strong>: Sicherstellung, dass bestimmte Eigenschaften in Partitionen gleich verteilt sind</p>

<p><strong>Bias und Varianz</strong>: Gesamtfehler eines Lernverfahrens aus beiden Summanden (vgl. 5-13)</p>
<ul>
  <li>Bias hoch: Fehler beim lernen konsistent (auch unendlicher gro√üer Trainingsdatenbestand w√ºrde Fehler nicht eliminieren)</li>
  <li>Varianz des Lernverhaltens: Fehler verursacht durch Begrenztheit des Trainingsdatenbestand, hohe Varianz ‚Üí hoher zuf√§lliger Fehler</li>
</ul>

<p><strong>Konfusionsmatrix:</strong> Wunschergebnis hohe Werte auf Diagonale, Diagonale sind richtig vorhergesagte Instanzen, wenige FP und FN; horizontal = Vorhersage, vertikal = tats√§chliche Klasse
$Gesamt-Erfolgsquote := \frac{TP+TN}{TP+FN+FP+TN}$</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>Ja</th>
      <th>Nein</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Ja</strong></td>
      <td>TP</td>
      <td>FN</td>
    </tr>
    <tr>
      <td><strong>Nein</strong></td>
      <td>FP</td>
      <td>TN</td>
    </tr>
  </tbody>
</table>

<p><strong>Kappa-Koeffizient</strong>: <a href="https://de.wikipedia.org/wiki/Cohens_Kappa">Cappa Kohens</a> Wie gut ist Vorhersage? Ma√ü zur Einsch√§tzung zwischen zwei Classifier</p>

<p><strong>Loss-Function</strong>: Verlust durch falsche Vorhersagen</p>
<ul>
  <li>
    <p><em>Quadratic Loss Function:</em> k m√∂gliche Vorhersagen, Vorhersageverfahren mit Vektor $(p_1, ‚Ä¶ p_k)$ mit Summe 1, tats√§chliche Klassenzugeh√∂rigkeit mit Vektor $(a_1, ‚Ä¶ a_k)$ ein $ a_i $ hat Wert 1
Rest 0; $ Quadratic loss function = \sum_j(p_j-a_j)¬≤ $<br />
Beispiel 1:  $ a = (1, 0, 0,.., 0), p = (0, 1, 0,‚Ä¶, 0)$ ‚Üí Quadratic loss = $(1-0)¬≤+(0-1)¬≤+(0-0)¬≤+‚Ä¶+(0-0)¬≤ = 1+1 = 2 $<br />
Beispiel 2:  $ a = (1, 0, 0,.., 0), p = (0, 0,5, 0,5, ‚Ä¶, 0)$ ‚Üí Quadratic loss = $(1-0)¬≤+(0-0,5)¬≤+(0-0,5)¬≤+‚Ä¶+(0-0)¬≤ = 1 + 0,25 + 0,25 = 1,5 $<br />
 ‚Üí Quadratic Loss ist nie gr√∂√üer als 2</p>
  </li>
  <li>
    <p><em>Informational Loss Function:</em> $ -log_2 p_i $ mit tats√§chlicher Klassenzugeh√∂rigkeit i</p>
  </li>
</ul>

<p><strong>Lift</strong>: Faktor, um den sich R√ºcklaufquote erh√∂ht, nach oben gew√∂lbte Kurve (im vgl. zu Geraden x), am h√∂chsten auf x, am niedrigsten auf y ist Ziel; andere M√∂glichkeit ist
<em>Cost/Benefit Kurve</em> x-Wert ist Gewinn, y-Wert ist R√ºcklaufquote</p>

<p><strong>Receiver-Operating-Characteristic-Kurve (ROC)</strong>: Sortieren der Datenbest√§nde nach absteigender Wahrscheinlichkeiten, Bewegen auf x-Achse f√ºr TP, bewegen auf y-Achse f√ºr FP,
je besser desto weiter links oben</p>
<ul>
  <li><em>FP-Rate:</em> $ 100 * \frac{FP}{FP+TN}$</li>
  <li><em>TP-Rate (Recall):</em> $ 100 * \frac{TP}{TP+FN}$</li>
  <li><em>Precision:</em> $ 100 * \frac{TP}{TP+FP}$
Kennzahlen f√ºr Kurve:</li>
  <li><em>Area under Cureve (AUC):</em> Fl√§che unter Kurve</li>
  <li><em>f-Measure:</em>  $ \frac{2<em>TP}{2</em>TP+FP+FN}$</li>
  <li><em>Erfolgsquote:</em>  $ \frac{TP + TN}{TP+FP+TN+FN}$</li>
</ul>

<p><strong>Qualit√§tsma√üe f√ºr numerische Vorhersagen</strong>: $p_i $ Vorhersage f√ºr i-tes Objekt, $a_i$ tats√§chlicher Wert, $ƒÅ$ Mittelwert des Trainingsdatenbestandes, $ƒÅ_{test}$ Mittelwert des Testdatenbestandes</p>
<ul>
  <li><em>Mean-squared error:</em> $\frac{(p_1 - a_1)¬≤+‚Ä¶+(p_n - a_n)¬≤}{n}$</li>
  <li><em>Root mean-squared error:</em> $\sqrt{\frac{(p_1 - a_1)¬≤+‚Ä¶+(p_n - a_n)¬≤}{n}}$</li>
  <li><em>Mean-absolute error:</em> $\frac{|p_1 - a_1|+‚Ä¶+|p_n - a_n|}{n}$</li>
  <li><em>Relative-squared error:</em> $\frac{(p_1 - a_1)¬≤+‚Ä¶+(p_n - a_n)¬≤}{(a_1 - ƒÅ)¬≤+‚Ä¶+(a_n - ƒÅ)¬≤}$ Obwohl gleich weit von Mittelwert weg, kann Fehler ‚Äúkrass‚Äù sein</li>
  <li><em>Root relative-squared error:</em> $\sqrt{\frac{(p_1 - a_1)¬≤+‚Ä¶+(p_n - a_n)¬≤}{(a_1 - ƒÅ)¬≤+‚Ä¶+(a_n - ƒÅ)¬≤}}$</li>
  <li><em>Relative-absolute error:</em> $\frac{|p_1 - a_1|+‚Ä¶+|p_n - a_n|}{|a_1 - ƒÅ|+‚Ä¶+|a_n - ƒÅ|}$</li>
  <li><em>Correlation coefficient:</em> $\frac{\sum_i{(p_i-pÃÉ)(a_i-ƒÅ_{test})}}{n-1}$ Qualit√§tsma√ü, gro√ü wenn a und p √§hnlich</li>
</ul>

<p><strong>Minimum Description Length (MDL)</strong>: Erfolg beim Finden von Regelm√§√üigkeiten = K√ºrze des Modells/K√ºrze der Beschreibung, z.B. Suche eines Codes in String-Kette, k√ºrzester Code intuitiv 
Bester  $ \forall_x: L_C_P (x) = \lceil -log P(x) \rceil$, Beispiel: Code ‚Äúabacabacabac‚Äù: P(a) = 1‚ÅÑ2, P(b) = P(c) = 1‚ÅÑ4 ‚Üí $a: \lceil -log \frac{1}{2} \rceil = 1$,
 $b,c: \lceil -log \frac{1}{4} \rceil = 2$; kleine Wahrscheinlichkeiten entsprechen gro√üen Code-L√§ngen</p>
<ul>
  <li><em>nicht-uniformer Code:</em> unterschiedliche Codel√§ngen</li>
  <li><em>uniformer Code:</em> gleiche Codel√§ngen durch gleiche Wahrscheinlichkeiten, Repr√§sentation als Bin√§rbaum m√∂glich</li>
</ul>

<p><strong>Theorem zum Codierungsaufwand</strong>: $ -log P(\frac{x}{M})$</p>

<p><strong>Pr√ºfungsfrage: Was ist die ‚Äû10-fold cross validation‚Äú?</strong> <br />
<strong>Pr√ºfungsfrage: Wie haben wir die Erfolgsquote definiert?</strong> <br />
<strong>Pr√ºfungsfrage: Was ist ein Lift Chart? Wie unterscheidet es sich von der ROC Kurve?</strong> <br />
<strong>Pr√ºfungsfrage: Was f√ºr Fehlerarten gibt es bei Vorhersagen von Klassenzugeh√∂rigkeiten? Was f√ºr Kennzahlen kennen Sie, die diese Fehlerarten s√§mtlich ber√ºcksichtigen?</strong> <br />
<strong>Pr√ºfungsfrage: Was ist Unterschied zwischen Kovarianz und Correlation Coefficient?</strong> <br />
<strong>Pr√ºfungsfrage: Warum kommt bei der informational loss Funktion die Logarithmusfunktion zur Anwendung?</strong></p>

<h3 id="association-rules">Association Rules</h3>
<p><strong>Association Rules</strong>: wichtige Art von Mustern an der man bei Datenanalyse interessiert ist,
Muster mit einfacher Struktur</p>

<p><strong>Wichtige Begriffe</strong>:</p>
<ul>
  <li><em>Item:</em> einzelnes Element</li>
  <li><em>Itemset:</em> Menge von Items</li>
  <li><em>Transaktion:</em> Menge von Items, die im Datenbestand tats√§chlich vorkommen</li>
</ul>

<p><strong>Support</strong>: H√§ufigkeit der Regel in Menge der Transaktionen, hoher Wert ‚Üí Regel
beschreibt Gro√üteil des Datenbestands, $p(A u B)$,
 $\sigma$ Schwellenwert f√ºr minimalen Support</p>

<p><strong>Frquent Itemset</strong>: Itemset mit Support $‚â• \sigma$; FIS identifizieren Mengen von Items, die
positiv miteinander korreliert sind, wenn der Support-Schwellenwert gro√ü ist.</p>
<ul>
  <li><em>maximal:</em> Frequent Itemset ist maximal, wegnn es nicht Teilmenge eines anderen Frequent
Itemsets ist ‚Üí es reicht maximale FIS explizit zu erzeugen, um die FIS zu kennen</li>
</ul>

<p><strong>Closedness von Itemsets</strong>: Itemset ist Closed, wenn es keine echte Obermenge, 
mit genau dem gleichen Support gibt</p>

<p><strong>Confidence</strong>: Anteil der Transaktionen in A, die auch in B enthalten sind, Sch√§tzung
der bedingten Wahrscheinlichkeit
von A B $\frac{Support(A u B)}{Support(A)}$; minimum COnfidence = $\gamma$</p>

<blockquote>
  <p>Bevorzugte Regeln sollten $s \geq \sigma$ und $c \geq \gamma$</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>Minimum Support</th>
      <th>Minimum Confidence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Hoch</strong></td>
      <td>wenige FIS, wenige Regeln kommen oft vor</td>
      <td>wenige Regeln, aber alle ‚Äúlogisch fast wahr‚Äù</td>
    </tr>
    <tr>
      <td><strong>Niedrig</strong></td>
      <td>viele g√ºltige Regeln, kommen selten vor</td>
      <td>viele Regeln, aber sehr viele ‚Äúunsicher‚Äù</td>
    </tr>
    <tr>
      <td><strong>Typische Werte</strong></td>
      <td>$\sigma = 2 / 10%$</td>
      <td>$\gamma = 70 / 90%$</td>
    </tr>
  </tbody>
</table>

<p><strong>Apriori</strong>: ALgorithmus zum Finden von FIS und Association Rules 
<a href="https://de.wikipedia.org/wiki/Apriori-Algorithmus#Beispiel">Beispiel Wikipedia</a></p>

<p><strong>Multi-Level Association Rules:</strong> Menge von Regeln auf unterschiedlichen Hierarchieebenen</p>

<p><strong>Level-Crossing Association Rules:</strong> zur Erzeugung von Kandidatenmengen k√∂nnen Itemsets 
unterschiedlicher Ebenen verkn√ºpft werden</p>

<p><strong>Pr√ºfungsfrage: Was sind Association Rules?</strong> <br />
<strong>Pr√ºfungsfrage: Wie findet man sie?</strong> <br />
<strong>Pr√ºfungsfrage: Wie √ºberpr√ºft man rasch f√ºr viele Transaktionen, welche Kandidaten sie enthalten?</strong> <br />
<strong>Pr√ºfungsfrage: Geben Sie ein Beispiel f√ºr eine Association Rule mit hohem/niedrigem Support und hoher/niedriger Confidence.</strong> <br />
<strong>Pr√ºfungsfrage: Was sind multidimensionale Association Rules?</strong> <br />
<strong>Pr√ºfungsfrage: Was sind Multi-Level Association Rules, und wie findet man sie?</strong></p>

<h4 id="schnelles-bestimmen-von-frequent-itemsets">Schnelles Bestimmen von Frequent Itemsets</h4>
<p><strong>Hash-Filter</strong>: √ºblicherweise viele Kandidaten f√ºr kleine k, verglichen mit der Zahl der 
k-Itesets ‚Üí $C_2$ sehr gro√ü: $|C_2| = \binom{|L_1|}{2}$ 
 Beim Z√§hlen des Supports der Elemente von $C_k$ werden (k+1)-elementige Teilmengen jeder
 Transaktion betrachtet, Support Counting f√ºr alle Itemsets mit gleichem Hash-Wert, 
 √úbersteigen des minsups ‚Üí Itemsets frequent</p>

<p><strong>Sampling</strong>: Ziel: weniger Schritte √ºber die Datenbank; Apriori erfordert hohe I/O-Kosten, 
 da f√ºr jede Itemset-Gr√∂√üe ein Scan √ºber die Datenbank passieren muss; Ziel m√∂glichst 
 viele Berechnungen auf einer Stichprobe durchzuf√ºhren, die in Hauptspeicher passt;
 Berechnung der Negative Border mit Sample, Support kannsowohl etwas gr√∂√üer als auch 
 etwas kleiner gew√§hlt werden, Negative Border mit einem Scan √ºber DB √ºberpr√ºft</p>

<p><strong>Apriori-B</strong>: (weitestgehend) nur Betrachtgung von FIS, die nicht maximal sind, Idee:
 FIS in gr√∂√üeren Schritten durchlaufen, v.a. bei gro√üen Itemsets sinnvoll, Verfahren:</p>
<ol>
  <li>Sortieren der FIS innerhalb einer Transaktion nach Gesamth√§ufigkeit (1,5 Scans)</li>
  <li>√úberf√ºhrung der sortierten Transactions-DB in kompakte, baumartige Darstellung
  (FP-Tree, 0,5 Scans)</li>
  <li>Exrahieren der FIS aus FP-Tree<br />
‚Üí wenn DB zu gro√ü, passt FP-Tree nicht in Hauptspeicher ‚Üí Partitionierung der 
Transaktionsmenge</li>
</ol>

<p><strong>Projected Database</strong>: p-projected Database enth√§lt nur Transaktionen die p enthalten, d.h.
ist sie kleiner als Ausgangsdatenbestand; mittlere Zahl von Transaktionen ‚Üí mittlere Zahl 
von Kombinationsm√∂glichkeiten; ist projected Database immernoch zu gro√ü, muss Partitionierung
verfeinert werden</p>

<p><strong>Pr√ºfungsfrage: In welchen Situationen ist Apriori teuer, und warum?</strong>
<strong>Pr√ºfungsfrage: Was kann man gegen diese Schw√§chen tun?</strong>
<strong>Pr√ºfungsfrage: Was sind FP-Trees, und wie lassen sie sich f√ºr die Suche nach Frequent Itemsets verwenden?</strong>
<strong>Pr√ºfungsfrage: Was kann man tun, wenn FP-Trees f√ºr den Hauptspeicher zu gro√ü sind?</strong></p>

<h3 id="pattern-mining">Pattern Mining</h3>
<blockquote>
  <p>Finden h√§ufiger Teilfolgen (mehr Informationen als in Mengen: Reihenfolge)</p>
</blockquote>

<p><strong>Pruning bei AR mit Contraints</strong>:</p>
<ul>
  <li><em>Support-basiertes Pruning:</em> Kandidat wird eliminiert, wenn er (oder eine seiner Teilmengen) nicht frequent ist</li>
  <li><em>Contraint-basiertes Pruning:</em> Kandidat wird eliminiert, wenn er aus vorgegebenen Contraint abgeleitetes Contraint nicht
erf√ºllt</li>
</ul>

<p><strong>Constraints</strong>: Einschr√§nkungen</p>
<ul>
  <li><em>Data Contraints:</em>  konkrete Werte (Intervalle), Attribute des Raums</li>
  <li><em>Rule Contraints:</em> spezifikation der Struktur oder von Eigenschaften der zu ermittelnden Regeln (z.B. nur FIS der Gr√∂√üe 3)</li>
</ul>

<p><strong>Meta-Rule Guided Mining</strong>: zugrundeliegende relationale DB mit Schema:</p>
<blockquote>
  <p>student(name, sno, status, major, gpa, birth_date,birth_place, address) <br />
   course(cno, title, dept) <br />
   grading(sno, cno, instructor, semester, grade)</p>
</blockquote>

<p><em>Data Mining Query/Constraint:</em> alleRegeln der Form:</p>
<blockquote>
  <p>major (s: student, x) ${\land}$ Q(s, y) ‚áí R(s, z) <br />
   from student <br />
   where birth_place = ‚ÄúCanada‚Äù <br />
   in relevance to major, gpa, status, address</p>
</blockquote>

<p>Variable f√ºr Pr√§dikate = Gro√übuchstabe, Variable f√ºr Attributwerte = Kleinbuchstabe, Constraint erlaubt viele<br />
Strukturen und Ergebnisse; zweite Zeile ‚Äúmeta-rule‚Äù, Q,R Variable f√ºr Pr√§dikate mit Attributen instanziierbar;
enth√§lt Data Constraints und Rule Constraints</p>

<p><strong>1-var/2-var Contraints</strong>: zugrundeliegende Struktur: Menge von Items mit Attributen; i.d.R Aggregation von Werten/Belegungen 
mehrerer/aller Items, z.B. sum(LHS) &lt; 100 (davon die Linke Seite)</p>
<ul>
  <li><em>1-var:</em> Constraint, das nur eine Seitde der Regel (L oder R) einschr√§nkt: z.B. sum(LHS) &lt; 100
    <ul>
      <li>Class Constraints: S ‚äÇ A; S Mengenvariable, A Attribut, S ist Menge von Werten aus Definitionsbereich von A, z.B: $S_1 ‚äÇ Item$</li>
      <li>Aggregate Constraints: min, max, sum, count, avg; {=, ‚â†, &lt;, ‚â§, &gt;, ‚â•}; z.B. min(S) &lt; 10</li>
    </ul>
  </li>
  <li><em>2-var:</em> Contraint bez√ºglich beider Seiten: z.B. sum(LHS) &lt; min(RHS)</li>
</ul>

<p><strong>Mining von Association Rules</strong>:</p>
<ul>
  <li><em>Postprocessing:</em> Finde alle Frequent Item Sets mit Apriori und √ºberpr√ºfe dann, ob sie Constraints entsprechen</li>
  <li><em>Optimierung:</em> umfassende Analyse der Eigenschaften der Constraints mit dem Ziel ‚Äútief in den Algorithmus hineinzudr√ºcken‚Äù</li>
</ul>

<p><strong>Anti-Monotonizit√§t</strong>: Ziel Constraint fr√ºh zu √ºberr√ºfen und Pruning so fr√ºh wie m√∂glich stattfinden zu lassen; single-variable
Constraint ist antimonoton, gdw f√ºr alle Mengen S, S‚Äô gilt: S ‚äá S‚Äô, S erf√ºllt C ‚áí S‚Äô erf√ºllt C, d.h. jede Teilmenge erf√ºllt Constraint, 
auch bei L√∂schen aus Menge ist Constraint noch erf√ºllt: <br />
<em>Beispiel</em>: min(S) ‚â• v ist anti monoton f√ºr $v = 5$ und $S = {10,15,20,27,19}$</p>
<ul>
  <li>max(S) ‚â• v: Abh√§ngig von v, aber nicht anti-monoton, da z.B. mit v = 25 und L√∂schen von 27 nichtmehr erf√ºllt</li>
  <li>size(S) ‚â§ v: v = 5, anfangs erf√ºllt, egal wie viel raus gel√∂scht wird immer kleiner gleich 5 ‚Üí anti-monoton</li>
  <li>size(S) ‚â• v: nein, wenn Menge kleiner wird, ist Gleichung nichtmehr erf√ºllt <br />
Anti-Monotonizit√§t ist interessant, weil Obermengen bei Apriori nichtmehr betrachtet werden m√ºssen, wenn Teilmenge schon 
Constraint nicht erf√ºllt</li>
</ul>

<p><strong>Succinctness</strong>: Ziel Kandidaten, die Constraint nicht erf√ºllen, gar nicht erst zu erzeugen, ‚ÄúSuccinct‚Äù = kurz und b√ºndig, 
kurz und knapp; Eigenschaft von Constraints; Constraint ist succinct, wenn alle Itemsets, die das Constraint erf√ºllen
in kurzer Art und Weise hinschreiben kann <br />
<em>Beispiel:</em> nur drei Produkte mit Type = Nonfood, d.h. Itemsets der drei Produkte erzeugen und ein mal Support-Counting</p>

<p><strong>Support-basiertes Pruning</strong>: Kandidat c der L√§nge k, c wird eliminiert, wenn (k-1)-elementige Teilfolge, die Constraint R erf√ºllt
nicht frequent ist <br />
<em>Beispiel:</em> Constraint (nicht anti-monoton): (ab)*, abab hat Teilfolge aab, aab ist m√∂glicherweise frequent aber nie in $L_3$</p>

<p><strong>Constraint-basiertes Pruning</strong>: Schritt k generiert $L_k$ (Menge der h√§ufigen k-Folgen, die Constraint erf√ºllen); wenn R sehr
selektiv (nicht anti-monoton), dann funktioniert Constraint-basiertes Pruning gut, Support-basiertes nicht (betrachtet alle Teilstrukturen
der Gr√∂√üe k-1 in $L_{k-1}, gibt aber mglws wenige)</p>

<blockquote>
  <p>Schw√§chere Constraints erleichtern Support-basiertes Pruning</p>
</blockquote>

<p>Wenn Constraint nicht anti-monoton: naives postprocessing, succinctness, kombination contraint-basiertes pruning mit urspr√ºnglichem
Constraint, Pruning mit abgeschw√§chtem Constraint: sowohl anti-monoton, als auch nicht anti-monoton</p>

<p><strong>Pr√ºfungsfrage: Was ist Constraint-basiertes Mining? Was sind die Vorteile?</strong><br />
<strong>Pr√ºfungsfrage: Was f√ºr Arten von Constraints kennen sie? Beispiele hierf√ºr.</strong><br />
<strong>Pr√ºfungsfrage: Was ist Anti-Monotonizit√§t, Succinctness? &lt;F√ºr ein bestimmtes Constraint sagen/begr√ºnden, ob anti-monoton/succinct.&gt;</strong><br />
<strong>Pr√ºfungsfrage: Wie l√§sst sich Apriori f√ºr das Mining von Teilfolgen verallgemeinern?</strong><br />
<strong>Pr√ºfungsfrage: Antagonismus von Support-basiertem und Constraint-basiertem Pruning erkl√§ren k√∂nnen.</strong><br />
<strong>Pr√ºfungsfrage: Alternativen f√ºr Constraint-basiertes Pruning (wenn Constraint nicht anti-monoton) erkl√§ren k√∂nnen.</strong></p>

<h3 id="clustering-teil-1">Clustering (Teil 1)</h3>
<blockquote>
  <p>Ziel von Clustering: √§hnliche Datenobjekte zu einem Cluster zusammenfassen</p>
</blockquote>

<p><strong>Clustering</strong>: Datenmenge mit N d-dimensionalen Datenobjekten; Finde nat√ºrliche Partitionierung der Daten in mehrere Cluster (k Cluster)
und noise, Wahl der Cluster:</p>
<ul>
  <li><em>Intra-cluster Similarity maximal:</em> Items im gleichen Cluster sind √§hnlich</li>
  <li><em>Inter-cluster Similarity minimiert:</em> Items in unterschiedlichen Clustern sind verschieden</li>
</ul>

<p><strong>Criterion Function</strong>: Bewertung des Clusterverfahrens durch$E = \sum_{i=1}^k \sum_{\vec{x} \in C_i} d(\vec{x},\vec{m_i})$, 
Ziel des Clustering <em>Criterion Function</em> zu optimieren,k gegeben, minimiere E, wenn Punkt im falschen Cluster, wird E gr√∂√üer; 
Criterion Function nur sinnvoll bei gleichem k, bei unterschiedlichem k nicht fair und kann keinen Vergleich mehr bieten</p>

<p><strong>Silhouette-Koeffizient</strong>: Wahl eines Datenpunktes in Cluster als <em>Repr√§sentant</em> des Clusters, Objekte sollen Repr√§sentaten des Clusters √§hneln, 
durchschnittlicher Abstand der Objekte zum Repr√§sentanten des Clusters; Objekte in unterschiedlichen Clustern sollten m√∂glichst un√§hnlich sein</p>
<ul>
  <li><em>a(o):</em> Durchschnittliche Distanz zwischen Objekt o und Objekten in seinem Cluster $a(o) = \frac{1}{|C(o)|} \sum_{p \in C(o)} dist(o,p)$</li>
  <li><em>b(o):</em> Durchschnittlidche Distanz zwischen o und Objekten im zweitn√§chsten Cluster $b(o) =  min_{C_i ‚â† C(o)}(\frac{1}{|C(o)|}
 \sum_{p \in C(o)} dist(o,p))$</li>
  <li><em>s(o):</em> Silhouette von Objekt o $s(o) = \begin{cases} 0 &amp; a(o) = 0 (e.g. |C_i| = 1)
 \\ \frac{b(o)-a(o)}{max{a(o), b(o)}} &amp; \text{else} \end{cases}$; Wertebereich [-1,1]</li>
</ul>

<p><em>Interpretation Silhouette von Objekt o</em>: Wie gut ist die Zuordnung von o zu seinem Cluster?</p>
<ul>
  <li>s(o) = -1: schlecht, o ist im Mittel n√§her bei Elementen von B</li>
  <li>s(o) = 0: zwischen A und B</li>
  <li>s(o) = 1: gut, o geh√∂rt zu Cluster A</li>
</ul>

<p><em>Silhouette-Koeffizient eines Clusterings</em>: $sil(C) = \frac{1}{C} \sum_{C_i \in C} \frac{a}{|C_i|} \sum_{o \in C_i} s(o)$; Wertebereich [-1,1]</p>

<p><em>Interpretation Silhouette eines Clusterings</em>: Durchschnit der Silhouetten aller Objekte</p>
<ul>
  <li>$0,7 &lt; s_C ‚â§ 1,0$: gute Strukturierung</li>
  <li>$0,5 &lt; s_C ‚â§ 0,7$: mittelm√§√üige Strukturierung</li>
  <li>$0,25 &lt; s_C ‚â§ 0,5$: schwache Strukturierung (kommt fast nie vor, 0,3 ist z.B. sehr schlechtes Clustering)</li>
  <li>$s_C ‚â§ 0,25$: keine Strukturierung</li>
</ul>

<p>Silhouette-Koeffizient ist weitgehend unabh√§ngig von k. Ist k klein, ist der Abstand zum Cluster-Mittelpunkt gro√ü, verglichen mit
gro√üem k. Allerdings sind dann auch die Abst√§nde zu anderen Clustern gr√∂√üer, daher hebt es sich auf. Fall k = n kommt fast nie vor.</p>

<blockquote>
  <p>Effektive und effiziente Clustering Algorithmen f√ºr hochdimensionale Datenbest√§nde mit hohem Noise-Anteil erfordern
Skalierbarkeit hinsichtlich:</p>
  <ul>
    <li>Anzahl der Datenpunkte (N)</li>
    <li>Anzahl der Dimensionen (d)</li>
    <li>Noise Anteil</li>
  </ul>
</blockquote>

<p><strong>Distanzfunktionen f√ºr Mengen von Objekten</strong>: $dist(p,q)$</p>
<ul>
  <li><em>Single Link:</em> $dist_{sl}(X,Y) = min_{x \in X, y \in Y} dist(x,y)$</li>
  <li><em>Complete Link:</em> $dist_{cl}(X,Y) = max_{x \in X, y \in Y} dist(x,y)$</li>
  <li><em>Average Link:</em> $dist_{al}(X,Y) =\frac{1}{|X|*|Y|} \sum_{x \in X, y \in Y} dist(x,y)$</li>
</ul>

<h4 id="partitionierende-verfahren">Partitionierende Verfahren</h4>
<p><strong>k-Means</strong>: iterativer Algorithmus (Hill Climbing), der jeden Medoid in Richtung des Schwerpunkts der ihm zugeordneten 
Menge von Punkten verschiebt</p>

<p><em>Medoid:</em> ist ein Datenpunkt, der als Surrogat f√ºr den Schwerpunkt des Clusters dient. Ein Medoid ist schlecht und sollte
im n√§chsten Schritt ersetzt werden, wenn dem Medoid des Clusters am wenigsten Punkte zugeordnet werden oder weniger als
N/k * minDev (Konstante) Punkte zugeordnet werden. Diese Punkte sind vermutlich Outlier oder Teile eines anderen Clusters.</p>

<p><em>Vorgehen:</em> (img k-means-alg.png)</p>

<p>Ziel der <em>Initialisiserungsphase</em> ist es, m√∂glichst gute Seeds zu finden. Dabei werden die Seeds einer nach dem anderen ausgew√§hlt 
(Greedy-Verfahren). Die Seeds sind Datenpunkte. Neue Seeds sollen so gew√§hlt werden, dass der Abstand zu bisherigen Seeds gro√ü genug ist.<br />
Das <em>Distance Stop Kriterium</em> tritt ein und der Algorithmus terminiert, wenn eine bestimmte Anzahl von Schritten keine Verbesserung 
mehr bringt oder diese Verbesserung nur noch sehr klein ist. Die Wahl des Schwellenwert, ab dem das Distance Stop Kriterium erreicht ist,
h√§ngt stark vom Anwendungsfall ab. Bei einem kleinen Wert ist das Ergebnis n√§her am Optimum, der Rechenaufwand aber deutlich h√∂her.
Ein gro√üer Wert liefert ein weniger genaues Ergebnis, hat aber eine bessere Performance.<br />
Bei besonders gro√üen Datens√§tzen kann K-Means auch mit einer <em>Stichprobe</em> durchgef√ºhrt werden, um unn√∂tig gro√üe Laufzeiten zu
ersparen. Um das Ergebnis zu verbessern, k√∂nnen auch mehrere <em>Runs/Durchl√§ufe</em> des Algorithmus ausgef√ºhrt werden.
In Experimenten zeigen f√ºnf Durchl√§ufe mit unterschiedlichen Stichproben ein gutes Ergebnis.<br />
Zur Verfeinerung kann au√üerdem der Medoid, dem kaum Datenpunkte zugeordnet werden, ersetzt werden.</p>

<p><em>CLARANS</em> ist eine Variation von k-Means. k-Means ist zu stark von der Initialisierung abh√§ngig ist und liefert keine Garantie 
f√ºr die Richtigkeit der Cluster. CLARANS stellt das Problem als Graph dar, bei dem jeder Knoten einer Menge von Datenobjekten
entspricht, die wiederum Medoide sind. Dabei entsteht ein sehr gro√üer Graph, der nicht explizit erzeugt wird. Kanten
zwischen zwei Knoten exisitieren genau dann, wenn ein Objekt in der Menge im Knoten unterschiedlich ist. Auch bei 
CLARANS m√ºssen die Seeds geschickt gew√§hlt werden.</p>

<p>(img clarans.png)</p>

<p>Als Parameter ben√∂tigt CLARANS die Anzahl der betrachteten Nachbarn, die Azahl der Runs und ein Abbruchkriterium. Dadurch
entsteht ein weniger ‚Äúunkontrolliertes‚Äù Betrachten der Nachbarn als bisher, da mehrere Nachbarn in einem Schritt betrachtet werden. 
Der Mittelpunkt wird zwischen den Nachbarn verschoben wenn der Mittelpunkt an der Stelle des Nachbarn besser ist. Dadruch ensteht eine 
bessere Qualit√§t, da die Suche breiter ist. Allerdings ist dadurch auch der Aufwand h√∂her, da die potentiellen neuen Clustermittelpunkte
in jedem Schritt berechnet werden m√ºssen.</p>

<p><strong>Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)</strong>: ist ein Clustering-Verfahren f√ºr gro√üe Datenmengen.
BIRCH baut ein Modell (Baum) auf, der die Datenverteilung beschreibt, ist dabei systematischer als partitionsbasierte
Verfahren und kommt mit weniger Speicherplatz aus. Die I/O-Kosten wachsen linear mit der Gr√∂√üe des Datenbestands, wenn der
CF-Tree in den Hauptspeicher passt. Eine Iteration des Algorithmus liefert bereits ein Clustering, mehrfaches
iterieren liefert ein besseres Clustering. Der Wert k ist die gew√ºnschte Anzahl an Clustern und ist Parameter des Algorithmus.
Zudem hat BIRCH eine gute Laufzeit, da das Daten aus der DB lesen Kosten produziert und die Daten dann aber direkt in den Baum eingef√ºgt
werden k√∂nnen, welche Kosten vernachl√§ssigbar ist.</p>

<p>Gegeben einer Menge von Punkten hei√üt der Mittelpunkt <em>Centroid</em>, der <em>Radius</em> des Clusters gibt an, wie nah die Punkte
beieinander liegen und der <em>Durchmesser</em> wird gro√ü, wenn die Datenobjekte weit auseinander liegen und klein, wenn die 
Punkte nah beieinander liegen. $D_2$ ist die <em>Inter-Cluster-Distanz</em>.</p>

<p>(img inter-cluster-distance.png)</p>

<p><em>Clustering Feature (CF):</em> ist die aggregierte Information zu jeder Menge von Punkten die BIRCH mitf√ºhrt und definiert als 
$CF = (N, \vec{LS}, \vec{SS})$. Dabei ist N die Anzahl der Punkte, die Lineares Summe (LS, Linear sum) die
Summe der Punkte in einem Cluster und die Quadrat Summe (SS, square sum) die quadrierte Summe der Punkte eines Clusters.
Aus dem CF lassen sich Centroid und Radius berechnen (TODO!). Beim Zusammenf√ºhren von zwei Clustern l√§sst sich das Clustering
Feature trivialerweise aus den Ausgangsclustern berechnen.</p>

<p><em>CF Tree:</em> ist ein h√∂henbalancierte Baum, in dem jedem Knoten des Baums ein Cluster entspricht. Wenn ein Knoten zu einem Cluster A
in einem Knoten zu Cluster B enthalten ist, hei√üt dass, dass das Cluster A in Cluster B enthalten sein muss. Ein <em>Blatt</em> 
ist eine Menge von Clustering Features (Elementar-Cluster). Die Gr√∂√üe des Baums ist damit (relativ) unabh√§ngign von der Anzahl
der Datenobjekte. Die inneren Knoten enthalten Eintr√§ge der Form $[CF_i, child_i]$ ($child_i$ enth√§lt einen
Pointer auf den Kindknoten) dabei ist $CF_i$ ein Clustering Feature von $child_i$.</p>

<p>Der CF Tree hat die <em>Parameter</em>:</p>
<ul>
  <li><em>B</em>: ist der Fan Out (Verzweigungsgrad) f√ºr innere Knoten. Die Kapazit√§t f√ºr innere Knoten ist kleiner, da zudem
noch die Pointer auf die Kindknoten gespeichert werden m√ºssen.</li>
  <li><em>B‚Äô</em>: ist die Kapazit√§t eines Blattes.</li>
  <li><em>T</em>: ist der Schwellenwert, der kleiner sein muss als der Radius (bzw. Durchmesser) des Elementarclusters. Ohne diese Bedingung
w√ºrden alle Daten punkte in ein einziges Elementarcluster eingef√ºgt werden. Ist T zu klein gew√§hlt, entstehen zu viele 
Elementarcluster, der Baum wird zu kleinteilig und passt nicht in den Hauptspeicher. W√§hlt man das T hingegen zu gro√ü, 
entstehen zu wenige Elementarcluster und die Repr√§sentation ist zu grob, um damit arbeiten zu k√∂nnen.</li>
</ul>

<p>(img birch-illustration.png)</p>

<p>Beim <em>Einf√ºgen</em> in den CF Tree wandert jeder Punkt in den Knoten, zu dessen Schwerpunkt er
den kleinsten Abstand hat. Passt ein Punkt in kein Cluster auf dem Blatt, wird er
als neues Cluster in ein neues Blatt eingeordnet. Die Knoten werden <em>gesplittet</em>, sobald der
Cluster zu gro√ü wird (zu viele Teil-Cluster existieren). <br />
Beim <em>Node-Splitting</em> werden
die am weitesten voneinander entfernten Punkte als Seeds gew√§hlt. Die N√§he der
anderen Punkte entscheidet √ºber die Zuordnung zum jeweiligen neuen Cluster. Sobald der 
Knoten, der einem Seed entspricht voll wird, wird einfach der andere Knoten aufgef√ºllt.
Das Splitten der Knoten findet ledigtlich √ºber das Clustering Feature statt, alle 
Informationen √ºber die Punkte sind im CF enthalten. BIRCH arbeitet ausschlie√ülich mit CF, 
damit m√∂glichst viele Daten in den Hauptspeicher passen.</p>

<p>(img birch-split1.png birch-split2.png birch-split3.png)</p>

<p>Ein <em>Merge</em> der Knoten (Geschwisterknoten) passiert ggf. nach dem Splitting, um Platz zu sparen
und eine bessere Qualit√§t des Clusterings zu gew√§hrleisten. Zwei Kinder des Knotens, der 
 nach dem Splitting zwei neue Knoten enth√§lt, mit minimalem Abstand zueinander werden zu einem
 neuen Knoten zusammengefasst. Ein Merge kann ein <em>Resplitting</em> verursachen.</p>

<h4 id="hierarchisches-clustering">Hierarchisches Clustering</h4>
<p>Oft reichen partitionierende Clustering Verfahren nicht aus, um den Datenbestand angemessen
zu repr√§sentieren. Deshalb ist ein <em>hierarchisches Clustering</em> sinnvoll, um den Datenbestand
in eine Menge hierarchisch geschachtelter Cluster zu unterteilen.</p>

<p>(img motivation-hierarchisches-c.png)</p>

<p>Ein <em>Dendogramm</em> ist die √ºbliche Darstellung des Ergebnisses eines hierarchisches Clusterings.
Die Knoten des Dendogramms stehen f√ºr m√∂gliche Cluster. Die Konstruktion kann entweder
bottom-up (<em>agglomerativ</em>) oder top-down (<em>divisiv</em>) stattfinden. <br />
Die Wurzel repr√§sentiert den gesamten Datenbestand, die Bl√§tter die einzelnen Datenobjekte.
Innere Knoten sind die Vereinigung der Datenobjekte mit seinem Teilbaum und die H√∂he eines
internen Knotens, ist der Abstand zwischen seinen zwei Kindknoten.
Die Kanten, die sich beim horizontalen Schneiden durch den Baum ergeben, sind die Cluster. 
Bei n Paaren (also Kanten), die nach dem Zusammenfassen noch m√∂glich sind, ergibt sich
 eine Laufzeit von O(n log n).</p>

<p>(img dendogram.png dendogramm-interpret.png)</p>

<p><strong>Agglomeratives hierarchisches Clustering</strong>: ben√∂tigt eine Distanzfunktion, um den Abstand
zwischen den Clustern zu berechnen (z.B. Euklidischer Abstand, Manhattan Distanz).</p>
<ol>
  <li>Jedes Objekt ist einelementiger Cluster</li>
  <li>Berechnung aller paarweisen Abst√§nde zwischen Clustern</li>
  <li>Die Paare mit dem kleinsten Abstand werden gemerged</li>
  <li>besteht nur noch ein Cluster wird terminiert, andernfalls Schritt drei wiederholt</li>
</ol>

<p><strong>Divisives hierarchisches Cluserting</strong>: teilt ein gro√ües Cluster, bestehend aus alles Objekten
schrittweise in kleinere Cluster, bis alle Objekte einem eigenen Cluster entsprechen.
<em>DIANA</em>: ist ein Algorithmus, der divisives hierarchisches Clustering umsetzt. 
Starte mit einem Cluster, der alle Beobachtungen enth√§lt.</p>
<ol>
  <li>Berechne den Durchmesser aller Cluster.</li>
  <li>Der Durchmesser ist die maximale Distanz oder Un√§hnlichkeit aller Objekte 
innerhalb des Clusters.</li>
  <li>Der Cluster mit dem gr√∂√üten Durchmesser wird in zwei Cluster geteilt.</li>
  <li>Dazu wird das Objekt in dem Cluster bestimmt, das die gr√∂√üte durchschnittliche 
Distanz oder Un√§hnlichkeit zu allen anderen Objekten hat. Es bildet den Kern der 
‚ÄúSplittergruppe‚Äù.</li>
  <li>Jedes Objekt, das n√§her an der Splittergruppe liegt als an den restlichen Objekten, 
wird nun der Splittergruppe zugeordnet.</li>
  <li>Die Schritte 2‚Äì5 werden solange wiederholt, bis alle Cluster nur noch ein Objekt enthalten.</li>
</ol>

<p>Der markierte Punkt hat einen eher kleinen Abstand zum Objekt o, im Vergleich zu den
Abst√§ndern zu den anderen Datenpunkten (links). Und wird daher der <em>Splittergruppe</em> 
zugeordnet. Nach dem Verfahren geh√∂ren alle Punkt des linken und mittleren Clusters zur 
Splittergruppe.</p>

<p>(img Splittergruppe.png)</p>

<p><strong>Vergleich agglomerativ vs. divisiv</strong>: Beide Verfahren ben√∂tgen n-1 Schritte. Divisives
Verfahren ist konzeptionell anspruchsvoller, weil das Vorgehen beim Split nicht direkt
offensichtlich ist. Agglomeratives Clustering ber√ºcksichtigt lokale Muster, divisives
hingegen die globale Datenverteilung und liefert dadurch ggf. bessere Resultate.</p>

<h4 id="hochdimensionale-merkmalsr√§ume">Hochdimensionale Merkmalsr√§ume</h4>
<p>In der Regel m√∂chte man Cluster in allen Dimensionen herausfinden. Cluster die z.B im 
zweidimensionalen richtig erscheinen, ergeben im h√∂herdimensionalen Raum keinen Sinn.</p>

<p><strong>Projected Clustering</strong>: liefert gegeben einer Anzahl von Clustern k und der 
durchschnittlichen Anzahl der Dimensionen pro Cluster I, die Partitionierung der Daten in k+1
Mengen (Warum k+1?) und die Teilmengen $D_i$.</p>

<p>(img projected-clustering-alg.png)</p>

<p><em>Ermitteln der Dimensionen</em> zu einem Medoid m. F√ºr jeden Medoid, weden die Punkte in seiner
N√§he betrachtet. Die <em>Locality</em> L ist definiert als die Menge der Punkte, deren Abstand
von m kleiner ist als der Abstand von m zu einem Medoiden eines anderen Clusters.</p>

<p>(img locality.png)</p>

<p>Beim Vergleich der Distanzen ist zu beachten, dass die absoluten Werte irref√ºhrend sind, da 
nicht normiert. Die Standardabweichung (Durchschnittliche Absweichung vom Mittelwert) der 
Punkt zum Medoiden sollte betrachtet werden.</p>

<p><em>Bestimmung der Cluster</em>: findet √ºber die Manhattan Distance statt. Jeder Punkt wird
zu seinem n√§chsten Medoiden zugeordnet.</p>

<p>(img manh-distance-projected.png)</p>

<p>(img dim-diff.png)</p>

<h4 id="umgang-mit-kategorischen-attributen">Umgang mit kategorischen Attributen</h4>
<p>Als <em>kategorische Attribute</em>, werden jene bezeichnet, welche eine endliche Menge von 
Werten in ihrem Wertebereich haben (z.B. {braun, schwarz, wei√ü,‚Ä¶}) und deren Wertebereich
nicht boolsch ist. Ziel ist es, Flags f√ºr eine bestimmte Menge von Attributen zu setzen, und
so Datenobjekte mit gleichen Attributen zu finden. Algorithmen verwenden √ºblicherweise 
die Euklidische Distanz zur Berechnung der Abst√§nde. Dies funktoniert gut f√ºr numerische
Attribute.</p>

<p>Bei Datenbest√§nden mit kategorischen Attributen, kommt es zu <em>Problemen</em>, wenn ein Attribut
mengenwertig ist, also die Anzahl der Items/Attribute sehr gro√ü ist. Im Warenkorbszenario
beispielsweise haben Kunden mit √§hnlichem Kaufverhalten (geh√∂ren ins gleiche Cluster) nur
wenige gleiche Produkte gekauft. Die Wahl des Schwellenwerts gestaltet sich dann sehr
schwierig. <br />
Zudem k√∂nnen die Abstandsma√üe zu schlicht oder zu schwer zu
definieren sein. Der <em>Overlap measure</em> ist ein √Ñhnlichkeitsma√ü, das bei gleichen Objekten
1 und bei unterschiedlichen Objekten 0 liefert. Dabei bleiben die H√§ufigkeiten allerdings
unber√ºcksichtigt. Der <em>Jaccard Koeffizient</em> bestimmt die √Ñhnlichkeit zwischen zwei
Transaktionen T1 und T2 $\frac{|T1 \cap T2|}{|T1 \cup T2|}$ mit der maximalen √Ñhnlichkeit
1 (alle Objekte gleich) und der minimalen √Ñhnlichkeit 0 (kein Objekt gleich). Der Jaccard
Koeffizient ber√ºcksicht nur die zwei Punkte, die Nachbarschaft bleibt unber√ºcksichtigt.</p>

<h4 id="dichte-basierte-methodendarstellung-der-cluster-ergebnisse">Dichte-basierte Methoden/Darstellung der Cluster-Ergebnisse</h4>
<p><em>Link-basierte</em> oder auch <em>Dichte-basierte</em> Methoden machen keine Annahme √ºber die Form des
Clusters und liefern bessere Ergebnisse als andere Verfahren.</p>

<p><strong>Link-basierte Methode</strong>: verbindet alle Punkte, deren Distanz kleiner als d ist. Links
 l√∂sen die klassischen Probleme beim Umgang mit kategorischen Attributen. Zwei Punkte sind
 Nachbarn, wenn die √Ñhnlichkeit zwischen ihnen gr√∂√üer als ein Schwellwert sind. Die Anzahl
 von Links zwischen zwei Punkten entspricht der Anzahl gemeinsamer Nachbarn dieser Punkte.
 Das Vorgehen ist ein wiederholter Merge von Clustern mit maximaler Link-Anzahl. (TODO: 
 Zeigen Sie, dass dieser Ansatz mit agglomerativem Clustering
 zu korrektem Clustering f√ºhrt, wenn (Schwellenwert) ÔÅ±=0.5
 und Jaccard Koeffizient.)</p>

<p>(img link-based.png)</p>

<p><strong>Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</strong>: (Wikipedia)[https://de.wikipedia.org/wiki/DBSCAN]:
 ist ein Data-Mining-Algorithmus zur Clusteranalyse. Der Algorithmus arbeitet dichtebasiert und ist in der Lage, mehrere 
 Cluster zu erkennen. Rauschpunkte werden dabei ignoriert und separat zur√ºckgeliefert. <br />
 Die Grundidee des Algorithmus ist der Begriff der Dichteverbundenheit. Zwei Objekte gelten als dichte-verbunden, wenn es
  eine Kette von dichten Objekten (Kernobjekte, mit mehr als minPts Nachbarn) gibt, die diese Punkte 
  miteinander verbinden. Die durch dieselben Kernobjekte miteinander verbundenen Objekte bilden einen Cluster. Objekte, die 
  nicht Teil eines dichte-verbundenen Clusters sind, werden als Rauschen (engl. Noise) bezeichnet.</p>

<p>In DBSCAN gibt es drei Arten von Punkten:</p>

<ul>
  <li><em>Kernobjekte</em>, welche selbst dicht sind. (rote Punkte)</li>
  <li><em>Dichte-erreichbare Objekte</em>. Dies sind Objekte, die zwar von einem Kernobjekt des Clusters erreicht werden k√∂nnen, 
  selbst aber nicht dicht sind. Anschaulich bilden diese den Rand eines Clusters. (gelbe Punkte)</li>
  <li><em>Rauschpunkte</em>, die weder dicht, noch dichte-erreichbar sind. (blauer Punkt)</li>
</ul>

<p>(img Datei_DBSCAN_Illustration.svg)</p>

<p>Der Algorithmus verf√ºgt √ºber zwei Parameter: Œµ und minPts. Dabei definiert Œµ die Nachbarschaftsl√§nge eines Punktes:
 Von einem Punkt erreichbar ist ein zweiter Punkt genau dann, wenn sein Abstand kleiner als Œµ ist. minPts definiert dagegen,
 wann ein Objekt dicht (d. h. ein Kernobjekt) ist: wenn es mindestens minPts Œµ-erreichbare Nachbarn hat.</p>

<p>Dichte-erreichbare Punkte k√∂nnen von mehr als einem Cluster dichte-erreichbar sein. Diese Punkte werden von dem 
 Algorithmus nicht-deterministisch einem der m√∂glichen Cluster zugeordnet. Dies impliziert auch, dass Dichteverbundenheit 
 nicht transitiv ist; Dichte-Erreichbarkeit ist nicht symmetrisch.</p>

<p>Wenn <em>Œµ kleiner</em> wird, kann es sein, dass Objekte, die vorher Teil des Clusters waren, nichtmehr im gleichen Cluster sind.
 Wenn <em>minPts kleiner</em> wird, werden mehr Objekte keinem Cluster mehr zugeorndet (‚Üí mehr Ausrei√üer).</p>

<p><em>DBSCAN Erweiterung: Ordering Points To Identify the Clustering Structure (OPTICS)</em>: (Wikipedia)[https://de.wikipedia.org/wiki/OPTICS]:
  Das Grundprinzip des Algorithmus entstammt DBSCAN,
  jedoch l√∂st der Algorithmus eine wichtige Schw√§che des DBSCAN-Algorithmus: im Gegensatz zu diesem kann er Cluster 
  unterschiedlicher Dichte erkennen. Gleichzeitig eliminiert er (weitgehend) den Œµ-Parameter (<em>Core-Distance</em>) des DBSCAN-Algorithmus. 
  Hierzu ordnet OPTICS die Punkte des Datensatzes linear so, dass r√§umlich benachbarte Punkte in dieser Ordnung nahe 
  aufeinander folgen. Gleichzeitig wird die sogenannte ‚ÄûErreichbarkeitsdistanz‚Äú notiert. Zeichnet man diese 
  Erreichbarkeitsdistanzen in ein Diagramm, so bilden Cluster ‚ÄûT√§ler‚Äú und k√∂nnen so identifiziert werden.</p>

<p>(img optics-idea.png)</p>

<p>Die <em>Core-Distance</em> $core-distance_{MinPts}(o)$ ist der kleinste Wert, den Œµ annehmen kann, 
  sodass o immer noch ein dichtes Objekt ist.</p>

<p>In der Priorit√§tsliste (die die Datenpunkte enth√§lt) sollten Datenpunkte, die nahe 
  beieinander liegen, aufeinander folgen (p1 ist nahe an dichtem Punkt o, p2 nicht: p1,o,p2).
  Ist p n√§her an o als core-distnace, dann spielt die genaue Distanz f√ºr die Sortierung keine
  Rolle. Ist Œµ (Œµ1) kleiner als die core-distance, dann ist o nicht dicht. Ist Œµ gr√∂√üer ist p (p1)
  von o <em>dichteerreichbar</em>. F√ºr den Fall, dass p von o weiter weg ist als die core-distance, 
  so h√§ngt es von Œµ (Œµ2) ab, ob p (p2) von o dichteereichbar ist. )</p>

<p>(img core-dist.png)</p>

<p>Die <em>Ereichbarkeitsdistanz</em> (reachibility distance) ist definiert als das Maximum des 
  echten Abstandes und der Core-Distance des verweisenden Punktes.</p>

<p>(img reach-dist.png)</p>

<p>OPTICS verwendet eine Priority Queue (Control List (CL)) zum Speichern
   der Datens√§tze. Eine Liste w√§re zu aufw√§ndig. OPTICS ordnet jetzt die Objekte in der 
   Datenbank, indem es bei einem beliebigen unbearbeiteten Punkt anf√§ngt, die Nachbarn 
   in der Œµ-Umgebung ermittelt und sie sich nach 
   ihrer bisher besten Erreichbarkeitsdistanz in einer Vorrangwarteschlange merkt. Es wird
    jetzt immer derjenige Punkt als N√§chstes in die Ordnung aufgenommen, der die kleinste 
    Erreichbarkeitsdistanz hat. Durch das Verarbeiten eines neuen Punktes k√∂nnen sich die 
    Erreichbarkeitsdistanzen der unverarbeiteten Punkte verbessern. Durch die Sortierung 
    dieser Vorrangwarteschlange verarbeitet OPTICS einen detektierten Cluster vollst√§ndig, 
    bevor er beim n√§chsten Cluster weitermacht.</p>

<p>(img optics-vis.png)</p>

<p><em>Beispiel - OPTICS:</em> Zun√§chst wird Objekt 1 angeschaut. Seine Core-Distance ist gro√ü, da
   es u keinem nat√ºrlichen Cluster geh√∂rt. Die Reachability distance ist undefined, da es das
   erste Objekt, und somit noch kein Referenzobjekt in der Ergebnisliste steht. Die 
   erreichbaren Nachbarn von Objekt 1 sind die Objekte 2 und 3. Diese werden in die CL 
   geschrieben, 1 in die Ergebnisliste. Da Objekt 2 n√§her an Objekt 1, als Objekt 3 liegt, 
   werden die Objekte in dieser Reihenfolge in die CL geschrieben (CL: 2,3; Output: 1).
   Als n√§chstes wird Objekt 2 aus der CL genommen und abgearbeitet. Objekt 16 befindet
   sich in der Œµ-Umgebung von Objekt 2, also wird es in die CL aufgenommen (CL: 3,16; 
   Output: 2,3). Objekt 3 steht als n√§chstes in der Cl, wird also als n√§chstes abgearbeitet.
   Es hat eine kleinere core-distance, da viele Objekte in seiner Umgebung liegen. Die 
   Objekte die mit Objekt 3 im Cluster liegen ({4,‚Ä¶,15}) werden nun als n√§chstes in die CL
   geschrieben. <br />
   Ist die CL komplett abgearbeitet, wird das n√§chste Objekt zuf√§llig aus dem Datensatz 
   gew√§hlt.</p>

<p>(img optics-bsp.png optics-bsp2.png  optics-datastruc.png)</p>

<p>Der bei OPTICS verwendete <em>Reachability Plot</em> repr√§sentiert die Cluster-Struktur 
   dichtebasiert und ist leichter zu analysieren. Zudem ist er unabh√§ngig von der
   Dimensionalt√§t des Cluserbestands.</p>

<p>OPTICS liefert ein gutes Ergebnis, solange die Parameterwerte gro√ü genug sind. F√ºr ein 
   kleines Œµ bleiben gr√∂√üere Nachbarschaften au√üen vor, f√ºr gr√∂√üere Œµ ist das Ergebnis des
   Clusters aber ungenauer. F√ºr kleine minPts entstehen zu kleinteilige Cluster, da Objekte
   mit eher wenigen Objekte in der Nachbarschaft als dicht gelten und ein Cluster bilden.</p>

<h4 id="cluster-ensembles">Cluster-Ensembles</h4>
<p>Zur Generierung <em>robuster</em> Clustering-Ergebnisse, werden bei <em>Cluster-Ensembles</em> die
Resultate mehrere Clustering-Modelle kombiniert. Dazu generiert man k unterschiedliche
Clusterings und leitet daraus die Resultate ab.</p>

<p>Man unterscheidet zwischen:</p>
<ul>
  <li><em>Modell-basiert:</em> unterschiedliche Verfahren bilden ein Ensemble bzw. das selbe Verfahren
mit unterschiedlichen Parametern/Initialisierungen ausgef√ºhrt.</li>
  <li><em>Daten-basiert:</em> unterschiedliche Teilmengen der Daten generieren unterschiedliche Cluster.
Dabei beschr√§nkt man sich auf unterschiedliche Dimensionen.</li>
</ul>

<p>Zum <em>Vereinen</em> der Verfahren kann man unterschiedliche Algorithmen anwenden:</p>
<ul>
  <li><em>Hypergraph Partitioning Algorithmus:</em> jedes Objekt ist ein Knoten in einem Graphen. Cluster
sind sogennante <em>Hyper-Kanten</em>. Jede Hyper-Kante kann mehr als zwei Knoten miteinander 
verbinden.</li>
  <li><em>Meta-Clustering Algorithmus:</em> Auch hier ist die Darstellung des Clusterings ein Graph. 
Bei √úberlappung der Objekt-Mengen im Cluster existieren Kanten zwischen den 
Repr√§sentantenknoten. Der Jaccard-Koeffizient fungiert als Kantengewicht.</li>
</ul>

<h4 id="probabilitsitsches-clustering">Probabilitsitsches Clustering</h4>
<p>Da ein Wert im Allgemeinen zu mehr als einem Cluster geh√∂ren kann, ist die 
wahrscheinlichkeitsbasierte Sichtweise auf Clustering notwendig. Man m√∂chte die Cluster 
finden, die f√ºr gegebene Daten am wahrscheinlichsten sind.</p>

<p>Bei der Betrachtung von gekauften Kameras (A: Privat, B: Professionell) mit dem Preis
an der x-Achse, lassen sich Werte besonders an der √úberlappung der Cluster nicht eindeutig
zuordnen. Unter der Annahme, dass eine Gauss-Verteilung zugrunde liegt, m√∂chte man nun
herausfinden, mit welcher Wahrscheinlichkeit eine Kamera von einer Privatperson oder einem
Profi gekauft wurde.</p>

<p>(img kamera.png)</p>

<p>Das <em>Finite-Mixture-Problem</em> ist das Problem Wahrscheinlichkeitsverteilung durch eine Menge
von k Wahrscheinlichkeitsverteilungen zu beschreiben. Dabei kann jede der k Verteilungen 
ein Cluster beschreiben, die Verteilungen innerhalb eines Clustes sind aber nicht bekannt.
Cluster selbst k√∂nnen unterschiedlich wahrscheinlich sein.</p>

<p><em>Clustering mit Finite-Mixtures</em> finden die Parameter (pA, ŒºA, ŒºB, œÉ¬≤A, œÉ¬≤B) gegeben der
Datenobjekte, der Anzahl der CLuster k und der Art der Verteilung der Cluster. Unter der
Annahme einer Gauss-Verteilung ist dieses Problem l√∂sbar, ohne Annahmen im Allgemeinen aber 
nicht.</p>

<p>Die Idee des <strong>Expectation Maximization Algorthmus</strong> ist es, mit einem zuf√§llig gew√§hlten 
Modell zu starten, und abwechselnd die Zuordnung der Daten zu den einzelnen Teilen des 
Modells (<em>Expectation-Schritt</em>) und die Parameter des Modells an die neueste Zuordnung 
(<em>Maximization-Schritt</em>) iterativ zu verbessern.</p>

<p>Die initiale Belegung der Parameter (pA, ŒºA, ŒºB, œÉ¬≤A, œÉ¬≤B) wird geraten, die Cluster-
Wahrscheinlichkeiten f√ºr jedes Datenobjekt ausgerechnet (<em>Expectation-Schritt</em>), die
Parameter des Clusters neu berechnet (<em>Maximization-Schritt</em>) und dann das Vorgehen wiederholt.
Die Sch√§tzung der Parameter ist abh√§ngig davon, wie wahscheinlich ein Datenobjekt zu einem
Cluster geh√∂rt. Die Wahrscheinlichkeiten wirken dabei wie Gewichte.
EM terminiert dann, wenn das G√ºterma√ü des Clustering (<em>overall likelihood</em>)
erreicht wird. Der optimale Punkt wird allerdings nie erreicht. Es kann nur das lokale 
Optimum gefunden werden, nicht das globale. Man kann den Algorithmus mit unterschiedlichen
Initialisierungen wiederholen und das G√ºtema√ü als Auswahlkriterium f√ºr den besten Algorithmus
w√§hlen.   Angenommen, die Verteilungen
beschreiben Datenbestand gut:Dann sind viele Instanzennahe bei einem Cluster-Mittelpunkt
(z. B. ŒºB ), und Pr[x i | B] ist gro√ü. Angenommen, die Cluster-Mittelpunkte sind abseitig:
Dann ist sowohl Pr[x i | A] als auch Pr[x i | B] f√ºr viele Instanzen klein.</p>

<p>(img overall-likelihood.png)</p>
:ET