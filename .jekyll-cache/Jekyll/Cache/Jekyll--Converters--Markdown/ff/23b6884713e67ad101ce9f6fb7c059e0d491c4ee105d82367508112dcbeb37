I"Ÿ:<div style="background-color: #EAEFF4; border: 1px solid #b5aeb1; border-radius: 3px;  padding: 10px; margin-right: 10px">
    <strong>Vorlesung:</strong> <a href="https://campus.studium.kit.edu/ev/_qIONru0So6lB_y2FMgGxg">Maschinelles Lernen 1 - Grundverfahren</a>, Vorlesung, 3 ECTS <br />
    <strong>Dozent:</strong>Prof. Dr.-Ing. R√ºdiger Dillmann<br />
   <strong>ILIAS:</strong> <a href="https://ilias.studium.kit.edu/goto.php?target=crs_884298&amp;client_id=produktiv">https://ilias.studium.kit.edu/goto.php?target=crs_884298&amp;client_id=produktiv</a><br />   
   <strong>Videos:</strong> <a href="">TODO</a>, technische Probleme: ersten beiden VL fehlen<br />   
   <strong>DIVA Videos:</strong> <a href="https://mediaservice.bibliothek.kit.edu/#/details/DIVA-2017-548">WS 17/18</a> nur im KIT-Netz<br />  
   <strong>Klausur:</strong> schriftlich, 11./12./13.2.2018, wird noch bekanntgegeben <br />
   <strong>Einordnung:</strong> Vertiefungsfach "Anthropomatik und Kognitive Systeme", Profil "Datenintensives Rechnen", Robotik, KI... <br />
</div>

<h2 id="organisatorisches">Organisatorisches</h2>

<h3 id="vorlesungen">Vorlesungen</h3>
<ul>
  <li><strong>17.10.2018</strong>: Organisatorisches und Foliensatz 1-1 bis 1-59</li>
  <li><strong>24.10.2018</strong>: Wiederholung von Kapitel 1, Foliensatz 2</li>
  <li><strong>30.10.2018</strong>: Wiederholung von Kapitel 2, Foliensatz 3</li>
  <li><strong>06.11.2018</strong>: Foliensatz 4 <a href="#vl4">Neuronale Netze</a></li>
  <li><strong>13.11.2018</strong>: Foliensatz 5</li>
</ul>

<h3 id="material">Material</h3>
<p>VL wird aufgezeichnet. Das Material wird auf ILIAS zur Verf√ºgung gestellt. Die Suche der Verantstaltung
funktioniert nicht. Daher muss man manuell suchen. Daf√ºr geht man bei ‚ÄúOrganisationseinheiten‚Äù zu den
Wirtschaftswissenschaften und sucht dort die Vorlesung. Das Passwort wurde in der Vorlesung mitgeteilt.</p>

<h3 id="√ºbung">√úbung</h3>
<p>√úbung findet montags alle zwei Wochen von 14:00 bis 15:30 statt (Sport R007). Beginn des √úbungsbetriebes
 am 29.10. Ist f√ºr Wirtschaftswissenschafler Pflicht,
f√ºr Informatiker freiwillig. Vor erster √úbung sollte Python am eigenen Laptop installiert werden.
Dazu wird auf <a href="https://git.scc.kit.edu/snippets/236">diese Anleitung</a> verwiesen.</p>

<p>Aufbau der √úbungseinheiten:</p>
<ul>
  <li><strong>29.10.2018</strong>: √úbung 01 Einf√ºhrung: Python, Jupyter Notebook, Numpy, Pandas, Matplotlib, etc.</li>
  <li><strong>19.11.2018</strong>: √úbung 02 Neuronale Netze</li>
  <li><strong>26.11.2018</strong>: √úbung 03 Klassifikation und Regression</li>
  <li><strong>10.12.2018</strong>: √úbung 04 Reinforcement Learning</li>
  <li><strong>14.01.2019</strong>: √úbung 05 HMM und Bayes</li>
  <li><strong>28.01.2019</strong>: √úbung 06 Un√ºberwachte Lernverfahren</li>
</ul>

<h3 id="klausur">Klausur</h3>
<p>Die Klausur findet schriftlich statt. Termin noch unklar, 11./12./13.2.2018, wird noch bekanntgegeben.</p>

<h2 id="vorlesungsinhalt">Vorlesungsinhalt</h2>
<h3 id="einf√ºhrung-und-√ºberblick">Einf√ºhrung und √úberblick</h3>
<p><strong>Intelligenz</strong>: viele widerspr√ºchliche Definitionen, bereits 1021 von Psychologen, Kontext (im technischen
Sinne) oft eingeschr√§nkt; m√∂gliche Komponenten: Denken und Probleml√∂sen, Lernen und Erinnern, Sprache,
Wissen abspeichern und √ºbertragen, Kreativit√§t und Bewusstsein, √úberleben in komplexen Welten, rezeptive und
motorische F√§higkeiten (Robotik)</p>

<p><strong>Lernen</strong>: Lernen von Entscheidungen, Aktionsfolgen, Beschreibungen, Modellen; Entwicklung motorischer und
kognitiver F√§higkeiten durch Anweisung oder Training; Neuorganisation/Umorganisation/Transformation von Wissen</p>

<p><strong>Lernendes System</strong>: in der Lage unbekannte Eigenschaften eines Prozesses/seiner Umgebung durch schritweises,
wiederholtes Handeln und Beobachten zu erfassen; mit gewonnener Erfahrung Vorhersagen treffen, klassifizieren,
Entscheidungen treffen mit Ziel optimales Systemverhalten zu erzielen oder Leistung zu steigern.<br />
Komponenten eines Lernenden Systems (vgl. Abb 1-30):</p>
<ul>
  <li><em>Informationsquelle</em></li>
  <li><em>Lernendes Element</em>: Was und wie wird gelernt? Wie wird das genutzt? Inferenz?</li>
  <li><em>Wissensbasis</em>: Wie kann Wissen/Hypothesen repr√§sentiert werden?</li>
  <li><em>Ausf√ºhrendes Element</em>: Was und wie wird gelernt? Wie wird das genutzt? Inferenz?</li>
</ul>

<p><strong>Maschinelles Lernen</strong>: System lernt aus Erfahrung E im Hinblick auf Klasse von Aufgaben T und Performanzma√ü P, wenn
Leistungen bei Aufgabe aus T mit P durch Erfarhgung E steigt; generiert ein/mehrere L√∂sungshypothesen H;</p>

<p><strong>Inferenz</strong>: bezeichnet den Vorgang eines Algorithmus aus Fakten und Vermutung (korrekte) Schl√ºsse zu ziehen.</p>

<p><em>Ziel:</em> Hypothesen finden, Interferenz erm√∂glichen; Hypothesen beschreiben ein Modell, z.B. Diskriminatives Modell (Klassifikation),
Generatives Modell (Beschreibung/Erzeugung von Daten), Probabilistisches Modell (Schlie√üen auf Zufallsvariablen), Markovsches Entscheidungsmodell
(Bestimmen von Aktionsketten)</p>

<p><strong>Deduktiver Schluss</strong>: Aus A folgt B, es gibt Folge von Regeln um B abzuleiten ‚áî  $ A ‚Üí B $ <br />
Beispiel: ‚ÄúAlle Menschen sind sterblich.‚Äù, ‚ÄúSokrates ist ein Mensch.‚Äù ‚Üí ‚ÄúSokrates ist sterblich.‚Äù</p>

<p><strong>Abduktion</strong>: H folgt aus Hintergrundwissen B und Beobachtungen D abduktiv ‚áî $ B ‚à™ H ‚Ü¶ D $
Beispiel: ‚ÄúAlle Menschen sind sterblich.‚Äù, ‚ÄúSokrates ist sterblich.‚Äù ‚Üí ‚ÄúSokrates ist ein Mensch.‚Äù</p>

<p><strong>Induktiver Schluss</strong>: Hypothese H folgt induktiv aus Menge D von Grundbeispielen und Hintergrundwissen D ‚áî
$ B ‚à™ H ‚Ü¶ D, B ‚Üõ D,  B ‚à™ D ‚Üõ ¬¨H $, <strong>Vorsicht</strong>: Aus Hypothesen sind einzelne Beispiele ableitbar, aber nicht umgekehrt!
Mit ausreichenden Beispielen: ‚ÄúSokrates ist ein Mensch.‚Äù, ‚ÄúSokrates ist sterblich.‚Äù ‚Üí ‚ÄúAlle Menschen sind sterblich.‚Äù</p>

<p><strong>Wissensrepr√§sentationen</strong>: verschiedene M√∂glichkeiten Wissen zu repr√§sentieren:</p>
<ul>
  <li><em>Assoziierte Paare</em></li>
  <li><em>Parameter in algebraischen Ausdr√ºcken</em></li>
  <li><em>Entscheidungsb√§ume</em></li>
  <li><em>Formale Grammatiken</em></li>
  <li><em>Produktionsregeln</em></li>
  <li><em>Formale logikbasierte Ausdr√ºcke</em></li>
  <li><em>Graphen und Netzwerke</em></li>
  <li><em>Probabilistische Graphische Modelle</em></li>
  <li><em>Frames, Schemata, Semantische Netze</em></li>
  <li><em>Prozedurale Kodierung</em></li>
  <li><em>Taxonomien</em></li>
  <li><em>Markov-Ketten</em></li>
</ul>

<p><strong>Historie</strong>:</p>
<ul>
  <li><em>Beginn und Enthusiasmus (1955-1968):</em> Lernen ohne Wissenstrukturen, Auswendiglernen und Lernen als Suche,‚Ä¶</li>
  <li><em>Depression (1969-1976):</em> Symbolisches begrifforientiertes Lernen,‚Ä¶</li>
  <li><em>Renaissance (1976-1986):</em> erste erfolgreiche Anwendung, Neuronale Netze, Statistisches Lernen,‚Ä¶</li>
  <li><em>Maturit√§t - Reife (1986-‚Ä¶):</em> Reinforcement Learning, Deep Learning, Kombination induktiver und deduktiver Verfahren,‚Ä¶</li>
  <li><em>Heute:</em> Betrachtung komplexerer realer Anwendungen, Verbesserung/Erweiterung der Grundverfahren</li>
</ul>

<p><strong>Lernen als Prozess</strong>: (vgl. Abb 1-48)</p>
<ol>
  <li>Identifikation der Lernaufgabe</li>
  <li>Initiale Wissenakquisation</li>
  <li>Vorverarbeitung/Segmentierung/Generierung von Trainingsdaten</li>
  <li>Konfigurierung des Lernverfahrens</li>
  <li>Initialer (offline) Lernprozess</li>
  <li>Bewertung des erlernten Wissens</li>
  <li>Wissensanapassung/-verfeinerung/-erweiterung</li>
</ol>

<p><strong>Einordnungskriterien von Lernverfahren</strong>:</p>
<ul>
  <li><em>Typ der Inferenz:</em> induktiv ‚áî deduktiv</li>
  <li><em>Ebenen des Lernens:</em> symbolisch ‚áî subsymbolisch (z.B. Matrixmultiplikation)</li>
  <li><em>Lernvorgang:</em> √ºberwacht ‚áî un√ºberwacht</li>
  <li><em>Beispielumgebung:</em> inkrementell (mit neuen Daten St√ºck f√ºr St√ºck weiterlernen) ‚áî nicht inkrementell</li>
  <li><em>Umfang der Beispiele:</em> umfangreich ‚áî gering</li>
  <li><em>Hintergrundwissen:</em> empirisch ‚áî axiomatisch</li>
</ul>

<h3 id="induktives-lernen">Induktives Lernen</h3>
<p><strong>Induktion</strong>: Prozess des <em>plausiblen</em> Schlie√üens vom Speziellen zum Allgemeinen; wahrheitserweiternd, Generierung neuter Hypothesen;</p>

<p><strong>Deduktion</strong>: Prozess des <em>korrekten</em> Schlie√üens vom Allgmeinen zum Speziellen; wahrheitserhaltend, Ableiten neuer Reglen/Fakten</p>

<p><strong>Induktive Lernverfahren</strong>: beispiel Konzeptlernen  (min Anpassungen)
Gegeben:</p>
<ul>
  <li><em>Instanzraum/Merkmalsraum</em> $X$:</li>
  <li><em>Trainingsmenge</em> $D$:</li>
  <li><em>Zielkonzept/Sollausgabe</em> $c(‚Ä¶)$: Untermenge von Objekten/Ereignissen, Boolsche Funktion</li>
  <li><em>Hypothesenraum</em> $H$:</li>
</ul>

<p>Gesucht:</p>
<ul>
  <li><em>Hypothese</em> $h$: jede Hypothese, die Zielfunktion √ºber gen√ºgend gro√üe Menge von Trainingsbeispielen gut approximiert, wirds Zielfunktion auch
bei unbekannten Beispielen gut approximieren.</li>
</ul>

<p><em>Konzeptlernen:</em> induktives Lernverfahren, schlie√üen auf Boolean-wertige Funktion asu Trainingsbeipielen ihres Inputs und Outputs</p>

<p><strong>Definitionen im Hypothesenraum</strong>:</p>
<ul>
  <li><em>Konsistenz:</em> keine negativen Beispiele werden positiv klassifiziert</li>
  <li><em>Vollst√§ndigkeit:</em> alle positiven Beispiele werden positiv klassifiziert<br />
‚Üí Vozugskriterium: konstistent + vollst√§ndig</li>
</ul>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/C1cI7wUUg6M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p><strong>Versionsrau (Version Space)</strong>: Versionsraum ist Untermenge von Hypothesen in H, die mit Trainingsbeispielen <em>konsistent</em> und <em>vollst√§ndig</em> sind<br />
parallele Anwendung von <em>Suche vom Allgemeinen zum Speziellen</em> (Ausgangspunkt allgemeinste Hypothese, neg. Beispiele: Spezialisierung,
 pos. Beispiele: nicht betrachtet) und <em>Suche vom Speziellen zum Allgemeinen</em>  (Ausgangspunkt spziellste Hypothese, pos. Beispiele: Verallgemeinerung,
 neg. Beispiele: nicht betrachtet)<br />
 Spezifischste Hypothese = Allgemeinste Hypothese, wenn alle Beispiele konsistent, korrekte Hypothese im Hypothesenraum enhalten, <em>Probleme:</em> bei
 fehlerhaften Trainingsdaten (Rauschen), Zielkonzept nicht von Hypothesenrepr√§sentation abgedeckt, disjunktive Begriffe, konsistente Beispiele n√∂tig</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/TlkMkC64XpE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p><strong>Induktiver Bias</strong>: Induktives Lernen erfordert Vorannahmen = <strong>Inductive Bias</strong>, je strenger Vorannahmen, desto mehr unbekannte Beispiele k√∂nnen
klassifiziert werden, z.B. Version Space: Zielkonzept in H repr√§sentierbar, Specific-to-General: alle Instanzen negativ, soland nicht Gegenteil
bekannt.
Beispiel: Ockham‚Äôs razor</p>

<p><strong>Bias (Vorzugskriterium)</strong>: Vorschrift nach der Hypothesen gebildet werden</p>
<ul>
  <li><em>Hypothesenraumbias</em>: h in beschr√§nktem Hypothesenraum</li>
  <li><em>Pr√§ferenzbias</em>: h mit h√∂chste Pr√§ferenz in geordnetem Hypothesenraum</li>
</ul>

<p>‚Üí Problem: es existiert keine Funktion h, die konsistent mir Trainingsbeispiele ist, z.B. bei verrauschten Trainingsdaten<br />
‚Üí L√∂sung: Anpassen des Hypothesenraumbias (gute Klassifikation aber Overfitting), Anpassen des Pr√§ferenzbias (w√§hle h, dass m√∂glichst viel
richtig klassifiziert, m√∂gliche Missklassifikation)</p>

<h3 id="lerntheorie">Lerntheorie</h3>
<p><strong>Lernmaschine</strong>: lernende Maschine mit <em>Hypothesenraum</em> ${h_\alpha: \alpha ‚àà A}$ und  Lernverfahren (Methode $\alpha_{opt}$ mit Hilfe von
Lernbeispielen zu finden) ‚Üí Entscheidungsmodell $M_{opt}$</p>

<p><strong>Probleme beim Lernen</strong>:</p>
<ul>
  <li><em>Statistisches Problem:</em> ‚Äúzu gro√üer‚Äù Hypothesenraum, f√ºr Trainingsdaten mehrere gleich gute Hypothesen (bis vor f√ºnf Jahren w√§re noch die
pauschale Antwort gewesen, dass man gem√§√ü Occam‚Äôs Razor die einfachere der beiden Hypthosen w√§hlt)</li>
  <li><em>Komplexit√§tsproblem:</em> nicht garantierte optimale L√∂sung im Hypothesenraum, Gefahr einer suboptimalen L√∂sung</li>
  <li><em>Repr√§sentationsproblem:</em> Zielfunktion nicht gen√ºgend gut approximiert</li>
</ul>

<p><strong>Fehler</strong>: reale Fehler nicht berechenbar ($ \int $) ‚Üí empirischer Fehler sch√§tzbar ($ \sum $);
m√∂gliche Fehler: Lerndaten ‚Üí Lernfehler, Verifikationsdaten ‚Üí Verifikationsfehler, Testdaten ‚Üí Generalisierungsfehler</p>

<p><strong>Fehlerminimierung</strong>: definiere $h_a$, finde beste $\alpha_{opt}$ durch iterative Minimierung des empirischen Lernfehlers $E_D(h_{\alpha}), z.B. durch Gradientenabstieg (Gradient
muss berechenbar oder absch√§tzbar sein)</p>

<p><strong>Overfitting</strong>: zu starke Spezialisierung der Maschine auf Lernbeispiele (‚Äúauswendig Lernen‚Äù); Lernfehler f√§llt, Testfehler steigt ‚Üí Generalisierung f√§llt; Erkl√§rung: Lerndatenmenge und
Testdatenmenge unterschiedlich; L√∂sung: Lernprozess durch Verifikationsfehler steuern, Lernprozess im ‚Äúrichtigen Moment‚Äù stoppen, richtige Wahl und Suche der optimale Hypothesen</p>

<p><strong>Modellg√ºte bestimmen</strong>: je nach Aufgabenstellung unterschiedliche Methoden:</p>
<ul>
  <li><em>Klassifikation:</em> Anzahl richtiger/falscher Klassifizierungen</li>
  <li><em>Regression:</em> Entfernung zu Sch√§tzungsziel</li>
  <li><em>Un√ºberwachtes Lernen:</em> wie gut werden Daten abgebildet ‚Üí schwierig</li>
  <li><em>Reinforcement learning:</em> wie gut passt Aktionsfolge  ‚Üí keine allgemeinen Metriken</li>
</ul>

<p><strong>Klassifikation</strong>: Unterscheidung von vier Ergebnisklassen:</p>
<ul>
  <li><em>True Positive (TP):</em> korrekte Klassifikation positiver Instanzen</li>
  <li><em>True Negative (TN):</em> korrekte Klassifikation negativer Instanzen</li>
  <li><em>False Positive (FP):</em> falsche Klassifikation positiver Instanzen (typische Verwendung)</li>
  <li><em>False Negative (FN):</em> falsche Klassifikation negativer Instanzen</li>
</ul>

<p><em>Konfusionsmatrix:</em> Wunschergebnis hohe Werte auf Diagonale, wenige FP und FN; horizontal = Vorhersage, vertikal = tats√§chliche Klasse</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>Ja</th>
      <th>Nein</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Ja</strong></td>
      <td>TP</td>
      <td>FN</td>
    </tr>
    <tr>
      <td><strong>Nein</strong></td>
      <td>FP</td>
      <td>TN</td>
    </tr>
  </tbody>
</table>

<p><em>Metriken:</em></p>
<ul>
  <li>Klassifikationsfehler: m√∂glichst klein $\frac{errors}{total} = \frac{FP+FN}{TP+FN+FP+TN}$</li>
  <li>G√ºte: = 1-Fehler $\frac{correct}{total} = \frac{TP+TN}{TP+FN+FP+TN}$</li>
  <li>False Positve Rate (FPR) / False Alarm Rate (FA): m√∂glichst klein $FPR = \frac{FP}{FP+TN}$</li>
  <li>False Negative Rate (FNR) / Miss Rate (MR): m√∂glichst klein $FNR = \frac{FN}{TP+FN} = 1 - TPR$</li>
  <li>Genauigkeit (Precision): m√∂glichst hoch $P = \frac{TP}{TP+FP}$</li>
  <li>True Positve Rate (TPR) / Recall / Positive R√ºckmeldung: m√∂glichst hoch $TPR = R =\frac{TP}{TP+FN} = 1 - FNR $</li>
  <li>F1-Ma√ü: Harmonisches Mittel zwischen Precision und Recall. M√∂glichst hoch $F_1  =\frac{2}{\frac{1}{R}+\frac{1}{P}}$</li>
</ul>

<p>Durch Kombination von Metriken Auswahl der besten Hypothese m√∂glich,
z.B.</p>

<ul>
  <li>TPR/FPR-Graph auch <strong>ROC</strong> [Receiver operating characteristic(https://en.wikipedia.org/wiki/Receiver_operating_characteristic) genannt</li>
  <li>Precision-Recall-Graph</li>
</ul>

<p><strong>Cross-Validierung:</strong> Modell statistisch auswerten, mit Teil der Daten evaluieren ‚Üí Modell gut oder schlecht?;
Vorgehen: Teile Daten wiederholt in Lern- und Validierungsdaten auf, bestimme gute Hypothese, berechne Metriken
 ‚Üí wiederhole n mal ‚Äún-fold-cross-validation‚Äù</p>

<p>Resultat: Die Varianz der Daten variiert √ºber die Aufteilungen, dadurch w√ºrde
 eine instabile Lernmaschine identifiziert werden. Solche Maschinen sind nicht
 w√ºnschenswert.</p>

<p><strong>Bootstrap:</strong> Wie mit einfachen Verfahren mehr erreichen?
Zuf√§lliges Ziehen der Beispiele mit Zur√ºcklegen, Modellbestimmung &amp; Parameter, Wiederholen‚Ä¶
bestimme Mittelwert, Varianz des Modells;</p>

<blockquote>
  <p>‚ÄúDas Bootstrapping-Verfahren, oder Bootstrap-Verfahren (selten M√ºnchhausenmethode), ist in der Statistik eine Methode des Resampling. Dabei werden wiederholt Statistiken auf der Grundlage lediglich einer Stichprobe berechnet. Verwendung finden Bootstrap-Methoden, wenn die theoretische Verteilung der interessierenden Statistik nicht bekannt ist.‚Äù</p>
</blockquote>

<p>https://de.wikipedia.org/wiki/Bootstrapping-Verfahren</p>

<p><strong>Bagging</strong> (Bootstrap aggregation)
Variante Bagging: verwende mehrere Lernmaschinen ‚Üí Kombination der Lernmaschinen
und dann Mehrheitsentscheid der einzelnen Maschinen</p>

<p><strong>Boosting f√ºr Klassifikation</strong>: Kombination ‚Äúschwacher‚Äù Modelle, um gutes Modell zu erhalten</p>

<p><strong>Adaptive Boosting / AdaBoost</strong>: Iteratives Erstellen eines komplexen Klassifikators in k Stufen,
Ziehen von Lernbeispielen enstprechend definierter Gewichte; mit neuer Lernmenge
wird mit Hilfe eines Lernverfahrens n√§chster Klassifikatior bestimmt.</p>

<p>Adaptive Boosting (1996): Man versucht iterativ besser zu werden. Also w√§hlt man
als Trainingsdaten f√ºr den n√§chsten anzutrainierenden Klassifikator die
Daten, die von den bisherigen Modellen falsch klassifiziert wurden.
Dann kann man ein <strong>Ensemble</strong> aus den Modellen erstellen, wobei es auch
eine ‚ÄúSicherheitsmetrik‚Äù gibt.</p>

<p>Gutes Beispiel: Kaskadierung von <strong>Viola &amp; Jones (2001)</strong>:
Sie haben Gesichterkennung mit Haar-like Filtern gemacht.</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/uEJ71VlUmMQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Das Cascading ist interessant, weil folgendes passiert:
Erster Klassifikator hat precision 0.99 und recall 0.3. Wenn man zehn
solcher Klassifikatoren hintereinanderreiht, dann bekommt man precision = 0.99^10 ~= 0.9
 und recall = 0.3^10 ~= 0.00.</p>

<p><strong>General Adversarial Networks (GAN)</strong> (auch Turing-Lernen) profitiert vom
Boosting.</p>

<p><strong>Probably Approximate Correct</strong>:</p>

<blockquote>
  <p>‚ÄúKann man mit einfacher Lernmaschine ein gegebenes Konzept lernen?‚Äù Den Fehler
approximiert man auf ein epsilon. PAC Stichprobenkomplexit√§t: Wie viele Lerndaten
brauche ich, um das Problem zu l√∂sen?</p>
</blockquote>

<p>Aus Menge X von Instanzen mit L√§nge n, Konzept C, Hypothesenraum H und Lerndatenmenge D kann keine korrekte Hypothese gefunden werden, aber eine Œµ-genaue:
$E_D(h) ‚â§ Œµ, 0 &lt; Œµ &lt; \frac{1}{2}  $ (Approximate Correct); Wert kann mit Wahrscheinlichkeit Œ¥ $1 - Œ¥, 0 &lt; Œ¥ &lt; \frac{1}{2}$ gefunden werden; Anzahl ben√∂tigter Lerndaten:
$ m ‚â• \frac{1}{Œµ}(lm\frac{1}{Œ¥} + ln|H|)$ <br />
‚Üí je gr√∂√üer gew√ºnschte Sicherheit, je kleiner zul√§ssige Fehler, je gr√∂√üer Hypothesenraum ‚Üí umso gr√∂√üer Anzahl ben√∂tigter Daten</p>

<p>Eigentlich ist das eine sch√∂ne Sache, aber es geht leider nur bei einfachen
Lernmaschinen.</p>

<p><strong>Vapnik-Chervonenkis (VC) Dimension</strong>: ist maximale Anzahl von Datenpunkten die von der Hypothese beliebig separiert werden k√∂nnen; eine Hypothese h separiert die Daten D wenn zwei
Untermengen definiert werden k√∂nnen: ${x|h(x) = 0 } $ und ${x|h(x) = 1 } $; Beispiel: Hypothesenraum durch Geraden separiert, maximal 3 Werte durch Geraden spariert (nur wenn Aufteilung egal);
Allgemein: Hyperebenen in $R^n ‚áí n+1 $ separierte Werte</p>

<p>‚Äúbessere PAC-Strichprobenkomplexit√§t‚Äù</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/puDzy2XmR5c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p><strong>‚ÄúKorrektes Lernen‚Äù:</strong> Vapnik</p>

<ul>
  <li>Kapazit√§t so gering wie m√∂glich</li>
  <li>Optimierung so gut wie m√∂glich</li>
  <li>Trainingsdaten so repr√§sentativ und viele wie m√∂glich</li>
</ul>

<p><strong>Absch√§tzung des Testfehlers</strong>: Lernerfolg abh√§ngig von Kapazit√§t der lernenden Maschine (so gering wie m√∂glich), Optimierungsmethode (so gut wie m√∂glich), Lernbeispiele (so viele wie m√∂glich,
repr√§sentativ)</p>

<p><strong>Prof: Man bekommt die obere Schranke des Fehlers nicht nur durch
Reduktion des empirischen Fehlers des Modells hin, sondern auch durch:</strong></p>

<p><strong>Structural Risk Minimization</strong>: finde Maschine $VC(h_a)$, Beispiele $N$ und Minimum des empirischen Fehlers $\alpha$: $ min_{H_n} (E_{emp} (h_a)+ \sqrt (‚Ä¶ \frac{VC(h_{\alpha})}{N} ‚Ä¶)) $ <br />
Strukturiere Hypothesenraum, Iteriere √ºber Teile des Hypothesenraum, Suche Minimum f√ºr $E(h_{\alpha}), stoppe wenn Summe minimal; Berechnung VC Dimension schwer, rechenintensiv</p>

<blockquote>
  <p>‚ÄúSRM ist Meta-Algorithmus: Wie bekommt man strukturell aufsteigende Kapazit√§t
√ºber Hypothesenraum? Zum Beispiel AdaBoost implementiert SRM:
Kapazit√§t steigt mit jedem neuen Klassifikator. Auch SVM macht das implizit.‚Äù</p>
</blockquote>

<p><strong>Das schien dem Professor sehr wichtig zu sein!</strong></p>

<p>GAN generieren neue Lernbeispiele, dadurch wird N in der VC-Ungleichung gr√∂√üer
und die obere Schranke geht nach unten.</p>

<p>Der Professor verweist auf Tom Mitchells Buch ‚ÄúMachine Learning‚Äù und
auf das erste Kapitel von Vapniks ‚ÄúStatistical Learning Theory‚Äù</p>

<p>F√ºr die Klausur sollte man wissen:</p>

<ul>
  <li>Strukturelles Risiko</li>
  <li>Vapnik-Chervonenkis grobe Entwicklung</li>
  <li>PAC grunds√§tzliche Aussage</li>
</ul>

<h3 id="neuronale-netze">Neuronale Netze</h3>
<p><strong>K√ºnstliche Neuronale Netze (KNN)</strong>: externe Eingabe ‚Üí Gewichtsbelegung ‚Üí Propagierungsfunktion ‚Üí Aktivierungsfunktion ‚Üí Ausgabe<br />
Einsatzfelder:</p>
<ul>
  <li><em>Klassifikation und Mustererkennung:</em> Diagnose, Spracherkennung, Schrifterkennung, Bilderkennung</li>
  <li><em>Funktionsapprocimierung/Regression:</em> Kontinuierliche Abbildung, Steuerung, Vorhersage</li>
  <li><em>Musterervervollst√§ndigung:</em> Bilderzeugung, generative Modelle, Kodierung, Komprimierung</li>
</ul>

<p><strong>Perzeptron</strong>: Anlehnung an nat√ºrliche Wahrnehmung,
Perceptron realisiert Trennhyperebene (in $R¬≤$ eine Gerade), Gewichte definieren (stellen Normale dar),
Gegeben Positive und Negative‚Äú Daten $(P,N)$ erfolgt eine Entscheidung durch gewichtete Summe ‚Üí Skalarprodukt mit dem Nomalenvektor, <em>Lernen = Anpassen der Gewichte ‚Üí
Gesucht wird die beste Trennhyperebene</em></p>

<p><a title="By Mayranna [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Perceptron_moj.png"><img width="512" alt="Perceptron moj" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Perceptron_moj.png/512px-Perceptron_moj.png" /></a></p>

<p><em>Lernalgorithmus:</em></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Gegeben</span> <span class="n">Lerndatenmenge</span> <span class="n">P</span> <span class="err">‚à™</span> <span class="n">N</span>    
<span class="n">Erzeuge</span> <span class="n">der</span> <span class="n">Gewichtsvektor</span> <span class="n">w</span> <span class="n">zuf</span><span class="err">√§</span><span class="n">llig</span>  
<span class="n">While</span> <span class="n">Z</span><span class="err">√§</span><span class="n">hler</span> <span class="o">&lt;</span> <span class="n">Schwellenwert</span><span class="p">:</span>  
    <span class="n">W</span><span class="err">√§</span><span class="n">hle</span> <span class="n">ein</span> <span class="n">Trainingsbeispiel</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">P</span> <span class="err">‚àà</span> <span class="n">N</span> <span class="n">zuf</span><span class="err">√§</span><span class="n">llig</span>  
    <span class="n">If</span> <span class="n">x</span> <span class="err">‚àà</span> <span class="n">P</span><span class="p">:</span>  
        <span class="n">If</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="p">(</span><span class="n">korrekt</span> <span class="n">klassifiziert</span><span class="p">):</span>  
            <span class="n">dann</span> <span class="n">tue</span> <span class="n">nichts</span>  
        <span class="n">Else</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="err">‚â§</span> <span class="mi">0</span><span class="p">:</span>  
            <span class="n">Setze</span> <span class="n">w</span> <span class="err">‚Üê</span> <span class="n">w</span> <span class="o">+</span> <span class="n">x</span>  
    <span class="n">Else</span> <span class="n">x</span> <span class="err">‚àà</span> <span class="n">N</span><span class="p">:</span>  
        <span class="n">If</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="p">(</span><span class="n">korrekt</span> <span class="n">klassifiziert</span><span class="p">):</span>  
            <span class="n">dann</span> <span class="n">tue</span> <span class="n">nichts</span>  
        <span class="n">Else</span> <span class="n">x</span> <span class="err">‚àà</span> <span class="n">N</span> <span class="n">und</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="err">‚â•</span> <span class="mi">0</span><span class="p">:</span>  
            <span class="n">Setze</span> <span class="n">w</span> <span class="err">‚Üê</span> <span class="n">w</span> <span class="err">‚Äì</span> <span class="n">x</span>  
    <span class="n">If</span> <span class="n">alle</span> <span class="n">x</span> <span class="n">richtig</span> <span class="n">klassifiziert</span><span class="p">:</span>  
        <span class="n">Break</span><span class="p">,</span>  
    <span class="n">Update</span> <span class="n">Z</span><span class="err">√§</span><span class="n">hler</span>  </code></pre></figure>

<p><em>Grenzen des Perzeptron Lernalgorithmus:</em> $|w|¬†¬ª |x|$: sehr langsame Anpassung, Normierung n√∂tig, Worst Case antiparallele Vektoren ‚Üí Gradientenabstieg ‚Üí differenzierbare Aktivierungsfunktion</p>

<p>Ein Perzeptron hat niedrige Kapazit√§t ‚Üí Zusammenschalten von Neuronen (‚ÄúPerzeptronen‚Äù) kann man Kapazit√§t erh√∂hen</p>

<p><strong>Kernel Methode</strong>: lineare Trennung durch Kernel Methode realisierbar, Lineare Trennung im transformierten Raum f√ºhrt zu komplexer Trennung im Ursprungsraum</p>

<p><strong>Multi Layer Neural Network</strong>: mehrere versteckte (innere) Schichten, Lernverfahren mit Backpropagation-Algorithmus, nichtlineare Aktivierungsfunktionen in Neuronen</p>

<p><strong>Nichtlineare Aktivierungsfunktionen</strong>:</p>
<ul>
  <li><em>Sigmoid:</em> $f(x) = \frac{1}{1+e^{-x}}, \frac{\partial f}{\partial x} = f(x)(1-f(x))$</li>
  <li><em>Tangens Hyperbolicus:</em> $f(x) = tanh(x), \frac{\partial f}{\partial x} = (1+f(x))(1-f(x))$</li>
  <li><em>ReLU:</em> $f(x) = max(0,x), \frac{\partial f}{\partial x} = \begin{cases} 1 &amp; x &gt; 0 \\ 0 &amp; \text{sonst} \end{cases}$</li>
  <li><em>LeakyReLU:</em> $f(x) = \begin{cases} x &amp; x &gt; 0 \\ \alpha x &amp; x \leq 0 \end{cases}, \frac{\partial f}{\partial x} = \begin{cases} 1 &amp; x &gt; 0 \\ \alpha &amp; \text{sonst} \end{cases}$</li>
</ul>

<p><strong>Backpropagation Algorithmus</strong>: Aus Menge T von Trainingsbeispielen als Eingangs/Ausgabevektor, Lernrate $\mu$, Netztopologie finde Gewichtsbelegung W die T korrekt wiedergibt, Vorgehen
mittels Gradientenabstieg</p>

<p><strong>Anpassen der Gewichte</strong>:</p>
<ul>
  <li>Lernen aus Einzeldaten (Pattern learning): Anpassung nach jedem Lernbeispiel, schnelles Lernen (kein ‚Äúechter‚Äù Gradientenabstieg, aber gute Approximation)</li>
  <li>Lernen aus Teilmengen (mini batch learning): kleine Lernmenge mit Zur√ºcklegen, gutes Lernen (kein ‚Äúechter‚Äù Gradientenabstieg, aber gute Approximation)</li>
  <li>Epochenlernen(epoch learning): Mittelung der Gewichts√§nderung √ºber alle Beispiele, Anpassung nachdem Beispiele propagiert wurden, echter Gradientenabstieg, nicht Ausrei√üeranf√§llig</li>
</ul>

<p><strong>Optimierungen des Gradientenabstiegs</strong>:</p>
<ul>
  <li><em>Resilient Propagation (RPROP):</em> Implementiert normierte Schrittweite und Lernratenanpassung, Beschleunigung auf flachem Plateau, langsames Anpassen im Minimum ‚Üí schnelle Konvergenz (In den 90ern am KIT entstanden)</li>
  <li><em>Adaptive Moment Estimation (Adam):</em> Angepasste Lernraten f√ºr jeden Parameter,
pro Parameter werden gleitende Sch√§tzer f√ºr Mittelwert und Varianz mitgef√ºhrt, Implizites Momentum und
Anpassung an die Varianz der Gradienten.
<strong>Vorteil gg√º. RPROP:</strong> Abstiegskurve wird gegl√§ttet, was bei realen Daten wichtig ist</li>
  <li><em>Xavier ‚Äì Initialisierung:</em> Verringerung des Vanishing- und Exploding-Gradients durch optimierte Initialisierung</li>
</ul>

<p><strong>Topologieauswahl</strong>: Anzahl Neuronen pro Schicht zur Anzahl von Lerndaten wichtig,
Schichten stark abh√§ngig von Zielfunktion, ABER: zu viele Neuronen, zu wenig Lerndaten ‚Üí Overfitting;
Gleichzeitiges Trainieren des NN und finden der optimalen Topologie:</p>
<ul>
  <li>gro√ües Netzwerk verkleinern und am auswendig lernen hindern
‚Üí Weight Decay, Weight Elimination = Bestrafen von gro√üen W durch Verwendung erweiterter Fehlerfunktion
 ‚Äúoptimal brain damage‚Äù</li>
  <li>kleines Netzwerk vergr√∂√üern und erweitern bis es Daten lernt
    <ul>
      <li>Meiosis Netzwerke: Aufspaltung von Neuronen</li>
      <li>Cascade Correlation: dort neues Neuron einf√ºhren, wo der Fehler am gr√∂√üten ist</li>
    </ul>
  </li>
</ul>

<p><strong>Cascade Correlation</strong>: Initialisiere 2 schichtiges Netz, Festlegen der Abbruchkriterien, Trainieren, solange E(w) kleiner als Fehlerschranke: Neuron einf√ºgen,
Neuron trainieren, Netz trainieren <br />
<em>Vorteile:</em> nur eine Ebene zum selben Zeitpunkt trainiert, schnell, inkrementelles Training, iterative Anpassung des Netzes
<em>Nachteile:</em> spezielle Architektur ‚Üí schwer G√ºte zu bestimmen, nicht anwendbar bei komplexen Architekturen</p>

<p><strong>Dropout</strong>: Umsetzung von Bagging (bootstrap aggregation): k Modelle trainiern auf Daten (mit Zur√ºcklegen), Mittelung der Ergebnisse der Modelle, Ziel: Erwartungswert des Fehlers reduzieren;
bei sehr gro√üen Netzen ‚Üí viele Netze lernen ‚Üí overhead; Daher: approximative, implizite Umsetzung ‚Üí Maskierung/Inaktivierung von Neuronen beim Lernen ‚Üí auf allen Subnetzen gleichzeitg lernen,
Lernen mit jeweils aktiven Neuronen
<strong>Wichtig:</strong> Nach dem Trainieren muss man die Gewichte skalieren. Zum Beispiel
bei einem Dropout von 50% m√ºssen die Gewichte halbiert werden, da sie sonst
wenn alle Neuronen aktiv sin ein doppelt so gro√ües Ergebnis liefern im Vergleich zu Trainingphase.
<strong>Bemerkung:</strong> Das funktioniert, weil jedes Model im Ensemble seine eigene
Fehleroberfl√§che hat und das Mittel k√∂nnte besser sein als nur eine einzelne.</p>

<p><strong>Exploding/Vanishing-Gradient Problem</strong>: je weiter Schicht von Ausgabeschicht entfernt,
desto kleiner werden Gradienten, falls Gewichtsmatrizen klein sind, ergeben sich viele kleine Werte,
die miteinander multipliziert werden, Update im Gradientenabstiegsverfahren l√§sst Gewichte der unteren Schichten praktisch unver√§ndert und Training konvergiert nicht zu guter L√∂sung; falls
Gewichtsmatrizen gro√ü sind, ergeben sich viele gro√üe Werte, die miteinander
multipliziert werden, explodierende Gradienten k√∂nnen Lernen instabil machen, NN kann nciht aus Trainingsdaten
lernen.</p>

<ul>
  <li><strong>Gradient Clipping</strong>: Gradienten w√§hrend Backpropagation einschr√§nken, dass sie Schwellwert nicht √ºberschreiten</li>
  <li><strong>Gradient Norm</strong>:</li>
  <li><strong>Residual Learning</strong>: Optimierung durch zus√§tzliche Verbindungen √ºber Schichten hinweg, st√§rkere R√ºckpropagierung des Fehlers in untere Schichten</li>
</ul>

<p><strong>Early Stopping</strong>: Bei Gefahr des Overfittings Training abbrechen; in Intervallen von n Trainingszyklenn wird gepr√ºft ob der Testfehler p mal aufeinanderfolgend gr√∂√üer wird</p>

<p><strong>Label smoothing</strong>: Miteinberechnen, dass die Labels der Daten nur zu $1 - \epsilon$
richtig sind.</p>

<p><strong>Auswahl repr√§sentativer Trainingsbeispiele</strong>:</p>
<ul>
  <li><em>Lerndaten</em> ‚Üí Anpassung der Gewichte</li>
  <li><em>Testdaten</em> ‚Üí Testen des Fehlers und Overfitting</li>
  <li><em>Verifikationsdaten</em> ‚Üí Feststellen der Generalisierung<br />
‚Üí Gute Verteilung n√∂tig, aber unbekannt</li>
</ul>

<p><strong>Einsatz von MLNN</strong>:</p>
<ol>
  <li><em>Entwurf:</em> subsymbolische Repr√§sentation der Ein-/Ausgabe, Trainingsdatenauswahl, Verfahrensauswahl, Topologieauswahl, Parametereinstellung</li>
  <li><em>Auswahl des Lernverfahrens:</em> Optimierungsmethode, Initliasisierung, Lernansatz</li>
  <li><em>Lernfortschritt (Gewichtsanpassung):</em> Overfitting testen</li>
  <li><em>Training &amp; Verifikation (Test)</em></li>
</ol>

<h3 id="convolutional-neuronale-netze">Convolutional Neuronale Netze</h3>
<p><strong>Erste k√ºnstliche Ans√§tze</strong>: Architektur von Verarbeitungsschritten, pro Schritt zwei Komponenten, erste reagiert auf Bereich zuvor (Lernen),
zweite komprimiert den Bereich (Informationskomprimierung)</p>

<p><strong>Faltung</strong>: = Convolution, hier diskrete Faltung, bekannt aus Bildverarbeitung: Kantenfilter, Bildgl√§ttung, Bildsch√§rfung;
Beispiel in Vorlesung: $m√óm$ Eingabebild + $n√ón$ Filter ‚Üí $m_{neu}√óm_{neu}$ mit $m_{neu} = m - n + 1$</p>

<p><strong>Warum CNNs</strong>: Bild mit m<em>m Pixel: pro Neuron m</em>m Gewichte ‚Üí ung√ºnstiges Verh√§ltnis Parameter : Lernaufgabe ‚Üí nicht sinnvoll lern/berechenbar;
Beispiel Gesichtserkennung: √§hnliche Features werden mehrfach gelernt, da jedes Feature f√ºr jedes receptive fiesld unabh√§ngig gelernt wird</p>

<p><strong>Unterschied NN / CNN</strong>: Im NN sind die Gewichte von einem Layer zum anderen, im CNN nur eine Kante zwischen Layer</p>

<p><strong>Feature Maps</strong>: Form, in der Daten von Layer zu Layer √ºbertragen werden; jedes Layer enth√§lt als Eingabe r Feature Maps (m√óm),
im Input Layer ist Feature Map ‚â° Eingabebild; Featgures Maps zwischen zwei Layern = <em>Tensor</em></p>

<p><strong>Pooling Layer</strong>: Zusammenfassung kleiner Bildbereiche, Gr√∂√üe nach Zweck und Gr√∂√üe des Eingabebildes gew√§hlt, verschiedene
Strategien:</p>
<ul>
  <li>Max Pooling: Maximum wird gew√§hlt</li>
  <li>Mean Pooling: Mean wird gew√§hlt</li>
  <li>Stochastic Pooling: stochastische Auswahl abh√§ngig von Aktivierung</li>
</ul>

<p>‚Üí lokale Translationsinvarianz, Invarianz gegen leichte Ver√§nderung, Verzerrung, Datenreduktion</p>

<p><strong>Convolution Layer</strong>: Anwendung von Faltungsoperationen auf Eingabe, Eingabe: Feature Maps von vorherigen Layer ($m√óm$),
 Ausgabe: Feature Maps des Layers (entstanden durch Faltung der Eingabe $m√óm$ mit k Filtern $n√ón√óq$)
‚Üí Eingabe: $m√óm$ ‚Üí k Filter $n√ón√óq$ ‚Üí Ausgabe $m_{neu}√óm_{neu}√ók$ mit $m_{neu} = \frac{(m-n)}{s}+1$ mit s = Stride/Schrittgr√∂√üe
Stride muss zur Gr√∂√üe der Eingabe passen, bei Pooling Layern meist 1; ConvLayer mit Stride erreicht Reduktion durch ASulassen,
Max Pooling betrachtet alle Werte bei Reduktion, bei ConvLayer gr√∂√üer 1√ó1 ‚Üí Randbetrachtung notwendig: Informationsverlust bei Faltung,
Beheben:</p>
<ul>
  <li><em>don‚Äôt care:</em> Berechnung des inneren Bereichs, Reduktion der Ergebnisgr√∂√üe</li>
  <li><em>Padding:</em> stabilisiert Gr√∂√üenverlauf der Layer, auff√ºllen der verlorenen Zeilen/Spalten, Varianten: Zero Padding (
Auff√ºllen mit 0), Nearest Padding (Duplizieren der Randpixel), Reflect Padding (Matrix nach au√üen spiegeln); ohne Padding
gehen Informationen am Bildrand sukzessive verloren, in jedem ConvLayer ohne Padding, gro√üe Filter beschleunigen Problem,
Pooling mit gro√üen p auch; mit Padding Gefahr, dass Strukturinformationene entstehen die im Eingabebild nicht waren</li>
</ul>

<p><strong>Activation Layer</strong>: nichtlineare Aktivierung, in CNNs: ReLU, normalerweise nach ConvLayer
Aktivierungsfunktionen:</p>
<ul>
  <li><em>ReLU</em>: $f(x) = max(0,x), \frac{\partial f}{\partial x} = \begin{cases} 1 &amp; x &gt; 0 \\ 0 &amp; \text{sonst} \end{cases}$</li>
  <li><em>Leaky ReLU (LReLU)</em>: $f(x) = \begin{cases} x &amp; x &gt; 0 \\ 0.01 x &amp; x \leq 0 \end{cases}, \frac{\partial f}{\partial x} = \begin{cases} 1 &amp; x &gt; 0 \\ \alpha &amp; \text{sonst} \end{cases}$</li>
  <li><em>PReLU (Generalisierung von LReLU)</em>: $f(x) = \begin{cases} x &amp; x &gt; 0 \\ \alpha x &amp; x \leq 0 \end{cases}, \frac{\partial f}{\partial x} = \begin{cases} 1 &amp; x &gt; 0 \\ \alpha &amp; \text{sonst} \end{cases}$</li>
  <li><em>ELU (Exponential Linear Unit)</em>: $f(x) = \begin{cases} x &amp; x &gt; 0 \\ \alpha(e^x-1) x &amp; x \leq 0 \end{cases}, \frac{\partial f}{\partial x} = \begin{cases} 1 &amp; x &gt; 0 \\ \alpha &amp; \text{sonst} \end{cases}$</li>
</ul>

<p><strong>Features</strong>: CNN lernt Features f√ºr bestimmte Dom√§ne, Feature Detektoren in Filtern der Convolutional Layer, Gewichte werden durch Gradienten
abstieg gelernt, Features eines Layers basieren auf Ausgaben der Features des vorherigen Layers,</p>

<p><strong>Gewichte</strong>: Gewichte in CNNs werden nur in Filtern gelernt,
Methoden zur Initialisierung:</p>
<ul>
  <li><em>Random Initialization:</em> zuf√§llig</li>
  <li><em>Fixed Feature Extractor:</em> √úbernahme der Gewichte aus trainiertem Netz, fixieren in Feature Extraction Schicht</li>
  <li><em>Fine-Tuning:</em> √úbernahme der Gewichte aus trainiertem Netz, geringe Lernrate f√ºr ausgew√§hlte Schihten</li>
  <li><em>Pretrained Initialization:</em> √úbernahme der Gewichte aus trainiertem Netz nut zur Initialisierung, normale Lernrate f√ºr ausgew√§hlte Schichten</li>
</ul>

<blockquote>
  <p>CNNs werden besser, da Rechenkapazit√§t st√§ndig w√§chst, immer gr√∂√üere Datens√§tze verf√ºgbar, bessere CNN Architekturen</p>
</blockquote>

<p><strong>Fully Connected Layer</strong>: v.a. bei Klassifikation, Anzahl der Neuronen im letzten Layer korrespondiert zu der Anzahl an
Klassen, die Netz unterscheiden soll</p>

<p><strong>Fully Convolutional Networks (FCN)</strong>: gr√∂√üere Eingabebilder m√∂glich als bei Fully Connected CNNs, Fully Connected Schichten
in Convolutional Schichten konvertiert</p>

<p><strong>Ausgaben der FCN</strong>:</p>
<ul>
  <li>Fully Connected CNN (Klassifikationsnetz): Wahrhscheinlichkeitswert pro Klasse</li>
  <li>Fully Convolutional Network: Wahrhscheinlichkeitskart pro Klasse (gibt p an und Bereich in dem Bild zur Klasse geh√∂rt)</li>
</ul>

<h3 id="support-vector-machine-svm">Support Vector Machine (SVM)</h3>
<p><strong>Support Vektor Methode</strong>: Hypertrennebene finden, sodass Mengen A und B geteilt werden (Klassifikation). Ziel ist es die beste
Trenngerade zu finden, sodass der Rand (Abstand Gerade zu den Elementen der Ebene) maximal wird (Generalisierung)</p>

<p><strong>Trennhyperebene</strong>: Normalenvektor $\vec{w}$ beschreibt Gerade durch Ursprung, senkrecht zu $\vec{w}$ verlaufen Trennhyperebenen, welche die
Gerade $\vec{w}$ an einer Stelle schneiden, der Abstand der Schnittstelle zu Urpsung ist $b$: $\vec{w}\vec{x} + b = 0$<br />
<em>Rand</em>: ist Abstand zwischen den n√§chsten Datenpunkten jeder Klasse $\frac{\vec{w}(x_‚Åªx_2)}{|\vec{w}|}$<br />
<em>Normierung:</em> $\vec{w}\vec{x}+b = +1 $ bzw $\vec{w}\vec{x}+b = -1 $ ‚Üí maximaler Rand (Abstand zwischen zwei Klassen): $\frac{2}{|\vec{w}|}$<br />
<em>Optimale Hyperebene:</em> Abstand $\frac{1}{|\vec{w}|}$ zum n√§chsten Punkt und Abstand $\frac{2}{|\vec{w}|}$ zwischen zwei Klassen<br />
<em>Optimierungsproblem:</em> = Minimierungsproblem von $|\vec{w}|¬≤$ um $\frac{2}{|\vec{w}|}$ zu maximieren unter der Randbedingung, dass die
Daten auch korrekt klassifiziert werden; Vapnik: Lernmaschine mit kleinsten m√∂glichen VC-Dimension (= g√ºltiger Hypothesenraum), falls
Klassen linear trennbar<br />
<em>Prim√§res Optimierungsproblem:</em> Lagrange Methode, Finde den (eindeutigen) Sattelpunkt $L_p$ der Funktion mit den positiven Lagrange-Multiplikatoren
$\vec{\alpha}=(\alpha_1, ‚Ä¶, \alpha_n)$ mit $\alpha_1, ‚Ä¶, \alpha_n \geq 0$, Minimum von L bez√ºglich $\vec{w}b$, Maximum von L bez√ºglich
$\alpha_1, ‚Ä¶, \alpha_n$<br />
<em>Duales Optimierungsproblem:</em> Duale Lagrange Gleichung $W(\alpha)$, duales Optimierungsproblem unter der Randbedingung dass Summe der
$y_i\alpha_i = 0 $ und $\alpha_i \geq 0$ ‚Üí Vorteile nurnoch von $\alpha_i$ abh√§ngig und damit einfacher zu l√∂sen</p>

<p><strong>Support Vektoren</strong>: Vektoren, die am n√§chsten zur Trennhyperebene liegen (wichtige Vektoren f√ºr Lernmaschine); die meisten (Sattelpunkt)
Bedingungen sind erf√ºllt ‚Üí $\alpha_i = 0$; $\vec{w}$ ist Linearkombination weniger Support Vektoren</p>

<p><strong>Soft Margin</strong>: ‚ÄúAufweichen der Ebene‚Äù um einen ‚Äúweichen Rand‚Äù, erlaube geringer Zahl von Missklassifikationen, h√∂here Generalisierung,
indem Randbedingung um den Wert der Schlupfvariable $\xi \geq 0$ erweitert wird ‚Üí d.h. Datenpunkt kann auch leicht neben Gerade auf
‚Äúfalscher Seite‚Äù liegen <br />
<em>Regulierungsparameter $C$ (Gr√∂√üe des Randes)</em>:</p>
<ul>
  <li><em>$C$ gro√ü:</em> wenig Missklassifikationen</li>
  <li><em>$C$ klein:</em> maximaler Rand<br />
Folge f√ºr Support Vektoren $\vec{x_i}$ mit $\alpha_i &gt; 0$:</li>
  <li>$\alpha_i &lt; C$: Support Vektor liegt am Rand, Abstand zur Trennhyperebene ist $\frac{1}{|\vec{w}|}$</li>
  <li>$\alpha_i = C$:
    <ul>
      <li>Fehlklassifikation wenn $\xi_i &gt; 1$</li>
      <li>richtig, wenig Abstand (margin error): $0 &lt; \xi_i \leq 1$</li>
      <li>Rand Vektor (margin vetctor): $\xi_i = 0$</li>
    </ul>
  </li>
</ul>

<p><strong>Nichtlineare Kernel-Methoden</strong>: Transformiere Daten in anderen Raum und l√∂se dort linear (Klassifikation: lineare Trennung); meist
Transformation in h√∂her dimensionalen Raum, Problem: Transformation oft rechenintensiv:<br />
<em>Kernel-Trick:</em> wenn Daten als Skalarprodukt, Transformation nicht n√∂tig, implizites Rechnen im h√∂her dimensionalen Raum ‚Üí L√∂sung
im Ursprungsraum; Kernel-Funktion: $K(\vec{x}, \vec{y}) = \phi(\vec{x})\phi(\vec{y})$<br />
<em>Kernel-Funktionen:</em> Skalarprodukt, Polinomial, Radiale Basis-Funktionen (RBF), Sigmoid</p>

<p><strong>Version Space f√ºr SVM</strong>: Randbedingung f√ºr korrekte Klassifikation/g√ºltige Trenngeraden; jeder Datenpunkt definiert Hyperebene
im Hypothesenraum, g√ºltige $\vec{w}$ jeweils in reduziertem Hypothesenraum ‚Üí Suche nach $\vec{w}$ mit maximalen Abstand zu allen
Hyperebenen der Datenpunkte ‚Üí Mittelpunkt der Hyperkugel; bei Suche √ºber Trennhyperebene wird implizit Structural Risk Minimization
durchgef√ºhrt ‚Üí korrektes Lernen ist Mittelpunkt der verbleibenden Hyperkugel</p>

<p><strong>SVM Architektur</strong>: √Ñhnlichkeit zu NNs? ‚Üí lernen Gewichte, Kernel, hidden-Layer; ABER: sonst nichts: Optimierung, mathematischer
Hintergrund</p>

<p><strong>SVM Erweiterungen</strong>:</p>
<ul>
  <li>Einer-gegen-Alle: $k$ SVMs (pro Anzahl k an Klassen), Abstimmungsverfahren</li>
  <li>Einer-gegen-Einen: $\frac{k(k-1)}{2}$ SVMs, Abstimmungsverfahren</li>
  <li>Mehrfachzugeh√∂rigkeit: $k$ SVMs (pro Anzahl k an Klassen), Abstimmungsverfahren</li>
  <li>k-class SVM (Watkins): gemeinsames Optimierungsverfahren, kein Abstimmungsverfahren (‚Üí schlecht, schlechte Performance)</li>
</ul>

<p><strong>Probabilistische Sicht auf SVMs</strong>: Interpretation des Abstandes der Trennhyperebene als Klassifikationswahrscheinlichkeit, Alternative:
Maximierung der Klassifikationswahrscheinlichkeit des Randes</p>

<p><strong>Dichte-Tr√§ger-Sch√§tzung</strong>: Funktion f, die f√ºr ‚Äúkleine Region‚Äù (enth√§lt die meisten Lernbeispiele), Wert &gt; 0, sonst Wert $\leq$ 0
(√Ñhnlichj zum Clustering im un√ºberwachten Lernen), Abstand $\vec{w}$ zum Ursprung soll maximal sein, Trennung der Lernbeispiele vom
Ursprung im transformierten Merkmalsraum</p>

<p><strong>Kernel Perceptron</strong>: einfacher Algorithmus, als Kernel Methode realisierbar, lineare Trennung im transformierten Raum f√ºr zu
komplexer Trennung im Ursprungsraum</p>

<p><strong>Pro SVM</strong>:</p>
<ul>
  <li>Optimale Hyperebene ‚Üí gute Lernergebnisse</li>
  <li>optimale VC-Dimension ‚Üí korrektes Lernen</li>
  <li>hochdimensionale Daten ‚Üí schnelle Auswertung</li>
  <li>Entscheidung anhand Randregion getroffen</li>
</ul>

<p><strong>Kontra SVM</strong>:</p>
<ul>
  <li>Vorverarbeitung extern n√∂tig (kein ‚Äútiefes‚Äù Lernen)</li>
  <li>speicher- und rechenaufw√§ndig</li>
  <li>Anzahl Support Vektoren abh√§ngig von Problem und Parameter</li>
</ul>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<p><strong>Gesetz der Auswirkung</strong>: verschiedene Reaktionen, st√§rker mit Situation verkn√ºpft ‚Üí befriedigter Zustand oder
unangenehmer Zustand ‚Üí gegebene Situation geschw√§cht</p>

<p><strong>Reinforcement Learning (RL)</strong>: Lernziel: finde Aktionssequenz $a_1,‚Ä¶,a_n$ sodass maximale Bewertung erreicht wird</p>

<p><strong>Markov decision process (MDP)</strong>: <a href="https://de.wikipedia.org/wiki/Markow-Entscheidungsproblem">Wikipedia</a>: ‚ÄúNutzen eines Agenten
von einer Folge von Entscheidungen abh√§ngig, bei Zustands√ºberg√§ngen gilt: Wahrscheinlichkeit einen Zustand s‚Äô von Zustand
 s aus zu erreichen, ist nur von s abh√§ngig und nicht von Vorg√§ngern von s.‚Äù, Markov Bedingung: Keine Abh√§ngigkeit von Vergangenheit</p>
<ul>
  <li><em>Autonomer Agent &amp; Umwelt:</em> zustandsgetriebener Prozess, Sensorik $s_t \in S$ (Erfassung von Zust√§nden), Atorik $a_t \in A$
 (Einwirkung auf Umwelt durch Aktionen), Zustands√§nderung: $\delta: (SxA) ‚Üí S, \delta (s_t, a_t) = s_{t+1}$</li>
  <li><em>Bewertung von Aktionen</em>: direkt oder indirekt bekannt/messbar: $r:(SxA) ‚Üí R, r(s_t, a_t) = r_t</li>
</ul>

<p><strong>Strategielernen (Policy learning)</strong>: finde (optimale) Zielfunktion, so dass akkumulierte Bewertung (zum Ziel) maximiert wird</p>
<ul>
  <li><em>Zielfunktion:</em> $\pi: S ‚Üí A , \pi(s_t) = a$</li>
  <li><em>State Value Funktion (akkumulierte Bewertung):</em> $V^{\pi} (s_t) = \sum_{i=0}^{\infty} \gamma^i r_{t+1} $</li>
</ul>

<p>Gewichtung der Bewertungen (Diskontierungsfaktor): $ 0 ‚â§ \gamma ‚â§ 1$</p>
<ul>
  <li>0: aktuelle Aktionsbewertung ist wichtig (1-step)
-&gt;0: zuk√ºnftige (letzte) Bewertungen werden ber√ºcksichtigt (n-step)</li>
  <li>&lt;1: notwendig um konvergente V-Funktion zu erhalten</li>
</ul>

<p><strong>Simple Temporal Difference Learning</strong>: <a href="https://de.wikipedia.org/wiki/Temporal_Difference_Learning">Wikipedia</a>: 
‚ÄúAnpassung nicht erst bei Erhalt der Belohnung, sondern nach jeder Aktion auf Basis einer gesch√§tzten erwarteten Belohnung.‚Äù, <br />
<em>Algorithmus</em>: Initialisiere gesch√§tzte erwartete Belohnung zuf√§llig, wiederhole (do forever): w√§hle Zustand $s_t$ ermittle beste Aktion und 
Folgezustand und ersetze erwartete Belohnung; Problem: langsames Lernen (do forever) <br />
<em>verbesserter Algorithmus:</em> Wiederhole bis Endzustand erreicht ist<br />
<em>Problem:</em> Bewertungen und Zustands√§nderung m√ºssen bekannt sein ‚Üí nicht realistisch f√ºr echte Anwendungen</p>

<p><strong>Problemdimensionen bei RL</strong>:</p>
<ul>
  <li><em>Zielfunktion:</em> Vorhersage ‚áî Aktionswahl</li>
  <li><em>Bewertung:</em> direkt ‚áî verz√∂gert</li>
  <li><em>Zustands√ºberg√§nge:</em> deterministisch ‚áî stochastisch</li>
  <li><em>Modell des Systems:</em> vorhanden ‚áî nicht vorhanden</li>
  <li><em>Zustands- und Aktionsraum:</em> eindimensional ‚áî hochdimensional, diskret, ‚áî kontinuierlich</li>
</ul>

<p><strong>Q-Funktion</strong>: deterministischer MDP, $Q(s,a)$ maximale Bewertung, die erreicht werden kann f√ºr Zustand $s$ und Aktion $a$: <br />
$Q(s,a) = r(s,a) + \gamma V*(\delta(s,a))$ (<em>Bellmann</em>), $V<em>(s) = max_{a‚Äô} Q(s,a‚Äô) $; w√§hle beste Aktion anhand einer Strategie 
‚Üí in Anwendung wenig Wissen √ºber Zustands√ºberg√§nge n√∂tig <br />
*Algorithmus:</em> Ziel finde Sch√§tzung $\hat{Q}(s,a)$ der absoluten Funktion $Q(s,a)$; alle $\hat{Q}(s,a) = 0$, w√§hle Zustand s
und f√ºhre AKtion a aus ‚Üí Bewerte r, neuer Zustand s‚Äô, update $\hat{Q}$ mit s = s‚Äô; nach n Iterationen:$ 0 ‚â§ $\hat{Q}_n(s,a)$ ‚â§ $Q(s,a)$</p>

<p><strong>Suchstrategien</strong>: Welche Aktion soll gew√§hlt werden? ‚Üí maximales $\hat{Q}(s,a)$? ‚Üí ‚Äúlokales‚Äù Lernen nur bestimmter Aktionen, 
Aktionen, die nicht gew√§hlt werden, k√∂nnten besser sein ‚Üí Probabilistische Auswahl nach Faktor k:</p>
<ul>
  <li><em>k klein</em>: Exploration, globales Lernen, neue Aktionen untersuchen</li>
  <li><em>k gro√ü</em>: Exploitation, lokales Lernen, bekannte Aktionen untersuchen</li>
</ul>

<p>‚Üí beste L√∂sung: √Ñnderung von global zu lokal w√§hrend des Lernprozesses</p>

<p><em>Optimierungen</em>: in jeder Lernepisode $\hat{Q}$ vom Zustand s zum Ziel anpassen, Speichern von Bewertungen r f√ºr jedes $(s,a)$
‚Üí schnelle Konvergenz, Speicheraufwand steigt; Anwendung wenn Aktionen hohen zeitaufwand haben (z.B. Robotik)</p>

<p><strong>Lernen von Aktionssequenzen</strong>: Bewertung erst nach Sequenz von Aktionen bekannt bzw. machmal auch erst am Ziel</p>
<ul>
  <li><em>Temporal Difference Learning:</em> Differenz folgender Sch√§tzungen als Lernsignal</li>
  <li><em>Vorw√§rts ‚Äì Sicht (theoretisch):</em> gewichtete Anpassung an direkt nachfolgender Sch√§tzunh (1-step) oder an n Schritte nachfolgender
Sch√§tzung (n-step)</li>
  <li><em>R√ºckw√§rtige Sicht des TD-Lernen (praktisch):</em> Fehlersignale (temporal differences) in den Sch√§tzungen nach hinten weitergegeben</li>
  <li><em>Eligibility Traces (Verantwortlichkeitsspur):</em> Zust√§nde fpr Zustandsbewertung ‚Üí V-Lernen, Zustand/Aktion f√ºr Q-Wert Bewertung ‚Üí 
Q-Lernen</li>
</ul>

<h3 id="entscheidungsb√§ume">Entscheidungsb√§ume</h3>
<p><strong>Ziel:</strong> Partitionierung des Raums so, dass Daten in einer Region die gleiche Klasse haben.</p>

<p>(hier Abb. dt-ziel.png)</p>

<p><strong>Repr√§sentation</strong>:</p>
<ul>
  <li><em>Knoten:</em> Attributtest</li>
  <li><em>Zweig:</em> Attributwert</li>
  <li><em>Blatt:</em> Aussage (Klassifikation)</li>
</ul>

<p><strong>Anwendung</strong>: Instanzen lassen sich Attribut-Wert-Paar beschreiben, Zielfunktion besitzen diskrete Ausgabewerte, 
Disjunkte Hypothesen erforderlich, Beispieldaten m√∂glicherweise vertauscht, Beispieldaten enhalten fehlende Attribute</p>

<p><strong>Verfahren</strong>:</p>
<ul>
  <li><em>ID3:</em> nicht-inkrementelles Verfahren</li>
  <li><em>C4.5:</em> Verbesserung von ID3 durch generalisierte Regeln (Pruning), kommerzielles System</li>
  <li><em>ID5R:</em> inkrementelles Verfahren</li>
</ul>

<p><strong>Entropie</strong>: ist ein Ma√ü der Homogenit√§t der Trainingsdaten. Die Entropie gibt an, wie ‚Äú√ºberraschend‚Äù ein Ergebnis ist.
Mit der Menge der Trainingsbeipiele S, Anteil der positiven
Beispiele $p_+$ in S und Anteil der negativen Beispiele $p_-$ in S: $Entropie(S) = -p_+ log_2 p_+ - p_- log_2 p_-$</p>

<p>(Abb entropie.png)</p>

<p><em>Ziel</em>: ist es, die Daten durch einen Attributwert v m√∂glichst die Klassen + und - zuzuteilen und dabei schrittweise die Entropie
maximal zu reduzieren.</p>

<p><strong>Gewinn</strong>: ist die erwartete Reduzierung der Entropie durch Einsortierung √ºber dem Attribut A mit der Menge aller
m√∂glichen Attributwerte V(A) von A und der Untermenge $S_v$ von S, f√ºr die A den Wert v annimmt.
$Gewinn(S,A) = Entropie(S) - \sum_{v n V(A)} \frac{|S_v|}{|S|} Entropie(S_v)$</p>

<p><em>Ziel</em> der maximierung des Gewinns ist es, einen Entscheidungsbaum mit niedriger Tiefe zu erreichen.</p>

<p><strong>ID3</strong>: ID3 (Iterative Dichotomiser 3) ist ein Algorithmus, der zur Entscheidungsfindung dient. Er wird bei Entscheidungsb√§umen eingesetzt. 
<em>Algorithmus:</em> (abb id3-aufbau.png)
<em>Auswahl des besten Entscheidungsattribute (Schritt 1)</em>: Durch Vergleich der Teilb√§ume. Dazu schreibt man unter den Knoten 
ein Label mit dem Anzahl der Positiven Beispiele (+) und Anzahl der negativen Beispiele (-).</p>

<p><em>Beispiel ‚ÄúTennis Spielen‚Äù:</em> (abb bsp-tennis1.png und bsp-tennis2.png)</p>

<p><em>Suche im Hypothesenraum</em>: Typischerweise gibt es viele Entscheidungsb√§ume f√ºr die Trainingsbeispiele. Bei Entscheidungsb√§umen
ist der Hypothesenraum <em>vollst√§ndig</em>, d.h. dass die Zielfunktion im Entscheidungsbaum enthalten sein muss. Diese Hypothese wird
 ‚ÄúSimple-to-Complex‚Äù (Schrittweises erstellen ausgehend vom leeren Baum, ‚ÄúHill-Climbing‚Äù gennannt) erstellt. Dabei sind allerdings
 lokale Minima m√∂glich.</p>

<p><em>Induktiver Bias</em>: Die Pr√§ferenz bei Entscheidungsb√§umen liegt in kleinen B√§umen und B√§ume, deren Attribute nahe an der Wurzel einen
hohen Informationsgewinn besitzen. der ID3-Bias ist eine Pr√§ferenz f√ºr bestimmte Hypothesen (Pr√§ferenzbias) aber keine Eintschr√§nkung
des Hypothesenraums (Restriktionsbias) ‚Üí nach Occam‚Äôs Razor wird die einfachste Hypothese gew√§hlt, die mit den Trainingsdaten 
√ºbereinstimmt. Einfache (in diesem Fall ‚Äúkurze‚Äù, da kleine B√§ume) Hypothesen gibt es seltener als lange. Da ein kleiner Baum (=
kurze Hypothese), der die Daten korrekt beschreibt mit gro√üer Wahrscheinlichkeit auch richtig ist. Eine lange Hypothese k√∂nnte hingegen
reiner Zufall sein. Au√üerdem sind kurze B√§ume effiziente (in Rep√§sentation und Auswertung).</p>

<p><em>Overfitting</em>: Beim ID3-Verfahren w√§chst jeder Zweig so lange, bis die Trainingsbeispiele perfekt klassifiziert werden (basierend
auf statistisch-approximierten Informationsgewinn: Entropie, Gewinn). Dabei kann es zu Problemen kommen, wenn die Daten verrauscht
sind oder die Beispiele nicht repr√§sentativ sind (z.B. zu wenig Trainingsdaten). Die daraus entstehenden Hypothese enth√§lt dann
einen <em>Fehler</em>. Ein Fehler auf den Trainingsdaten hei√üt $Fehler_T(h)$, ein Fehler auf den gesamten Daten hei√üt $Fehler_D(h)$. <br />
Eine Hypothese h <em>overfittet</em> die Daten d, wenn es eine alternative Hypothese h‚Äô gibt, so dass <br />
1) $Fehler_T(h) &lt; Fehler_T(h‚Äô)   <br />
2) $Fehler_D(h) &gt; Fehler_D(h‚Äô)   <br />
‚Üí Das Modell lernt als auswendig.</p>

<p>(Abbildung id3-overfitting.png)</p>

<p>Um Overfitting zu vermeiden, kann man das Baumwachstum fr√ºhzeitig stoppen oder den Baum nachtr√§glich <em>Prunen</em> (besser in Praxis). Um die 
optimale Baumgr√∂√üe zu bestimmen kann man auf separaten Testdaten, die Richtigkeit des Baumen √ºberpr√ºfen oder statistische Test (wie
z.b. $\chi¬≤$) auf den Trainingsdaten ausf√ºhren.</p>

<p><em>Reduced Error Pruning:</em> Eine M√∂glichkeit zur Vermeidung von Overfitting ist das ‚ÄúZuschneiden‚Äù eines Entscheidungsbaums. 
Dadurch bekommt man die kleinste Variante des akkuratesten Unterbaumes. Allerdings erh√∂ht bei wenigen Daten, das Aufteilen der Daten
die Fehleranf√§lligkeit.
Vorgehen: (img. rep1.png)
(img rep2.png)</p>

<p><em>GewinnAnteil:</em> Attribute mit vielen Werten werden durch Gewinn gegen√ºber solchen mit wenigen Werten bevorzugt (
z.B. Datum). Aus diesem Grund werden nun Attributen ‚Äúbestraft‚Äù, wenn sie viele Werte besitzen. Danei ist die Menge aller m√∂glichen Attributwerte
V(A) von A und die Untermenge $S_v$ von S, f√ºr die A den Wert v annimmt:
$GewinnAnteil(S,A) = \frac{Gewinn(S,A)}{SplittInformation(S,A)}$ mit $Splitinformation (S,A) = -\sum_{v \in V(A)} \frac{|S_v|}{|S|} log_2
\frac{|S_v|}{|S|}$</p>

<p><em>Kontinuierliche Attributwerte</em>: Besitzt ein Attribut A kontinuierliche Werte, definieren wir dynamisch ein neues diskre-wertiges
Attribut $A_c$, dass ‚Äúwahr‚Äù ist, wenn A &gt; c. Der Schwellwert c wird √ºber den Informationsgewinn definiert. Nach Sortierung
der Beispiele nach ihren Werten, liegt der optimale Schwellwert in der Mitte zwischen zwei benachbarten Beispielen mit unterschiedlicher
Klassenzugeh√∂rigkeit. 
(img id3-bsp-kont.png)</p>

<p><em>Unbekannte Attributwerte</em>: Fehlen Attributwerte, so werden die Beispiele wie gewohnt in den Entscheidungsbaum einsortiert und fehlende
Attribute bekommen entweder den h√§ufigsten Attributwert der Beispiele oder der Beispiele. Bei der Klassifikation wird genauso verfahren.</p>

<p><em>Attribute mit Kosten:</em> Die Bestimmung der Attributwerde ist mit unterschiedlichen Kosten verbunden. Deshalb ist es das Ziel, einen 
Entscheidungsbaum zu finden, der niedrige erwartete Kosten hat:
$\frac{Gewinn(S,A)¬≤}{Kosten(A)}</p>

<p><em>Window</em>: ist eine Lernmethode f√ºr gro√üe Datenmengen. Dabei wird eine zuf√§llige Untermenge der Trainingsdaten ausgew√§hlt (=Window) und 
daraus der Entscheidungsbaum gebildet. Alle √ºbrigen Beispiele werden mit dem erstellten Entscheidungsbaum klassifiziert. Falls
nicht alle Daten korrekt klassifiziert werden, wird ein Teil der falsch klassifizierten Daten (zuf√§llige Auswahl) dem Window hinzugef√ºgt 
und ein neuer Entscheidungsbaum erstellt.</p>

<p><em>Einordnung</em>: (img einordnung-id3.png)</p>

<p><strong>C4.5</strong>: ist eine Weiterentwicklung des urspr√ºnglichen ID3-Algorithmus. Es unterst√ºtzt kontinuierliche Attribut-Werte und kann mit
fehlenden Attributwerten umgehen. Zudem verwendet es <em>Rule Post-Pruning</em>.</p>

<p><em>Rule-Post-Pruning:</em> (rpp-verfahren.png und rpp-beispiel.png)</p>

<p><em>Einordnung</em>: (img einordnung-c45.png)</p>

<p><strong>ID5R</strong>: ist ein inkrementelles Verfahren, d.h k√∂nnen neue Beispiele schrittweise eingebracht werden. Das Ergebnis ist
√§quivalent zu einem durch ID3 erstellten Entscheidungsbaum.</p>

<p><em>Repr√§sentation:</em> Zur Berechnung des Informationsgewinns auf jeder Ebene ohne die bisherigen Beispiele erneut zu betrachten, 
werden die Knoten aufgeteilt:</p>
<ul>
  <li>Antwortknoten(Bl√§tter): sind Klassenbezeichner und Beschreibungen der Instanzen, die zu der Klasse geh√∂ren</li>
  <li>Entscheidungsknoten: sind Attributtest mit Zweigen f√ºr jeden Attributwert. Pro Attributwert gibt es einen
Z√§hle f√ºr positive und negative Beispiel. Ein zus√§tzlicher Z√§hler z√§hlt zudem die noch ausstehenden Attributtests.
(img id5r-rep-bsp.png)</li>
</ul>

<p><em>Update des Baumes beim Hinzuf√ºgen von Beispielen</em>: Da ID5R ein inkrementeller Verfahren ist, k√∂nnen zur Laufzeit neue
Beispiele hinzugef√ºgt werden. Um damit umuzgehen muss der Baum updatebar sein. 
(img update-id5r-1.png und update-id5r-2.png)</p>

<p><em>Restrukturierung:</em> Wenn nach einem Update, das Attribut an der Wurzel nichtmehr den gr√∂√üten Informationsgewinn hat, muss umstrukturiert
werden, damit das Attribut mit dem gr√∂√üten Informationsgewinn an der Wurzel steht.
(img restruk-id5r.png)</p>

<p><em>Einordnung:</em> (img einordnung-id5r.png)</p>

<p><strong>Random Forests</strong>: ist ein Klassifikationsverfahren, das aus mehreren unkorrelierten Entscheidungsb√§umen besteht. 
Alle Entscheidungsb√§ume sind unter einer bestimmten Art von Randomisierung w√§hrend des Lernprozesses gewachsen. 
F√ºr eine Klassifikation darf jeder Baum in diesem Wald eine Entscheidung treffen und die Klasse mit den meisten 
Stimmen entscheidet die endg√ºltige Klassifikation. <a href="https://de.wikipedia.org/wiki/Random_Forest">Wikipedia</a>
Random Forests l√§sst sich <em>schnell trainieren</em>, da die einzelnen Entscheidungsb√§ume kleiner sind und
die Trainingszeit linear mit der Anzahl der B√§ume steigt. Au√üerdem ist es <em>parallelisierbar</em> und <em>effizient</em>
f√ºr gro√üe Datenmengen.</p>

<p><em>Vergleich zu Standard Entscheidungsb√§umen</em>: Das Pruning der B√§ume ist nicht notwendig. Overfitting ist erlaubt. Die
Attributwahl findet auf einer zuf√§lligen Untermenge aller Attribute statt (daher die Randomisierung).</p>

<p><em>Eigenschaften der B√§ume</em>: Jeder Baum sollte ein <em>guter Klassifikator</em> sein und die B√§ume untereinander m√∂glichst <em>unkorreliert</em>.</p>

<p><em>Randomisierungsm√∂glichkeiten</em>: sind entweder <em>Bootstraping</em> (Aus N Trainingsdaten werden N Trainingsbeispiele mit zur√ºcklegen
gezogen. Baum hat so ca ~63% der Gr√∂√üe verglichen mit allen Trainingsdaten.) oder die Auswahl des Attributtests aus einer Teilmenge der
vorhandenen Attributtests.</p>

<h3 id="hidden-markov-modelle">Hidden Markov Modelle</h3>
<p>Die <em>Prozesse</em> der realen Welt sind oft nichtdeterministisch aber stochastisch beschreibbar (z.B. durch Markovsche Entscheidungsprozesse).
Die <em>Signale</em> der realen Welt sind meist verrauscht und besitzen nichtdeterministische Eigenschaften. Mit stochastischen Prozess-
und Signalmodellen kann man diese Signale erkennen und theoretisch durch signalverarbeitende Systeme beschreiben oder simulieren. In der 
Praxis sind die Ergebnisse au√üerordentlich gut.</p>

<p><strong>Diskreter Markov Prozess</strong>: Markow Prozesse eignen sich sehr gut, um zuf√§llige Zustands√§nderungen eines Systems zu modellieren, 
falls man Grund zu der Annahme hat, dass die Zustands√§nderungen nur √ºber einen begrenzten Zeitraum hinweg Einfluss aufeinander 
haben oder sogar ged√§chtnislos sind. N diskrete Zust√§nde $S = {S_1,S_2,..,S_N}$, die Zeitpunkte der Zustands√ºberg√§nge t = 1,2,‚Ä¶,T
und der aktuelle Zustand $q_t$ zur Zeit t bilden einen <em>diskreten Markov Prozess</em>. <br />
Als Parameter sind die <em>Anfangszustandswahrescheinlichkeiten</em> $\pi_i = P(q_1 = S_i)$ und die <em>√úbergangswahrscheinlichkeiten</em>
$a_{ij} = P(q_t = s_j | q_{t-1} = S_i) = P(S_j | S_i)$ gegeben. <br />
<em>Markov-Bedingung:</em> Die √úbergangswahrscheinlicheiten geben an, wie sich ein Zustand in <em>einem</em> Zeitschritt entwickelt</p>
<ul>
  <li>Beschr√§nkter Horizont: Die Wahrscheinlichkeit einen Zustand zu erreichen ist nur von seinem <em>direkten</em> Vorg√§ngerzustand abh√§ngig
$P(q_{t+1} = S_j | q_t = S_i)$</li>
  <li>Zeitinvarianz: Die Wahrscheinlichkeit eines Zustands√ºbergangs ist unabh√§ngig von der Zeit.</li>
</ul>

<p>(img diskr-mark-pr.png)</p>

<p><em>Beispiel: Wetter</em>: Jeden Mittag wird das Wetter beobachtet. M√∂chliche Wetterlagen sind  S1 = Regen (Schnee), S2 = bew√∂lkt und S3 = sonnig.
Daraus ergeben sich die m√∂glichen Zust√§nde $S = {S_1,S_2,S_3}$ als √úbergangswahrscheinlichkeiten sind ist die √úbergangsmatrix A 
gegeben, woraus sich dieser Graph entwickeln l√§sst. (img matrix-a.png und graph-wetter.png) <br />
Wie wahrscheinlich ist es also, dass das Wetter in den n√§chsten 7 Tagen genau {sonnig, sonnig, Regen, Regen, sonnig, bew√∂lkt, sonnig} ist?</p>

<p>(img wetter-beobachtung.png)</p>

<p><strong>Hidden Markov Modelle (HMM)</strong>: Bisher waren die Ereignisse (Zust√§nde) direkt beobachtbar, das ist bei Hidden Markov
Modellen nichtmehr der Fall. Die Zust√§nde k√∂nnen nur indirekt beobachtet werden, durch eine stochastische Funktion des Zustands
(z.B. Mitbewohner der bei Regen oft einen Regenschirm mitnimmt). Hidden Markov Modelle beschreiben einen <em>doppelt
stochastische Prozess</em>. Das hei√üt, dass der zugrunde liegende stochastische Prozessnur indirekt durch eine andere
Menge an stochastischen Prozessen beobachtet werden kann und diese eine Beobachtungssequenz produziert. <br />
HMM ist ein F√ºnf-Tupel $ \lambda = {S,V,A,V,\Pi}$ bestehend aus:</p>
<ul>
  <li>S: Menge der Zust√§nde $ S= {S_1,S_2,‚Ä¶,S_N}</li>
  <li>V: Menge der Ausgaben/Beobachtungen $V={V_1,V_2,‚Ä¶,V_M}</li>
  <li>A: Matrix der √úbergangswahrscheinlichkeiten $A = (a_{ij}), A \in [0,1]^{NxN}$</li>
  <li>B: Matrix der Emissions/Beobachtungswahrscheinlichkeiten $B = (b_{ik}), B \in [0,1]^{NxM}$</li>
  <li>$\Pi$: Verteilung des Anfangszustands $\Pi = {\pi_i | \pi_i = P(q_1| S_i)}$ mit $q_1$ als Startzustand</li>
</ul>

<p>Dabei ist die Wahrscheinlichkeit f√ºr einen Zustand $p(q_{t+1} = S_j) = \sum_{S_i \in S} a_{ij} \cdot p(q_t = S_i)$ und die
Wahrscheinlichkeit f√ºr eine Beobachtung $p(o_{t+1} = V_j) = \sum_{S_i \in S} b_{ik} \cdot p(q_t = S_i)$. Dabei bezeichnet
$q_t$ den Zustand und $o_t$ die Ausgabe zum Zeitpunkt t.</p>

<p><em>Beispiel: Regenschirm</em>: Anstatt das Wetter direkt zu beobachten, wird nun der Mitbewohner beobachtet, 
der einen Regenschirm mitnimmt. Regnet es drau√üen? <br />
Daf√ºr ist die Zustandsmenge $S = { S_1 = Regen, S_2 = Sonne } $, die Ausgabemenge $V= { V_1 = Regenschirm, V_2 = kein Regenschirm } $</p>

<p>(img Matritzen-regenschirm-bsp.png und graph-regenschirm-bsp.png)</p>

<p><em>Beispiel: M√ºnzwurf</em>: Jemand wirft eine M√ºnze, dabei ist die Anzahl der gezinkten M√ºnzen unbekannt. Bekannt ist nur
das Ergebnis des Wurfs H = Kopf bzw. T = Zahl.</p>

<p>(img muenzwurf-bsp.png)</p>

<p><em>Beispiel: Urnen ziehen</em>: Es gibt N unterschiedliche Urnen mit unterschiedlicher Anzahl an B√§llen in M Farben. 
Beim stochastischen ziehen aus einer Urne mit zur√ºcklegen, wird die Sequenz der Farbe der gezogenen B√§llen beobachtet.</p>

<p>(img urnen-bsp.png)</p>

<p><strong>Probleme im Zusammenhang mit HMMs</strong></p>

<p><strong>Problem 1: Evaluationsproblem:</strong> (img hmm-prob-1.png)</p>

<p>Das Evaluationsproblem beschreibt wie gut ein gegebenes Modell $ \lambda = {S,V,A,V,\Pi}$
eine Beobachtungssequenz erkl√§rt. Gesucht wird die Wahrscheinlichkeit P(0| \lambda) f√ºr die Ausgabe $O = o_1 o_2 ‚Ä¶ o_t$. 
Da nicht bekannt sit, was die verdeckte Zustandsfolge ist, m√ºssen alle m√∂glichen Zustandsfolgen betrachtet werden. 
<em>Algorithmus</em>: Der Vorw√§rts- bzw. R√ºckw√§rtsalgorithmus berechnet die Teilresultate (in Tabellen) und errechnet daraus eine
L√∂sung, ohne nochmals alle Berechnungen zu starten und ist damit schneller, als der Naive Ansatz.</p>

<p><em>Vorw√§rtsalgorithmus:</em> (img vorwaerts-alg.png)</p>

<p><em>Beispiel:</em> Wie hoch ist die Wahrscheinlichkeit, dass ein Mitbewohner in den letzten drei Tagen
den Regenschirm mitgenommen hat? Die Zust√§nde  $S = {S_1 = Regen, S_2 = Sonne}$, die Beobachtungen $V={V_1 = Regenschirm, V_2 = kein Regenschirm}
sowie das Modell sind gegeben:</p>

<p>(img vorwaerts-modell.png)</p>

<p><em>R√ºckw√§rtsalgorithmus:</em> (img r√ºckw√§rts-alg.png)</p>

<p><strong>Problem 2: Dekodierungsproblem:</strong> (img hmm-prob-2.png)</p>

<p>Beim Dekodierungsproblem ist die Ausgabesequez O und das Modell $ \lambda = {S,V,A,V,\Pi}$
gegeben und die wahrscheinlichste Zustandsfolge Q (optimale Zustandsfolge) wird gesucht, die O erkl√§rt. Dabei
werden die Zust√§nde $q_t$ unter Verwendung des Vorw√§rts- und R√ºckw√§rtsalgorithmus so gew√§hlt, dass die Zust√§nde 
unabh√§ngig voneinander am Wahrscheinlichsten sind.<br />
<em>Optimalit√§tskriterium:</em> Bei nicht v√∂llst√§ndig vernetzen HMMs ergibt die Maximierung der wahrscheinlichen Zust√§nde unter Umst√§nden
keinen g√ºltigen Pfad (z.B. wenn der beste Zustand $S_1$ ist, aber der anschlie√üend folgende Zustands√ºbergang 0 ist). Daher
ist es besser die <em>insgesamt beste Zustandsfolge</em> zu finden.</p>

<p><em>Viterbi-Algorithmus</em>: Der Viterbi-Algorithmus findet diesen Pfad, mit der insgesamt besten Zustandsfolge, durch rekursive
Maximierung entlang m√∂glicher Zustandsfolgen (Pfade).</p>

<p>(img viterbi-alg1.png und viterbi-alg2.png)</p>

<p><strong>Problem 3: Lern- und Optimierungsproblem:</strong>  (img hmm-prob-3.png)
Das Lern- und Optimierungsproblem ist das schwierigste der drei Probleme und es ist kein analytischer L√∂sungsweg bekannt.
Gegeben einer Ausgabesequenz O und ein Suchraum (Hypothesenraum) f√ºr Modelle, 
ist die Anpassung der Parameter des Modells $ \lambda = {S,V,A,V,\Pi}$ gesucht, sodass O besser erkl√§rt wird. Es gibt
 keinen optimalen Weg, um die Modellparameter zu sch√§tzen. Das Modell $\lambda$ soll also weiter optimiert/trainiert werden. 
 Als Beispiel soll nun vorausgesagt werden k√∂nnen, wie die Wetterlage ist, abh√§ngig von der Beobachtung ‚ÄúRegenschirm‚Äù bzw. 
 ‚Äúkein Regenschirm‚Äù (sehr vereinfachtes Beispiel!).</p>

<p><em>Baum-Welch-Algorithmus:</em> ist Spezialfall des Expectation-Maximization-Algorithmus (EM) und ein iterativer Ansatz zur 
 lokalen Maximierung von P(O|$\lambda$). F√ºr eine Trainingssequen $O_{training}$ und einen Hypothesenraum f√ºr Modelle 
 $ \lambda = {S,V,A,V,\Pi}$ wird ein Modell gesucht, das die Daten am besten erkl√§rt. Dabei wird der Hypothesenraum
 so gew√§hlt, dass die Anzahl der Zust√§nde vorgegeben wird und lediglich die stochastischen Modellparameter angepasst werden.</p>

<p>(img baum-welch.png)</p>

<p>Es gilt P(O|$\lambda$) ‚â• P(O|={$\lambda$}). Damit vebessert sich das Modell auf der Menge der Trainingsdaten und
 realisiert den EM Ansatz. Das Training wird abgebrochen, wenn nur noch eine minimale Verbesserung eintritt, dennoch
 kann kein globales Maximum garantiert werden.</p>

<p><strong>Arten von Hidden Markov Modellen</strong>:</p>
<ul>
  <li><em>Ergodisches Modell:</em> Jeder Zustand kann von jedem anderen Zustand aus in einer endlichen Anzahl von Schritten erreicht werden.</li>
</ul>

<p>(img ergod-modell.png)</p>

<ul>
  <li><em>Links-nach-rechs-Modell (Bakis-Modell:</em> Der Zustandsindex wird mit der zeit gr√∂√üer oder bleibt gleich. Dieses Modell ist auch
 mit parallelen Pfaden m√∂glich.</li>
</ul>

<p>(img bakis.png)</p>

<p><strong>Einordnung</strong>: (img einordnung-hmm.png)</p>

<p><strong>Anwendungen</strong>: HMMs finden Anwendung in der Spracherkennung, der Gestenerkennung (z.B Robotik), im Autonomen Fahren (Ampelzustandssch√§tzung)
 und in der Bioinformatik (Genomanalyse). Kurz gesagt, √ºberall dort wo man zugrunde liegende stochastische Prozesse
 nur indirekt beobachten kann.</p>

<h3 id="lernen-nach-bayes">Lernen nach Bayes</h3>
<p><strong>Lernen nach Bayes</strong> ist ein <em>statistisches Lernverfahren</em> welches <em>vorhandenes Wissen</em>
(a priori Wahrscheinlichkeiten) mit <em>beobachteten Daten</em> kombiniert. Hypothesen k√∂nnen mit
einer Wahrscheinlichkeit angegeben werden. Jedes Beispiel kann die Glaubw√ºrdigkeit einer 
bestehenden Hypothese erh√∂hren oder verringern, allerding k√∂nnen Hypothesen dadruch nicht
ausgeschlossen werden. Um genauere Ergebnisse zu erzielen, k√∂nnen mehrere Hypothesen
gleichzeitig ausgewertet werden.</p>

<p>F√ºr dieses Verfahren ist allerding initiales Wissen √ºber viele Wahrscheinlichkeiten/Verteilungen
notwendig, dessen Sch√§tzung oft auf Hintergrundwissen oder vorhandenen Daten basiert. 
Zudem ist ein enormer Rechenaufwand notwendig.</p>

<p><strong>Wahrscheinlichkeitstheorie</strong> f√ºr Berechnungen:</p>
<ul>
  <li><em>Produktregel:</em> Konjunktion zweiter Ereignisse A und B: P(A ‚ãÄ B) = P(A|B) P(B) = P(B|A) P(A)</li>
  <li><em>Summenregel:</em> Disjunktion zweier Ereignisse A und B: P(A ‚ãÅ B) = P(A) + P(B) - P(A ‚ãÄ B)</li>
  <li><em>Theorem der totalen Wahrscheinlichkeit:</em> F√ºr sich gegenseitig ausschlie√üende Ereignisse
deren Gesamtwahrscheinlichkeit 1 ist gilt: $P(B) = \sum_{i=1}^{n} P(B|A_i) P(A_i)$</li>
  <li><em>Satz von Bayes:</em> $P(B|A) = \frac{P(A|B) P(B)}{P(A)}$</li>
</ul>

<p><strong>Theorem von Bayes</strong>:</p>
<ul>
  <li><em>P(h)</em> ist die Wahrscheinlichkeit, dass die Hypothese h aus dem Hypothesenraum H g√ºltig ist 
(a priori, d.h. vor der Beobachtung von D)</li>
  <li><em>P(D)</em> ist die Wahrscheinlichkeit, dass D als Ereignisdatensatz auftritt ohne zu Wissen, ob
die Hypothese g√ºltig ist.</li>
  <li><em>P(D|h)</em> ist die Wahrscheinlichkeit des Auftretens von D in einem Raum, in dem die 
Hypothese h gilt.</li>
  <li><em>P(h|D)</em> ist die Wahrscheinlichkeit, dass h gilt gegeben die beobachteten Daten D (a
posteriori)</li>
</ul>

<p>(img bayes-theorem.png)</p>

<p><strong>MAP-/ML-Hypothesen</strong> <br />
Bei der <strong>Auswahl von Hypothesen</strong> ist es das Ziel, die Hypothese h aus dem Hypothesenraum H
zu finden, die die gr√∂√üte Wahrscheinlichkeit gegeben der beobachteten Daten D liefert.
Diese Hypothese hei√üt <em>Maximum a posteriori (MAP) Hypothese</em>:</p>

<p>(img map.png)</p>

<p>Unter der Annahme P(hi) = P(hj) l√§sst sich die MAP Hypothese zur <em>Maximum Likelihood (ML)
Hypothese</em> vereinfachen:</p>

<p>(img ml.png)</p>

<p>Um die beste MAP-Hypothese zu finden, kann man <em>Brute Force</em> ausf√ºhren. Also f√ºr jede
Hypothese im Hypothesenraum H die a posteriori Wahrscheinlichkeit berechnen und die
Hypothese ausgeben, die die gr√∂√üte Wahrscheinlichekeit besitzt.</p>

<p><em>Beispiel: Medizinische Diagnose:</em> Angenommen 0.8% der Bev√∂lkerung leiden an Krebs. Hat man
Krebs, so ist der Test in 98% der F√§lle positiv. Hat man keinen Krebs, f√§llt der Test mit 3%
trotzdem positiv aus. Wie hoch ist die Wahrscheinlichkeit, dass eine Person mit einem positiven
Testeregebnis auch wirklich Krebs hat? (h = Krebs, h‚Äô = kein Krebs)</p>

<p>(img krebs1.png krebs2.png)</p>

<p>Beim <strong>Konzeptlernen</strong> ist ein endlicher Hypothesenraum H √ºber dem Raum der Instanzen X
gegeben. Die Aufgabe ist das Lernen eines Zielkonzepts c: X ‚Üí {0,1} mit der Sequenz von 
Instanzen {x1,‚Ä¶,xm} und Sequenz von Zielwerten D = {d1,‚Ä¶,dm}. Dabei wird angenommen, 
dass die Trainingsdaten D nicht vertauscht sind, also zugeh√∂rig der Instanzen vorliegen 
(di = c(xi)). Au√üerdem ist keine Hypothese wahrscheinlicher als eine andere.</p>

<p>(img konzeptlernen.png)</p>

<p><strong>Konsistenter Lerner</strong>: Ein Lernverfahren ist ein <em>konsistenter Lernen</em>, wenn es eine 
Hypothese liefert, die <em>keine Fehler auf den Trainingsdaten macht</em>. Unter den Voraussetzungen
des Konzeptlernens, liefert jeder konsistente Lerner eine <em>MAP-Hypothese</em>. Dies ist eine 
Methode um den induktiven Bias auszudr√ºcken. Die Entwicklung der a posteriori 
Wahrscheinlichkeiten mit wachsender Anzahl von Trainingsdaten sieht wie unten aus. F√ºr
inkonsistente Hypothesen gilt P(h) = 0.</p>

<p>(img konsis.png)</p>

<p><strong>Bayes-Klassifikator</strong></p>

<p>Bisher wurde nach der Hypothese mit der gr√∂√üten Wahrscheinlichkeite gegeben der Daten D
gesucht. Nun wollen wir die <em>wahrscheinlichste Klassifikation vj</em> einer Instanz x wissen.</p>

<p>(img klass-bayes-ex.png)</p>

<p><strong>Optimaler Bayes-Klassifikator</strong>: ist ein aus dem Satz von Bayes hergeleiteter Klassifikator.
 Er ordnet jedes Objekt der Klasse zu, zu der es mit der gr√∂√üten Wahrscheinlichkeit geh√∂rt.
 Um den Bayes-Klassifikator zu definieren, wird ein Kostenma√ü ben√∂tigt, das jeder m√∂glichen
 Klassifizierung Kosten zuweist. Der Bayes-Klassifikator ist genau derjenige Klassifikator,
 der die durch alle Klassifizierungen entstehenden Kosten minimiert.</p>

<p>(img bayes-klass.png bayes-klass-bsp.png)</p>

<p>Am obigen Beispiel bei der Klassifikation der positiv und negativ zugeordneten 
 Datenobjekten wird deutlich, dass die wahrscheinlichste Klasse mit a posteriori + w√§re, mit
 Bayes hingegen -. Kein anderes Klassifikationsverfahren schneidet durchschnittlich besser
 ab als Bayes. Allerdings ist es bei gro√üer Hyopthesenzahl sehr kostenintensiv.</p>

<p><strong>Gibbs Algorithmus</strong>: Der Gibbs Algorithmus ist ein Bayes-Klassifikator. Er w√§ht h aus H
 zuf√§llig gem√§√ü P(h|D) und nutzt h(x) als Klassifikation von x). Es gilt die Eigenschaft, dass
 der Fehler von Gibbs maximal so gro√ü ist die 2 * der Fehler von dem optimalen Bayes 
 Klassifikator.</p>

<p><strong>Naiver Bayes Klassifiktor</strong>: Die naive Grundannahme ist dabei, dass jedes Attribut nur 
 vom Klassenattribut abh√§ngt. Obwohl dies in der Realit√§t selten zutrifft, erzielen naive 
 Bayes-Klassifikatoren bei praktischen Anwendungen h√§ufig gute Ergebnisse, solange die 
 Attribute nicht zu stark korreliert sind.<br />
 Gegeben einer Instanz x als Konjuktion von Attributen a1,a2,‚Ä¶,an, einer endlichen Menge
 von Klassen V = {v1,‚Ä¶,vm} und einer Menge von klassifizierten Beispielen wird die 
 wahescheinlichste Klasse f√ºr die Instanz gesucht.</p>

<p>(img naiv-bay.png)</p>

<p>P(vj) l√§sst sich leicht aus dem Auftreten der KLasse vj in der Trainingsmenge berechnen
 (einfaches Z√§hlen). P(a1,a2,‚Ä¶,an|vj) ist schwerer zu berechnen, da es nur durch Ausz√§hlen
 aller Kombinationen der Attributwerte geht. Dazu ben√∂tigt man eine riesige Trainingsmenge.
 Deshalb wird vereinfacht angenommen, dass ai bedingt unabh√§ngig ist.</p>

<p>(img naiv-bay-unab.png)</p>

<p>Als naiver Bayes-Klassifikator ergibt sich daher: (img naiv-bay-klass.png)</p>

<p>(img bayes-tennis1.png bayes-tennis2.png)</p>

<p><strong>Laplace-Sch√§tzer</strong>: Nimmt ein Klasse und ein Attribut nicht einen bestimmten Wert
 in den Daten an, so kann der Wert gesch√§tzt werden. 
 Allgemein definiert man einen Laplace-Sch√§tzer als denjenigen Wert, der den Erwartungswert 
 einer Verlustfunktion unter der A-posteriori-Verteilung minimiert.</p>

<p><em>Beispiel: Klassifikation von Texten</em>: Das Ziel ist es zu Lernen, welche Nachrichten
 interessant sind: Dokument ‚Üí {+,-}. Zudem wird die Annahme (bag of words) gemacht, dass
 die Wahrscheinlichkeit des Auftretens eines bestimmten Wortes unabh√§ngig von der Position
 im Text ist.</p>

<p>(img text-bay.png)</p>

<p>Zum Sch√§tzen ben√∂tigt man das Vokabular, also alle W√∂rter und Token aus den Beispielen.</p>

<p>(img text-bay-schaetzen.png)</p>

<p><strong>Bayessche Netze</strong></p>

<p>Die naive Bayes-Annahme der bedingten Unabh√§ngigkeit ist oft zu beschr√§nkend. Allerdings
 ist ohne solche vereinfachenden Annahmen das Lernen nach Bayes oft nicht m√∂glich.</p>

<p><strong>Bayessche Netze</strong>: beschreiben bedingte (Un-)Abh√§ngigkeiten bzgl. Untermengen von 
 Variablen und erlauben somit die Kombination von a priori Wissen √ºber bedingte 
 (Un-)Abh√§ngigkeiten von Variablen mit den beobachteten Trainingsdaten.
 Ein <em>Bayessches Netz</em> ist ein gerichteter azyklischer Graph (DAG), 
 in dem die Knoten Zufallsvariablen und die Kanten bedingte Abh√§ngigkeiten zwischen den 
 Variablen beschreiben. Jedem Knoten des Netzes ist eine bedingte 
 Wahrscheinlichkeitsverteilung der durch ihn repr√§sentierten Zufallsvariable gegeben, 
 die Zufallsvariablen an den Elternknoten zuordnet. Sie werden durch 
 Wahrscheinlichkeitstabellen beschrieben. Vorg√§nger(Yi) ist die Menge der direkten
 Vorg√§nger von Yi. Y ist Nachfolger von X , wenn ein gerichteter Pfad von X nach Y
 existiert. Die Kanten repr√§sentieren die Zusicherung, dass eine Variable von ihren Nicht-
 Nachfolgern bedingt unabh√§ngig ist, gegeben ihre direkten Vorg√§nger.</p>

<p>(img bn-berechnung.png bn-graph.png bn-table.png)</p>

<p><strong>EM-Algorithmus</strong></p>

<p>Bei bekannter Struktur, aber nur einigen beobachtbaren Variablen, kommt der EM-Algorithmus
  zum Einsatz.</p>

<p><strong>Expectation-Maximization-Algorithmus (EM)</strong>: ist ein iterativer Ansatz zum Sch√§tzen der 
nicht beobachtbaren Werte (E-Schritt) und Anpassung der Parameter (M-Schritt). Er wird
verwendet zum Training von Bayesschen Netzen oder zum Lernen von Hidden Markov Modellen.</p>

<p>(img em.png)</p>

<p><em>Ablauf</em> des EM-Algorithmus:</p>
<ol>
  <li>
    <p>Zuf√§llige <em>Initialisierung</em> der Hypothese h.</p>
  </li>
  <li>
    <p><em>Expectation (E) Schritt</em>: Berechne den Erwartungswert E[zij] f√ºr jede versteckte Variable
unter der Annahme, dass die aktuelle Hypothese h g√ºltig ist.</p>
  </li>
  <li>
    <p><em>Maximization (M) Schritt</em>: Berechne eine nee Maximum Likelihood Hypothese h‚Äô unter der 
Annahme, dass die Werte der versteckten Variabe zij die im E-Schritt berechneten 
Erwartungswerte annehmen.</p>
  </li>
  <li>
    <p>Ersetze h durch die neue Hypotehese h‚Äô und iteriere.</p>
  </li>
</ol>

<p>Der EM-Algorithmus konvergiert gegn eine lokale Maximum Likelihood Hypothese und liefert 
Sch√§tzungen f√ºr die versteckten Variablen Z. Er sucht die Maximum Likelihood Hypothese h‚Äô, 
welche den Erwartungswert √ºber die m√∂glichen Werte der versteckten Variablen gegeben
der vollst√§ndigen Daten berechnet.</p>

<p><strong>Einordnung</strong>: (img einordnung-bayes.png)</p>

:ET